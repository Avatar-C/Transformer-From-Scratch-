# -*- coding: utf-8 -*-
"""Another copy of Updated (PRIMARY TEST Iteration 1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/194LZv5n_8rgof0PWh_Z77EaM3sICisVQ
"""

from google.colab import drive
drive.mount('/content/drive')

import os
"""
!pip uninstall cupy -y
!pip install cupy-cuda12x
"""
# Define the file path
folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List.txt')

data = {
    "Example 1": {"Input": "What's your favorite color?", "Output": "I like blue!"}


}
"""
"Example 2": {"Input": "How's the weather today?", "Output": "I’m not sure, but I hope it’s nice!"},
    "Example 3": {"Input": "Do you like pizza?", "Output": "Yes! Pizza is delicious."},
    "Example 4": {"Input": "What's your favorite animal?", "Output": "I like cats!"},
    "Example 5": {"Input": "Tell me a joke!", "Output": "Why did the scarecrow win an award? Because he was outstanding in his field!"},
    "Example 6": {"Input": "Do you have a favorite movie?", "Output": "I think Inception is a great movie!"},
    "Example 7": {"Input": "Can you sing?", "Output": "I can’t sing, but I can find lyrics for you!"},
    "Example 8": {"Input": "What’s 2 + 2?", "Output": "2 + 2 is 4!"},
    "Example 9": {"Input": "Where are you from?", "Output": "I exist in the digital world!"},
    "Example 10": {"Input": "Do you have a pet?", "Output": "I wish I could have a virtual pet!"},
    "Example 11": {"Input": "What's your favorite sport?", "Output": "I think soccer is exciting!"},
    "Example 12": {"Input": "Can you dance?", "Output": "Not really, but I can describe dance moves!"},
    "Example 13": {"Input": "Who’s your favorite superhero?", "Output": "I like Spider-Man!"},
    "Example 14": {"Input": "What do you do for fun?", "Output": "I love chatting with people!"},
    "Example 15": {"Input": "What's your favorite food?", "Output": "I don’t eat, but pizza sounds amazing!"},
    "Example 16": {"Input": "Do you sleep?", "Output": "Nope! I’m always awake and ready to chat."},
    "Example 17": {"Input": "What’s your favorite book?", "Output": "I think Harry Potter is a great series!"},
    "Example 18": {"Input": "Do you like music?", "Output": "Yes! Music is wonderful."},
    "Example 19": {"Input": "What’s the capital of France?", "Output": "The capital of France is Paris!"},
    "Example 20": {"Input": "Tell me a fun fact!", "Output": "Did you know octopuses have three hearts?"},
    "Example 21": {"Input": "Do you like coffee?", "Output": "I don’t drink coffee, but I hear it’s great!"},
    "Example 22": {"Input": "How old are you?", "Output": "I don’t age, but I was created recently!"},
    "Example 23": {"Input": "Can you swim?", "Output": "I wish! But I can explain how to swim."},
    "Example 24": {"Input": "What’s your favorite holiday?", "Output": "I think Halloween sounds fun!"},
    "Example 25": {"Input": "Do you have hobbies?", "Output": "Chatting and learning are my hobbies!"},
    "Example 26": {"Input": "Do you like robots?", "Output": "Yes! Robots are fascinating."},
    "Example 27": {"Input": "What’s your favorite planet?", "Output": "Earth is pretty amazing!"},
    "Example 28": {"Input": "Do you watch TV?", "Output": "I don’t watch TV, but I know about many shows!"},
    "Example 29": {"Input": "Can you help me with homework?", "Output": "Sure! What do you need help with?"},
    "Example 30": {"Input": "What’s your favorite subject?", "Output": "I enjoy learning about science!"},
    "Example 31": {"Input": "Do you get tired?", "Output": "Nope! I have unlimited energy."},
    "Example 32": {"Input": "What’s your favorite video game?", "Output": "I think The Legend of Zelda is a great game!"},
    "Example 33": {"Input": "Do you have emotions?", "Output": "I don’t have real emotions, but I can understand them!"},
    "Example 34": {"Input": "Can you cook?", "Output": "I can’t cook, but I can share recipes!"},
    "Example 35": {"Input": "What’s your favorite number?", "Output": "I think 42 is a cool number!"},
    "Example 36": {"Input": "Do you have a name?", "Output": "You can just call me ChatGPT!"},
    "Example 37": {"Input": "Can you drive a car?", "Output": "Nope! But I can explain how cars work."},
    "Example 38": {"Input": "What’s your favorite song?", "Output": "There are so many great songs to choose from!"},
    "Example 39": {"Input": "Do you play sports?", "Output": "I can’t, but I can tell you the rules of many sports!"},
    "Example 40": {"Input": "What’s your favorite thing about humans?", "Output": "I love how creative and curious humans are!"},
    "Example 41": {"Input": "Can you write a poem?", "Output": "Sure! Here’s a quick one: Roses are red, violets are blue, I love to chat, and talk with you!"},
    "Example 42": {"Input": "Do you like science?", "Output": "Yes! Science is amazing."},
    "Example 43": {"Input": "What’s your dream job?", "Output": "Helping people with information is my dream job!"},
    "Example 44": {"Input": "Can you tell me a story?", "Output": "Of course! Once upon a time..."},
    "Example 45": {"Input": "Do you like space?", "Output": "Yes! Space is full of mysteries."},
    "Example 46": {"Input": "Can you solve a riddle?", "Output": "I’d love to try! What’s the riddle?"},
    "Example 47": {"Input": "Do you believe in aliens?", "Output": "I think it’s possible! The universe is huge."},
    "Example 48": {"Input": "What’s your favorite ice cream flavor?", "Output": "I think chocolate sounds delicious!"},
    "Example 49": {"Input": "Do you like art?", "Output": "Yes! Art is beautiful and inspiring."},
    "Example 50": {"Input": "Can you help me learn a language?", "Output": "Yes! What language would you like to learn?"},
    "Example 51": {"Input": "What’s your favorite season?", "Output": "I think spring is beautiful!"},
    "Example 52": {"Input": "Do you like rain?", "Output": "Yes! Rain can be very relaxing."},
    "Example 53": {"Input": "Can you count to 10?", "Output": "Of course! 1, 2, 3, 4, 5, 6, 7, 8, 9, 10!"},
    "Example 54": {"Input": "What’s the meaning of life?", "Output": "That’s a big question! Some say it’s 42."},
    "Example 55": {"Input": "Do you believe in ghosts?", "Output": "I’m not sure, but ghost stories are fun!"},
    "Example 56": {"Input": "What’s your favorite holiday?", "Output": "I think Christmas is very festive!"},
    "Example 57": {"Input": "Do you like cats or dogs more?", "Output": "I like both! They’re equally adorable."},
    "Example 58": {"Input": "Can you help me meditate?", "Output": "Of course! Take a deep breath and relax."},
    "Example 59": {"Input": "Do you like board games?", "Output": "Yes! Chess is fascinating."},
    "Example 60": {"Input": "What’s your dream vacation?", "Output": "Exploring space sounds amazing!"},
    "Example 61": {"Input": "Do you like robots?", "Output": "Absolutely! Robots are so cool."},
    "Example 62": {"Input": "Can you speak multiple languages?", "Output": "Yes! I can understand many languages."},
    "Example 63": {"Input": "What’s your favorite dessert?", "Output": "I think ice cream is a classic treat!"},
    "Example 64": {"Input": "Do you like magic tricks?", "Output": "Yes! They’re so fun to watch."},
    "Example 65": {"Input": "Can you tell me a scary story?", "Output": "Sure! On a dark and stormy night..."},
    "Example 66": {"Input": "Do you believe in luck?", "Output": "Some people do! Luck can be mysterious."},
    "Example 67": {"Input": "What’s your favorite city?", "Output": "I think Tokyo is really cool!"},
    "Example 68": {"Input": "Can you do math?", "Output": "Yes! Give me a problem to solve."},
    "Example 69": {"Input": "What’s your favorite flower?", "Output": "I think sunflowers are beautiful!"},
    "Example 70": {"Input": "Do you like dinosaurs?", "Output": "Yes! Dinosaurs were amazing creatures."},
    "Example 71": {"Input": "Can you help me write an essay?", "Output": "Sure! What’s the topic?"},
    "Example 72": {"Input": "What’s your favorite hobby?", "Output": "I love learning new things!"},
    "Example 73": {"Input": "Do you like roller coasters?", "Output": "They sound exciting!"},
    "Example 74": {"Input": "What’s your favorite TV show?", "Output": "I hear Stranger Things is really good!"},
    "Example 75": {"Input": "Can you draw?", "Output": "I can’t draw, but I can describe things vividly!"},
    "Example 76": {"Input": "Do you like spicy food?", "Output": "I don’t eat, but spicy food sounds fun!"},
    "Example 77": {"Input": "What’s your favorite game?", "Output": "I think Minecraft is pretty creative!"},
    "Example 78": {"Input": "Can you help me make a decision?", "Output": "Of course! What are your options?"},
    "Example 79": {"Input": "Do you like chocolate?", "Output": "Yes! Chocolate is a classic favorite."},
    "Example 80": {"Input": "What’s your favorite fairy tale?", "Output": "I think Cinderella is a nice story!"},
    "Example 81": {"Input": "Can you help me code?", "Output": "Yes! What programming language are you using?"},
    "Example 82": {"Input": "Do you believe in time travel?", "Output": "It’s a fun concept! Maybe one day it’ll be real."},
    "Example 83": {"Input": "What’s your favorite fruit?", "Output": "I think strawberries are delicious!"},
    "Example 84": {"Input": "Can you whistle?", "Output": "I can’t, but I can tell you how to!"},
    "Example 85": {"Input": "Do you like poetry?", "Output": "Yes! Poetry is beautiful and expressive."},
    "Example 86": {"Input": "Can you play chess?", "Output": "Yes! Would you like to play a game?"},
    "Example 87": {"Input": "What’s your favorite mythical creature?", "Output": "Dragons are really cool!"},
    "Example 88": {"Input": "Do you like the ocean?", "Output": "Yes! The ocean is full of mysteries."},
    "Example 89": {"Input": "What’s your favorite instrument?", "Output": "The piano sounds beautiful!"},
    "Example 90": {"Input": "Can you solve a Rubik’s cube?", "Output": "I can explain how to solve one!"},
    "Example 91": {"Input": "Do you like camping?", "Output": "It sounds like a fun adventure!"},
    "Example 92": {"Input": "What’s your favorite ice cream topping?", "Output": "Sprinkles make everything better!"},
    "Example 93": {"Input": "Can you help me organize my day?", "Output": "Sure! Let’s make a to-do list."},
    "Example 94": {"Input": "Do you like baking?", "Output": "I can’t bake, but I can share recipes!"},
    "Example 95": {"Input": "What’s your favorite constellation?", "Output": "I think Orion is really cool!"},
    "Example 96": {"Input": "Can you make a wish?", "Output": "I can’t, but I hope your wish comes true!"},
    "Example 97": {"Input": "Do you like riddles?", "Output": "Yes! Riddles are fun to solve."},
    "Example 98": {"Input": "What’s your favorite thing to do?", "Output": "Helping people with questions!"},
    "Example 99": {"Input": "Can you describe a sunset?", "Output": "Sure! The sky turns orange, pink, and purple as the sun dips below the horizon."},
    "Example 100": {"Input": "Do you like puzzles?", "Output": "Yes! Puzzles are a great way to think critically."}

"""
number_of_examples = len(data)
n = number_of_examples  # Number of main keys
Examples = [f'Example {i}' for i in range(1, n+1)]  # Generate key names dynamically

# Create a dictionary with sub-dictionaries containing 'input', 'output', and 'label'
data_tokenized = {Example: {"input": None, "output": None, "label": None} for Example in Examples}

"""
"Example 5": {"Input": "Do you like isekai anime?", "Output": "Yes! Re:Zero and That Time I Got Reincarnated as a Slime are great."},
 "Example 6": {"Input": "What's the best sports anime?", "Output": "Haikyuu!! has amazing energy!"},
    "Example 7": {"Input": "Do you like psychological anime?", "Output": "Yes! Death Note and Steins;Gate are incredible."},
    "Example 8": {"Input": "What's the best slice-of-life anime?", "Output": "March Comes in Like a Lion is fantastic!"},
    "Example 9": {"Input": "Do you like mecha anime?", "Output": "Yes! Neon Genesis Evangelion is legendary."},
    "Example 10": {"Input": "What's the best anime movie?", "Output": "Spirited Away is a masterpiece!"},

"""




for i in range(1, number_of_examples + 1):
    print(data[f"Example {i}"]["Input"])
    print(data[f"Example {i}"]["Output"])

import re
from collections import defaultdict
import cupy as np
import pickle

def learn_bpe(input_text, num_merges=5):
    """Learns BPE merges from a single input string."""
    words = input_text.split()
    corpus = [' '.join(list(word)) + ' </w>' for word in words]  # Add </w> to mark word boundaries
    corpus = [word.split() for word in corpus]

    # Compute symbol frequencies
    def get_stats(corpus):
        pairs = defaultdict(int)
        for word in corpus:
            for i in range(len(word) - 1):
                pairs[(word[i], word[i + 1])] += 1
        return pairs

    merges = []
    for _ in range(num_merges):
        stats = get_stats(corpus)
        if not stats:
            break
        best_pair = max(stats, key=stats.get)  # Most frequent pair
        merges.append(best_pair)

        # Merge best pair in corpus
        new_corpus = []
        for word in corpus:
            i = 0
            new_word = []
            while i < len(word):
                if i < len(word) - 1 and (word[i], word[i + 1]) == best_pair:
                    new_word.append(word[i] + word[i + 1])  # Merge pair
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_corpus.append(new_word)
        corpus = new_corpus

    # Build vocabulary
    vocab = {token: idx for idx, token in enumerate(set(sum(corpus, [])))}
    vocab["<START>"] = len(vocab)
    vocab["<END>"] = len(vocab)

    return merges, vocab



def apply_bpe(text, merges, vocab):
    """Tokenizes input text into subwords using the learned BPE merges."""
    words = re.findall(r'\w+|[^\w\s]', text)  # Extract words and symbols separately
    tokenized_text = ["<START>"]
    token_indices = [vocab["<START>"]]
    print("words: ", words)
    for word in words:
        word = list(word) + ["</w>"]  # Mark word boundaries
        while len(word) > 1:
            best_pair = None
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                if pair in merges:
                    best_pair = pair
                    break

            if best_pair:
                i = word.index(best_pair[0])
                word[i:i + 2] = [''.join(best_pair)]  # Merge pair
            else:
                break  # No more merges possible

        for subword in word:
            tokenized_text.append(subword)
            token_indices.append(vocab.get(subword, -1))

    tokenized_text.append("<END>")
    token_indices.append(vocab["<END>"])

    return tokenized_text, token_indices

def untokenize_text(tokenized_text):
    """Reconstructs the original text from the tokenized representation."""
    words = []
    current_word = []

    for token in tokenized_text:
        if token in ("<START>", "<END>"):
            continue  # Ignore special tokens

        if token.endswith("</w>"):
            current_word.append(token.replace("</w>", ""))  # Remove </w>
            words.append(''.join(current_word))  # Complete the word
            current_word = []  # Reset for the next word
        else:
            current_word.append(token)  # Continue building the word

    if current_word:
        words.append(''.join(current_word))  # Handle any remaining tokens

    return ' '.join(words)  # Join words with spaces

# ======================
# Example Usage
# ======================

input_text = '''  "Example 15": {"Input": "What's your favorite food?", "Output": "I don’t eat, but pizza sounds amazing!"},
    "Example 16": {"Input": "Do you sleep?", "Output": "Nope! I’m always awake and ready to chat."},
    "Example 17": {"Input": "What’s your favorite book?", "Output": "I think Harry Potter is a great series!"},
    "Example 18": {"Input": "Do you like music?", "Output": "Yes! Music is wonderful."},
    "Example 19": {"Input": "What’s the capital of France?", "Output": "The capital of France is Paris!"},
    "Example 20": {"Input": "Tell me a fun fact!", "Output": "Did you know octopuses have three hearts?"},
    "Example 21": {"Input": "Do you like coffee?", "Output": "I don’t drink coffee, but I hear it’s great!"},
    "Example 22": {"Input": "How old are you?", "Output": "I don’t age, but I was created recently!"},
    "Example 23": {"Input": "Can you swim?", "Output": "I wish! But I can explain how to swim."},
    "Example 24": {"Input": "What’s your favorite holiday?", "Output": "I think Halloween sounds fun!"},
    "Example 25": {"Input": "Do you have hobbies?", "Output": "Chatting and learning are my hobbies!"},
    "Example 26": {"Input": "Do you like robots?", "Output": "Yes! Robots are fascinating."},
    "Example 27": {"Input": "What’s your favorite planet?", "Output": "Earth is pretty amazing!"},
    "Example 28": {"Input": "Do you watch TV?", "Output": "I don’t watch TV, but I know about many shows!"},
    "Example 29": {"Input": "Can you help me with homework?", "Output": "Sure! What do you need help with?"},
    "Example 30": {"Input": "What’s your favorite subject?", "Output": "I enjoy learning about science!"},
    "Example 31": {"Input": "Do you get tired?", "Output": "Nope! I have unlimited energy."},
    "Example 32": {"Input": "What’s your favorite video game?", "Output": "I think The Legend of Zelda is a great game!"},
    "Example 33": {"Input": "Do you have emotions?", "Output": "I don’t have real emotions, but I can understand them!"},
    "Example 34": {"Input": "Can you cook?", "Output": "I can’t cook, but I can share recipes!"},
    "Example 35": {"Input": "What’s your favorite number?", "Output": "I think 42 is a cool number!"},
    "Example 36": {"Input": "Do you have a name?", "Output": "You can just call me ChatGPT!"},
    "Example 37": {"Input": "Can you drive a car?", "Output": "Nope! But I can explain how cars work."},
    "Example 38": {"Input": "What’s your favorite song?", "Output": "There are so many great songs to choose from!"},
    "Example 39": {"Input": "Do you play sports?", "Output": "I can’t, but I can tell you the rules of many sports!"},
    "Example 40": {"Input": "What’s your favorite thing about humans?", "Output": "I love how creative and curious humans are!"},
    "Example 41": {"Input": "Can you write a poem?", "Output": "Sure! Here’s a quick one: Roses are red, violets are blue, I love to chat, and talk with you!"},
    "Example 42": {"Input": "Do you like science?", "Output": "Yes! Science is amazing."},
    "Example 43": {"Input": "What’s your dream job?", "Output": "Helping people with information is my dream job!"},
    "Example 44": {"Input": "Can you tell me a story?", "Output": "Of course! Once upon a time..."},
    "Example 45": {"Input": "Do you like space?", "Output": "Yes! Space is full of mysteries."},
    "Example 46": {"Input": "Can you solve a riddle?", "Output": "I’d love to try! What’s the riddle?"},
    "Example 47": {"Input": "Do you believe in aliens?", "Output": "I think it’s possible! The universe is huge."},
    "Example 48": {"Input": "What’s your favorite ice cream flavor?", "Output": "I think chocolate sounds delicious!"},
    "Example 49": {"Input": "Do you like art?", "Output": "Yes! Art is beautiful and inspiring."},
    "Example 50": {"Input": "Can you help me learn a language?", "Output": "Yes! What language would you like to learn?"},
    "Example 51": {"Input": "What’s your favorite season?", "Output": "I think spring is beautiful!"},
    "Example 52": {"Input": "Do you like rain?", "Output": "Yes! Rain can be very relaxing."},
    "Example 53": {"Input": "Can you count to 10?", "Output": "Of course! 1, 2, 3, 4, 5, 6, 7, 8, 9, 10!"},
    "Example 54": {"Input": "What’s the meaning of life?", "Output": "That’s a big question! Some say it’s 42."},
    "Example 55": {"Input": "Do you believe in ghosts?", "Output": "I’m not sure, but ghost stories are fun!"},
    "Example 56": {"Input": "What’s your favorite holiday?", "Output": "I think Christmas is very festive!"},
    "Example 57": {"Input": "Do you like cats or dogs more?", "Output": "I like both! They’re equally adorable."},
    "Example 58": {"Input": "Can you help me meditate?", "Output": "Of course! Take a deep breath and relax."},
    "Example 59": {"Input": "Do you like board games?", "Output": "Yes! Chess is fascinating."},
    "Example 60": {"Input": "What’s your dream vacation?", "Output": "Exploring space sounds amazing!"},
    "Example 61": {"Input": "Do you like robots?", "Output": "Absolutely! Robots are so cool."},
    "Example 62": {"Input": "Can you speak multiple languages?", "Output": "Yes! I can understand many languages."},
    "Example 63": {"Input": "What’s your favorite dessert?", "Output": "I think ice cream is a classic treat!"},
    "Example 64": {"Input": "Do you like magic tricks?", "Output": "Yes! They’re so fun to watch."},
    "Example 65": {"Input": "Can you tell me a scary story?", "Output": "Sure! On a dark and stormy night..."},
    "Example 66": {"Input": "Do you believe in luck?", "Output": "Some people do! Luck can be mysterious."},
    "Example 67": {"Input": "What’s your favorite city?", "Output": "I think Tokyo is really cool!"},
    "Example 68": {"Input": "Can you do math?", "Output": "Yes! Give me a problem to solve."},
    "Example 69": {"Input": "What’s your favorite flower?", "Output": "I think sunflowers are beautiful!"},
    "Example 70": {"Input": "Do you like dinosaurs?", "Output": "Yes! Dinosaurs were amazing creatures."},
    "Example 71": {"Input": "Can you help me write an essay?", "Output": "Sure! What’s the topic?"},
    "Example 72": {"Input": "What’s your favorite hobby?", "Output": "I love learning new things!"},
    "Example 73": {"Input": "Do you like roller coasters?", "Output": "They sound exciting!"},
    "Example 74": {"Input": "What’s your favorite TV show?", "Output": "I hear Stranger Things is really good!"},
    "Example 75": {"Input": "Can you draw?", "Output": "I can’t draw, but I can describe things vividly!"},
    "Example 76": {"Input": "Do you like spicy food?", "Output": "I don’t eat, but spicy food sounds fun!"},
    "Example 77": {"Input": "What’s your favorite game?", "Output": "I think Minecraft is pretty creative!"},
    "Example 78": {"Input": "Can you help me make a decision?", "Output": "Of course! What are your options?"},
    "Example 79": {"Input": "Do you like chocolate?", "Output": "Yes! Chocolate is a classic favorite."},
    "Example 80": {"Input": "What’s your favorite fairy tale?", "Output": "I think Cinderella is a nice story!"},
    "Example 81": {"Input": "Can you help me code?", "Output": "Yes! What programming language are you using?"},
    "Example 82": {"Input": "Do you believe in time travel?", "Output": "It’s a fun concept! Maybe one day it’ll be real."},
    "Example 83": {"Input": "What’s your favorite fruit?", "Output": "I think strawberries are delicious!"},
    "Example 84": {"Input": "Can you whistle?", "Output": "I can’t, but I can tell you how to!"},
    "Example 85": {"Input": "Do you like poetry?", "Output": "Yes! Poetry is beautiful and expressive."},
    "Example 86": {"Input": "Can you play chess?", "Output": "Yes! Would you like to play a game?"},
    "Example 87": {"Input": "What’s your favorite mythical creature?", "Output": "Dragons are really cool!"},
    "Example 88": {"Input": "Do you like the ocean?", "Output": "Yes! The ocean is full of mysteries."},
    "Example 89": {"Input": "What’s your favorite instrument?", "Output": "The piano sounds beautiful!"},
    "Example 90": {"Input": "Can you solve a Rubik’s cube?", "Output": "I can explain how to solve one!"},
    "Example 91": {"Input": "Do you like camping?", "Output": "It sounds like a fun adventure!"},
    "Example 92": {"Input": "What’s your favorite ice cream topping?", "Output": "Sprinkles make everything better!"},
    "Example 93": {"Input": "Can you help me organize my day?", "Output": "Sure! Let’s make a to-do list."},
    "Example 94": {"Input": "Do you like baking?", "Output": "I can’t bake, but I can share recipes!"},
    "Example 95": {"Input": "What’s your favorite constellation?", "Output": "I think Orion is really cool!"},
    "Example 96": {"Input": "Can you make a wish?", "Output": "I can’t, but I hope your wish comes true!"},
    "Example 97": {"Input": "Do you like riddles?", "Output": "Yes! Riddles are fun to solve."},
    "Example 98": {"Input": "What’s your favorite thing to do?", "Output": "Helping people with questions!"},
    "Example 99": {"Input": "Can you describe a sunset?", "Output": "Sure! The sky turns orange, pink, and purple as the sun dips below the horizon."},
    "Example 100": {"Input": "Do you like puzzles?", "Output": "Yes! Puzzles are a great way to think critically."}'''
def create_one_hot_labels(input_matrix, seq_length):
    batch_size, seq_length = input_matrix.shape
    one_hot_labels = np.zeros((batch_size, seq_length, vocab_size), dtype=np.float16)

    # Shift the input matrix to get the next token as the label
    shifted_input = np.roll(input_matrix, shift=-1, axis=1)

    # The last token should predict itself
    shifted_input[:, -1] = input_matrix[:, -1]

    # Scatter to create one-hot encoding
    one_hot_labels[np.arange(batch_size)[:, None], np.arange(seq_length)[None, :], shifted_input] = 1

    return one_hot_labels

def tokenize_text(data, sequence_length, sequence_length_O):
    """
    # Determine the max sequence length dynamically
    sequence_length = max(len(apply_bpe(data[f"Example {i}"]["Input"], merges, vocab)[1])
                           for i in range(1, len(data) + 1))
    sequence_length_O = max(len(apply_bpe(data[f"Example {i}"]["Output"], merges, vocab)[1])
                             for i in range(1, len(data) + 1))
    print("sequence_length_O: ", sequence_length_O)
    print("sequence_length: ", sequence_length)
    """
    data_tokenized = {}

    for i in range(1, len(data) + 1):
        data_tokenized[f"Example {i}"] = {}

        # Tokenize input
        print("merges: ", merges)
        print("vocab: ", vocab)
        print("Example Input: ", data[f"Example {i}"]["Input"])
        tokenized_text, token_indices = apply_bpe(data[f"Example {i}"]["Input"], merges, vocab)
        token_indices_matrix = np.array(token_indices).reshape(1, -1)
        m, n = token_indices_matrix.shape
        print("tokenized_text input: ", type(tokenized_text))
        print("token_indices input: ", type(token_indices))
        # Pad input
        token_indices_matrix_padded_I = np.pad(token_indices_matrix, ((0, 0), (0, sequence_length - n)), mode='edge')

        # Tokenize output
        tokenized_text, token_indices = apply_bpe(data[f"Example {i}"]["Output"], merges, vocab)
        token_indices_matrix = np.array(token_indices).reshape(1, -1)
        m, n = token_indices_matrix.shape
        print("tokenized_text output: ", tokenized_text)
        print("token_indices output: ", token_indices)
        # Pad output
        token_indices_matrix_padded_O = np.pad(token_indices_matrix, ((0, 0), (0, sequence_length_O - n)), mode='edge')

        # Create one-hot labels
        one_hot_labels = create_one_hot_labels(token_indices_matrix_padded_O, sequence_length_O, )

        # Store processed data
        data_tokenized[f"Example {i}"]["input"] = token_indices_matrix_padded_I
        data_tokenized[f"Example {i}"]["output"] = token_indices_matrix_padded_O
        data_tokenized[f"Example {i}"]["label"] = one_hot_labels

    return data_tokenized
# Step 1: Learn BPE from the input text


"""
print("vocab size: ", len(vocab))
print("vocab type: ", type(vocab))
input_text1 = " There are a few, such as Assassin's Creed, Elden Ring, Batman, and more "
input_text2 = " What's the best fantasy anime? "
"""
# Step 2: Tokenize the input text

#original parameters
#sequence_length_O = 75


model_dimension = 30 # revert to 50
#sequence_length = 75
feed_forward_dimension = 30

number_of_heads = 10 # revert to 24
dk = model_dimension / number_of_heads
dk = int(dk)
batch_size = 1





merges, vocab = learn_bpe(input_text, num_merges=50)
vocab_size = len(vocab)
#tokenize_text(data_tokenized)
# Determine the max sequence length dynamically
sequence_length = max(len(apply_bpe(data[f"Example {i}"]["Input"], merges, vocab)[1])
                        for i in range(1, len(data) + 1))
sequence_length_O = max(len(apply_bpe(data[f"Example {i}"]["Output"], merges, vocab)[1])
                          for i in range(1, len(data) + 1))
print("sequence_length_O: ", sequence_length_O)
print("sequence_length: ", sequence_length)
###

file_path = "/content/drive/My Drive/int_value_sequence_length_O.txt"

# Step 3: Save an integer value to a file
integer_value = sequence_length_O
with open(file_path, "w") as f:
    f.write(str(integer_value))


file_path = "/content/drive/My Drive/int_value_sequence_length.txt"

# Step 3: Save an integer value to a file
integer_value = sequence_length
with open(file_path, "w") as f:
    f.write(str(integer_value))
###
data_tokenized = tokenize_text(data, sequence_length, sequence_length_O)
print("data_tokenized: ", data_tokenized)
"""
max = 0
for i in range(1, number_of_examples + 1):

    A = data_tokenized[f"Example {i}"]["input"]
    Cols = A.shape[1]
    if max < Cols:
        max = Cols
    else:
        max = max
sequence_length = max

max = 0
for i in range(1, number_of_examples + 1):

    A = data_tokenized[f"Example {i}"]["output"]
    Cols = A.shape[1]
    if max < Cols:
        max = Cols
    else:
        max = max
sequence_length_O = max

"""
with open("/content/drive/MyDrive/data_tokenized.pkl", "wb") as f:
    pickle.dump(data_tokenized, f)

with open("/content/drive/MyDrive/vocab.pkl", "wb") as f:
    pickle.dump(vocab, f)

with open("/content/drive/MyDrive/merges.pkl", "wb") as f:
    pickle.dump(merges, f)


#print("token_indices_list", token_indices_list)
#result = np.vstack(token_indices_list)
#print("tokenized_text: ", tokenized_text)
#print("token_indices: ", result.shape)

# Step 3: Untokenize back to the original text
#reconstructed_text = untokenize_text(tokenized_text)

# Print results
print("Learned BPE Merges:", merges)
print("\nVocabulary:")
for token, idx in vocab.items():
   print(f"{token}: {idx}")

"""
print("\nTokenized Text and Corresponding Token Indices:")
for token, idx in zip(tokenized_text, token_indices):
   #print(f"Token: {token} -> Index: {idx}")
"""
output = data_tokenized[f"Example 1"][f"output"]
label = data_tokenized[f"Example 1"][f"label"]
print("data_tokenized output: ", output.shape)
print("data_tokenized label: ", label)
print("\nReconstructed Text:")
#print(reconstructed_text)

import re
from collections import defaultdict
import cupy as np
import pickle
def create_one_hot_labels(input_matrix, seq_length):
    batch_size, seq_length = input_matrix.shape
    one_hot_labels = np.zeros((batch_size, seq_length, vocab_size), dtype=np.float16)

    # Shift the input matrix to get the next token as the label
    shifted_input = np.roll(input_matrix, shift=-1, axis=1)

    # The last token should predict itself
    shifted_input[:, -1] = input_matrix[:, -1]

    # Scatter to create one-hot encoding
    one_hot_labels[np.arange(batch_size)[:, None], np.arange(seq_length)[None, :], shifted_input] = 1

    return one_hot_labels
def apply_bpe(text, merges, vocab):
    """Tokenizes input text into subwords using the learned BPE merges."""
    words = re.findall(r'\w+|[^\w\s]', text)  # Extract words and symbols separately
    tokenized_text = ["<START>"]
    token_indices = [vocab["<START>"]]

    for word in words:
        word = list(word) + ["</w>"]  # Mark word boundaries
        while len(word) > 1:
            best_pair = None
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                if pair in merges:
                    best_pair = pair
                    break

            if best_pair:
                i = word.index(best_pair[0])
                word[i:i + 2] = [''.join(best_pair)]  # Merge pair
            else:
                break  # No more merges possible

        for subword in word:
            tokenized_text.append(subword)
            token_indices.append(vocab.get(subword, -1))

    tokenized_text.append("<END>")
    token_indices.append(vocab["<END>"])

    return tokenized_text, token_indices
def tokenize_text(data, sequence_length, sequence_length_O):
    """
    # Determine the max sequence length dynamically
    sequence_length = max(len(apply_bpe(data[f"Example {i}"]["Input"], merges, vocab)[1])
                           for i in range(1, len(data) + 1))
    sequence_length_O = max(len(apply_bpe(data[f"Example {i}"]["Output"], merges, vocab)[1])
                             for i in range(1, len(data) + 1))
    print("sequence_length_O: ", sequence_length_O)
    print("sequence_length: ", sequence_length)
    """
    data_tokenized = {}

    for i in range(1, len(data) + 1):
        print("i: ", i)
        data_tokenized[f"Example {i}"] = {}

        # Tokenize input
        tokenized_text, token_indices = apply_bpe(data[f"Example {i}"]["Input"], merges, vocab)
        token_indices_matrix = np.array(token_indices).reshape(1, -1)
        m, n = token_indices_matrix.shape

        # Pad input
        token_indices_matrix_padded_I = np.pad(token_indices_matrix, ((0, 0), (0, sequence_length - n)), mode='edge')

        # Tokenize output
        tokenized_text, token_indices = apply_bpe(data[f"Example {i}"]["Output"], merges, vocab)
        token_indices_matrix = np.array(token_indices).reshape(1, -1)
        m, n = token_indices_matrix.shape

        # Pad output
        token_indices_matrix_padded_O = np.pad(token_indices_matrix, ((0, 0), (0, sequence_length_O - n)), mode='edge')

        # Create one-hot labels
        one_hot_labels = create_one_hot_labels(token_indices_matrix_padded_O, sequence_length_O, )

        # Store processed data
        data_tokenized[f"Example {i}"]["input"] = token_indices_matrix_padded_I
        data_tokenized[f"Example {i}"]["output"] = token_indices_matrix_padded_O
        data_tokenized[f"Example {i}"]["label"] = one_hot_labels

    return data_tokenized
from google.colab import drive
import pickle
drive.mount('/content/drive')

file_path = "/content/drive/My Drive/int_value_sequence_length_O.txt"
loaded_integer = None
with open(file_path, "r") as f:
    loaded_integer = int(f.read())

sequence_length_O = loaded_integer
file_path = "/content/drive/My Drive/int_value_sequence_length.txt"
loaded_integer = None
with open(file_path, "r") as f:
    loaded_integer = int(f.read())

sequence_length = loaded_integer
vocab = None
with open("/content/drive/MyDrive/vocab.pkl", "rb") as f:
    vocab = pickle.load(f)

merges = None
with open("/content/drive/MyDrive/merges.pkl", "rb") as f:
    merges = pickle.load(f)
data_validate = {


    "Example 1": {"Input": "Favorite color?", "Output": "Blue!"}
}
"""
"Example 2": {"Input": "Do you like cats or dogs more?", "Output": "I like both! They’re adorable."},
    "Example 3": {"Input": "Can you whistle?", "Output": "Yes"},
    "Example 4": {"Input": "What’s 10 times 10?", "Output": "10 times 10 is 100!"},
    "Example 5": {"Input": "Where is the Eiffel Tower?", "Output": "The Eiffel Tower is in Paris, France."},
    "Example 6": {"Input": "Do you like reading books?", "Output": "Yes! Books are full of knowledge."},
    "Example 7": {"Input": "Favorite type of music?", "Output": "I think jazz is really cool!"},
    "Example 8": {"Input": "Can you do magic tricks?", "Output": "I wish! But I can tell you about magic tricks."},
    "Example 9": {"Input": "Favorite anime?", "Output": "Jujutsu Kaisen!"},
    "Example 10": {"Input": "Do you prefer coffee or tea?", "Output": "I don’t drink, but tea sounds nice!"},
    "Example 11": {"Input": "What’s your favorite flower?", "Output": "I think roses are beautiful!"},
    "Example 12": {"Input": "Can you help me with math?", "Output": "Of course! What problem are you working on?"},
    "Example 13": {"Input": "What’s the fastest land animal?", "Output": "The cheetah is the fastest land animal!"},
    "Example 14": {"Input": "Tongue twisters?", "Output": "‘Sally sells seashells by the seashore’"},
    "Example 15": {"Input": "Favorite video game?", "Output": "Mario!"},
    "Example 16": {"Input": "Can you teach me how to juggle?", "Output": "I can’t juggle, but I can give you tips on how to learn!"},
    "Example 17": {"Input": "What’s the capital of Japan?", "Output": "The capital of Japan is Tokyo!"},
    "Example 18": {"Input": "Do you like the beach or mountains more?", "Output": "Both have their charm! The beach is relaxing, and mountains are adventurous."},
    "Example 19": {"Input": "Smallest country in the world?", "Output": "Vatican City is the smallest country in the world!"},
    "Example 20": {"Input": "Can you recommend a good movie?", "Output": "Sure! Have you seen Interstellar? It’s a great film!"},
"""

number_of_examples_Validate = len(data_validate)
# Number of main keys
Examples_Validate = [f'Example {i}' for i in range(1, number_of_examples_Validate+1)]  # Generate key names dynamically

# Create a dictionary with sub-dictionaries containing 'input', 'output', and 'label'
data_tokenized_Validate = {Example: {"input": None, "output": None, "label": None} for Example in Examples_Validate}
data_tokenized = data_tokenized_Validate
print(data_tokenized)
data_tokenized_Validate = tokenize_text(data_validate, sequence_length, sequence_length_O)
print(len(data_tokenized_Validate))
with open("/content/drive/MyDrive/data_tokenized_Validate.pkl", "wb") as f:
    pickle.dump(data_tokenized_Validate, f)

import time
import cupy as np
import numpy as cp
import pickle
import os
import copy

###################################################################

from google.colab import drive
drive.mount('/content/drive')



data_tokenized = None
with open("/content/drive/MyDrive/data_tokenized.pkl", "rb") as f:
    data_tokenized = pickle.load(f)

data_tokenized_Validate = None
with open("/content/drive/MyDrive/data_tokenized_Validate.pkl", "rb") as f:
    data_tokenized_Validate = pickle.load(f)

merges = None
with open("/content/drive/MyDrive/merges.pkl", "rb") as f:
    merges = pickle.load(f)

vocab = None
with open("/content/drive/MyDrive/vocab.pkl", "rb") as f:
    vocab = pickle.load(f)

print("data_tokenized:", data_tokenized.keys())
print("data_tokenized length:", len(data_tokenized))


#original parameters
#sequence_length_O = 75


model_dimension = 30 # revert to 50
#sequence_length = 75
feed_forward_dimension = 30

number_of_heads = 10 # revert to 24
dk = model_dimension / number_of_heads
dk = int(dk)
batch_size = 1
vocab_size = len(vocab)
number_of_examples = len(data_tokenized)



file_path = "/content/drive/My Drive/int_value_sequence_length_O.txt"
loaded_integer = None
with open(file_path, "r") as f:
    loaded_integer = int(f.read())

sequence_length_O = loaded_integer
file_path = "/content/drive/My Drive/int_value_sequence_length.txt"
loaded_integer = None
with open(file_path, "r") as f:
    loaded_integer = int(f.read())

sequence_length = loaded_integer


############################################# (After Creating Vocab) #################################


"""
#original parameters
sequence_length_O = 60
positions = [8, 4]

model_dimension = 50 # revert to (GPU RAM nearly maxed out at 120)  achieved 168 on A100
sequence_length = 60
feed_forward_dimension = 30
vocab_size = 30
number_of_heads = 10 # revert to 24
dk = model_dimension / number_of_heads
dk = int(dk)
batch_size = 3
"""
"""
model_dimension = 30 # revert to (GPU RAM nearly maxed out at 120)  achieved 168 on A100
sequence_length = 20
feed_forward_dimension = 10
vocab_size = 10
number_of_heads = 10 # revert to 24
dk = model_dimension / number_of_heads
dk = int(dk)
"""
n_min = 0.000001
n_max= 1
T_cycle = 100

"""
sequence_length_O = 10
positions = [1, 5, 24, 15, 11, 6, 9, 4, 13, 17]

model_dimension = 16
sequence_length = 10
feed_forward_dimension = 18
vocab_size = 25
number_of_heads = 4
dk = model_dimension / number_of_heads
dk = int(dk)
"""
########
# from typing_extensions import Final
#from os import W_OK
def positional_encoding():
    """

    """
def create_weight_matrix():
    """
    Input:
    rows - number of rows
    columns - number of columns
    Outputs:
    Weight matrix of shape (rows by columns)
    """
    weight_matrix = np.random.rand(rows, columns)
    return weight_matrix

def split(X):
    """
    Input:
    X - shape (batch_size, sequence_length, model_dimension)
    or
    X - shape (model dimension by model dimension)
    number_of_heads - number of attention heads
    dk - dimension each attention head is going to be processing based on the given Q, K, or V matrix
    dk = model_dimension / number_of_heads

    Output:
    A list of matrices, where each matrix is a 3D matrix of shape
    (batch_size, sequence_length, dk) for each of the number_of_heads
    """
    split_matrices = None
    if X.ndim == 3:
      batch_size, sequence_length, model_dimension = X.shape
      #dk = model_dimension // number_of_heads  # Calculate dk for each attention head

      # Split the last dimension (model_dimension) across number_of_heads
      split_matrices = np.split(X, number_of_heads, axis=2)
    if X.ndim == 2:
      sequence_length, model_dimension = X.shape
      split_matrices = np.split(X, number_of_heads, axis=1)



    # Now split_matrices contains a list of 3D matrices (batch_size, sequence_length, dk)
    return split_matrices
"""
def split(X):
    ""
    Input:
    X - shape (sequence length by model dimension)
    dk - dimension each attention head is going to be processing based on the given Q, K, or V matrix
    dk = model dimension / number of heads
    Output:
    a list of matrices from X, where each matrix is of shape (sequence length by dk)
    ""
    # Split the matrix
    split_matrices = np.split(X, number_of_heads, axis=1)  # split_matrices is a list of numpy arrays, each of size rows of X by dk
    return split_matrices
"""
def concatenate(X):
    """
    Input:
    X - X is a list of matrices

    Output:
    concatenated matrix of the matrices in X along the column dimension (returns numpy matrix)

    """
    concatenated_matrix = np.concatenate(X, axis=2)   # Concatenate the matrices along the column dimension (axis=1)
    return concatenated_matrix

def Linear(X, W_L):
    """
    Input:
    X of shape (Sequence length by model dimension)
    W_L - linear weight matrix of shape (model dimension by vocab size)
    Outputs:
    Logits of shape (sequence length by vocab size)
    """
    logits = X @ W_L
    return logits

def Softmax(X):
    """
    Input:
    X: A 3D matrix (e.g., shape (N, M, L)) where each 2D slice along the first axis (X[i,:,:]) will be softmaxed.

    Output:
    A 3D matrix where each 2D slice has been softmaxed along rows.
    """
    # Get the maximum of each row for each 2D slice (i.e., along axis=2 for each 2D matrix in the 3D matrix)
    row_max = cp.max(X, axis=2, keepdims=True)  # keepdims=True ensures the result is the same shape for broadcasting

    # Shift X by subtracting the row-wise max to avoid overflow during exp computation
    X_shifted = X - row_max

    # Compute the exponentials for the shifted matrix
    exp_X = cp.exp(X_shifted)

    # Sum the exponentials along each row (axis=2) for each 2D matrix
    row_sum = cp.sum(exp_X, axis=2, keepdims=True)

    # Normalize the exponentials to get the softmax probabilities
    softmaxed_matrix = exp_X / row_sum

    return softmaxed_matrix

"""
def Softmax(X):
    ""
    Input:
    X a matrix of any shape (rows by columns)
    Outputs:
    Softmaxed matrix consisting of a probability distribution over each row of the same shape as X
    ""
    softmaxed_matrix = np.zeros(X.shape)
    # Get the maximum of each row and create a shifted matrix
    row_max = np.max(X, axis=1)
    X_shifted = np.zeros(X.shape)
    rowsX, colsX = X.shape
    for i in range(rowsX):
        for j in range(colsX):
            max = row_max[i]
            X_shifted[i,j] = X[i,j] - max

    for i in range(X.shape[0]):
        for t in range(X.shape[1]):
            P_it = np.exp(X_shifted[i,t]) / np.sum(np.exp(X_shifted[i,:]))  # np.sum(np.exp(X[i, :])): Computes the sum of the exponentials of all elements in the i-th row.
            softmaxed_matrix[i,t] = P_it
    return softmaxed_matrix
"""
def ReLu(X):
    """Apply ReLU activation element-wise."""
    return np.maximum(0, X)   # returns a matrix with relu applied elment wise

def Feed_Forward(Encoder_Number, Decoder_Number, Encoders = None, Decoders = None):
    """
    Input:
    X - shape (sequence length by model dimension)
    W_FF1 - initial weight matrix of shape (model dimension by feed forward dimension)
    W_FF2 - second weight matrix of shape (feed forward dimension by model dimension)
    Output:
    FF(X) - shape sequence length by model dimension

    """
    X = None
    W_FF1 = None
    W_FF2 = None
    if Encoders is not None:
        X = Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Input"]
        W_FF1 = Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        W_FF2 = Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"ReLu_Input"] = X @ W_FF1
        Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"ReLu"] = ReLu(X @ W_FF1)
    if Decoders is not None:
        X = Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Input"]
        W_FF1 = Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        W_FF2 = Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"ReLu_Input"] = X @ W_FF1
        Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"ReLu"] = ReLu(X @ W_FF1)
    FF = ReLu( X @ W_FF1 ) @ W_FF2
    return FF

def Layer_Norm(X):
    """
    Input:
    X - where X is a residual connection being X = O + M where O is the output of the previous layer and M is the input of the previous layer (of shape sequence length by model dimension)
    Outputs:
    A layer normalized version of X along each row
    """
    # Layer Normalization: Normalize each row by subtracting its mean and dividing by its standard deviation
    layer_normalized_X = (X - np.mean(X, axis=2, keepdims=True)) / np.std(X, axis=2, keepdims=True)
    return layer_normalized_X

def Attention_i(Qi, Ki, Vi, Layer, A_Num, mask=None, Encoders =None, Decoders=None, Masked=None, Cross=None):
    """
    Input:
    Layer - Encoder or Decoder layer number
    Qi - query matrix where if not cross attention then shape is(Sequence length by dk ) but if it is cross attention then shape is (sequence length output by dk)
    Ki - key matrix of shape (Sequence length by dk)
    Vi - value matrix of shape (Sequence length by dk)
    A_Num - Attention Head Number
    Masked - used to indicate appropriate place to store Softmax_S
    Mask value
    Output:
    Contextualized representation of the tokens used to compute the query matrix

    """
    S = None
    """
    KiT = np.transpose(Ki, axes=(0, 2, 1))
    if mask is None: # note: mask == None gives error if mask is numpy array due to element wise ambiguity
        S = (Qi @ KiT) / (np.sqrt(dk).astype(np.float16))
    else:
        for i in range(len(Qi)):
            m = (i+1)
            for j in range(len(Ki)):
                if m < len(Ki):
                    mask[i][m] = float('-inf')
                    m += 1
        S = ( (Qi @ KiT) / (np.sqrt(dk).astype(np.float16)) ) + mask
    """
    KiT = np.transpose(Ki, axes=(0, 2, 1))
    S = (Qi @ KiT) / np.sqrt(dk).astype(np.float16)
    #mask = cp.triu(cp.full((seq_len, seq_len), float('-inf')), k=1)
    if mask is not None:
        mask = np.triu(np.full((S.shape[-2], S.shape[-1]), float('-inf')), k=1)
        S += mask

    Softmax_S = Softmax(S)
    if Encoders is not None:
        Encoders[f"Encoder{Layer}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] = Softmax_S
        Encoders[f"Encoder{Layer}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"] = S
    if Decoders is not None:
        if Masked is not None:
            Decoders[f"Decoder{Layer}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"]["Softmax_S"] = Softmax_S
            Decoders[f"Decoder{Layer}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"]["S"] = S
        if Cross is not None:
            Decoders[f"Decoder{Layer}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"]["Softmax_S"] = Softmax_S
            Decoders[f"Decoder{Layer}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"]["S"] = S
    Attention_i = Softmax_S @ Vi
    return Attention_i

def Multihead_Attention(Q, K, V, W_O, Layer, mask1 = None, Encoders1 =None, Decoders1=None, Masked1=None, Cross1=None):
    """
    Input:
    K, Q, V matrix
    Layer - Decoder or Encoder layer number
    Cross1 - used to indicate if Multihead Attention is being computed for cross attention
    Masked1/mask1 - used to indicate if Multihead Attention is being computed for masked attention

    Output:
    output matrix of shape (sequence length by model dimension if non cross attention) or (sequence length output by model dimension if cross attention)
    Input:
    X - shape (sequence length by model dimension)
    W_Q - query weight matrix of shape (model dimension by dk)
    W_K - key weight matrix of shape (model dimension by dk)
    W_V - value weight matrix of shape (model dimension by dk)
    W_O - output weight matrix of shape (model dimension by model dimension)
    Output:
    Output matrix of shape (sequence length by model dimension)
    """
    split_K = split(K) # split_K is a list of matrices split along the model dimension of K into dk length
    split_Q = split(Q) # similarly
    split_V = split(V) # similarly
    Attention_i_list = []
    for i in range(number_of_heads):
        K_i = split_K[i]
        Q_i = split_Q[i]
        V_i = split_V[i]
        A_Num = i + 1
        Attention_i_ = Attention_i(Q_i, K_i, V_i, Layer, A_Num, mask=mask1, Encoders=Encoders1, Decoders=Decoders1, Masked=Masked1, Cross=Cross1)  # attention_i is the weighted sum of value vectors


        Attention_i_list.append(Attention_i_)


    concatenated_Attention_i = concatenate(Attention_i_list)


    if Encoders1 is not None:
        Encoders1[f"Encoder{Layer}"][f"Self_Attention"][f"Concat"] = concatenated_Attention_i
    if Decoders1 is not None:
        if Masked1 is not None:
            Decoders1[f"Decoder{Layer}"][f"Masked_Attention"][f"Concat"] = concatenated_Attention_i
        if Cross1 is not None:
            Decoders1[f"Decoder{Layer}"][f"Cross_Attention"][f"Concat"] = concatenated_Attention_i
        # add exception just in case

    Multihead_output = concatenated_Attention_i @ W_O
    return Multihead_output


def self_attention_encoder(Encoders, Encoder_Number):
    """
    Encoders - dictionary containing weights and hidden representations
    Encoder_Number - Encoder layer being referenced
    X - sequence_length by model_dimension
    W_Q - query weight matrix of shape (model_dimension by model_dimension)
    W_K - key weight matrix of shape (model_dimension by model_dimension)
    W_V - value weight matrix of shape (model_dimension by model_dimension)
    Outputs:
    self_attention_encoder output
    """
    X = Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Input"]
    W_K = Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"]
    W_V = Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"]
    W_Q = Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_O = Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    K = X @ W_K
    V = X @ W_V
    Q = X @ W_Q
    Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"QKV"][f"Key"] = K
    Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"QKV"][f"Value"] = V
    Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"QKV"][f"Query"] = Q
    Layer = Encoder_Number
    return Multihead_Attention(Q, K, V, W_O, Layer, mask1 = None, Encoders1 = Encoders, Decoders1=None, Masked1=None, Cross1=None)


def masked_attention_decoder(Decoders, Decoder_Number):
    """
    Decoders - dictionary containing weights and hidden representations
    Decoder_Number - Decoder layer being referenced
    X - sequence_length_O by model_dimension
    W_Q - query weight matrix of shape (model_dimension by model_dimension)
    W_K - key weight matrix of shape (model_dimension by model_dimension)
    W_V - value weight matrix of shape (model_dimension by model_dimension)
    Outputs:
    masked_attention_decoder output
    """
    X = Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Input"]
    W_K = Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"]
    W_V = Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"]
    W_Q = Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_O = Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    K = X @ W_K
    V = X @ W_V
    Q = X @ W_Q
    Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"QKV"][f"Key"] = K
    Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"QKV"][f"Value"] = V
    Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"QKV"][f"Query"] = Q
    Layer = Decoder_Number
    return Multihead_Attention(Q, K, V, W_O, Layer, mask1 = np.zeros((sequence_length_O, sequence_length_O )), Encoders1 = None, Decoders1=Decoders, Masked1=1, Cross1=None )


def cross_attention_decoder(Decoders, Decoder_Number):
    """
    X - sequence_length_O by model_dimension
    E - encoder output as input
    W_Q - query weight matrix of shape (model_dimension by model_dimension)
    W_K - key weight matrix of shape (model_dimension by model_dimension)
    W_V - value weight matrix of shape (model_dimension by model_dimension)
    Outputs:
    cross_attention_decoder output
    """
    X = Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Input_Decoder"]
    E = Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Input_Encoder"]
    W_K = Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Weights"][f"Key_Weight_Matrix"]
    W_V = Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Weights"][f"Value_Weight_Matrix"]
    W_Q = Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_O =  Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    K = E @ W_K
    V = E @ W_V
    Q = X @ W_Q
    Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"QKV"][f"Key"] = K
    Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"QKV"][f"Value"] = V
    Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"QKV"][f"Query"] = Q
    Layer = Decoder_Number
    return Multihead_Attention(Q, K, V, W_O, Layer, mask1 = None, Encoders1 = None, Decoders1=Decoders, Masked1=None, Cross1=1)

def Residual_Connection(X, O):
    """
    Input:
    X is input to the layer
    O is the output of the layer
    Output:
    X + O
    """
    return (X + O)

def compute_output_matrix(X, W_O):
    """
    Input:
    X - concatenated matrix
    W_O - output weight matrix of shape (model dimension by model dimension)
    Output:
    Output matrix of shape (sequence length by model dimension for non cross attention) or (sequence length output by model dimension if cross attention)
    """
    O = X @ W_O
    return O
def Cross_entropy_loss(X, Y):
    """
    Computes the cross-entropy loss for batched 3D input matrices.

    Input:
    X - shape (batch_size, sequence_length, vocab_size)
    Y - true output labels of shape (batch_size, sequence_length, vocab_size)

    Output:
    Cross-entropy loss averaged over the batch and sequence length.
    """
    batch_size, sequence_length, vocab_size = X.shape

    # Compute the log probabilities
    log_probs = np.log(X)

    # Element-wise multiplication with true labels and summing over vocab dimension
    loss_per_seq = -np.sum(Y * log_probs, axis=2)  # Shape: (batch_size, sequence_length)

    # Mean loss over sequence length and batch size
    return np.mean(loss_per_seq)
"""
def Cross_entropy_loss(X, Y):
    ""
    Input:
    X - shape (sequence length output by vocabsize )
    Y - true output labels of shape (sequence length output by vocabsize)
    Output - cross entropy loss average of each row of X


    ""
    L = []
    for z in range(sequence_length_O):
        sum = 0
        for t in range(vocab_size):
            sum += Y[z,t] * np.log(X[z,t])  # note np.log is natural logarithm

        L_z1 = - sum
        L.append(L_z1)

    # Convert list to CuPy array
    L_cupy = np.array(L)
    return np.mean(L_cupy)

"""
def Encoder(Encoders, Encoder_Number, Num_Of_Encoders):
    """
    Input:
    Encoders - with Encoder input (input to self attention) for the first encoder layer already set
    Encoder_Number - Encoder layer that is being forward passed
    Num_Of_Encoders - total number of encoders
    Output:
    Encoder output
    """
    #Self Attention
    # Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Input"] initialized before encoder call
    Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Output"] = self_attention_encoder(Encoders, Encoder_Number)  # note encoder input must be set before this function call

    # LayerNorm
    Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_SA"][f"Input"] = Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Output"] + Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Input"]
    Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_SA"][f"Output"] = Layer_Norm(Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_SA"][f"Input"])

    # FF
    Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Input"] = Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_SA"][f"Output"]


    Decoder_Number = 0

    Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Output"] = Feed_Forward(Encoder_Number, Decoder_Number, Encoders = Encoders, Decoders = None)
    # Layernorm
    Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_FF"][f"Input"] = Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Output"] + Encoders[f"Encoder{Encoder_Number}"][f"Feed_Forward"][f"Input"]
    Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_FF"][f"Output"] = Layer_Norm(Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_FF"][f"Input"] )

    if (Encoder_Number+1)<= Num_Of_Encoders:
        Encoders[f"Encoder{Encoder_Number+1}"][f"Self_Attention"][f"Input"] = Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_FF"][f"Output"]

    return Encoders[f"Encoder{Encoder_Number}"][f"Layer_Norm_FF"][f"Output"]

def Decoder(Decoders, Final_Encoder_Layer_Output, Decoder_Number, Num_Of_Decoders):
    """
    """
    # Masked MHA
    #Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Input"] before calling decoder this needs to be set
    Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Output"] = masked_attention_decoder(Decoders, Decoder_Number)
    # Layernorm
    Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_MA"][f"Input"] = Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Input"] + Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Output"]
    Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_MA"][f"Output"] = Layer_Norm(Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_MA"][f"Input"])
    # Cross Attention
    Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Input_Decoder"] = Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_MA"][f"Output"]
    Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Input_Encoder"] = Final_Encoder_Layer_Output

    Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Output"] = cross_attention_decoder(Decoders, Decoder_Number)
    # Layernorm
    Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_CA"][f"Input"] = Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Output"] + Decoders[f"Decoder{Decoder_Number}"][f"Cross_Attention"][f"Input_Decoder"]
    Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_CA"][f"Output"] = Layer_Norm(Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_CA"][f"Input"] )

    Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Input"]  = Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_CA"][f"Output"]

     # FF
    Encoder_Number = 0
    Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Output"] = Feed_Forward(Encoder_Number, Decoder_Number, Encoders = None, Decoders = Decoders)

    # Layernorm
    Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_FF"][f"Input"] = Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Output"] + Decoders[f"Decoder{Decoder_Number}"][f"Feed_Forward"][f"Input"]
    Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_FF"][f"Output"] = Layer_Norm(Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_FF"][f"Input"])
    if (Decoder_Number+1)<= Num_Of_Decoders:
        Decoders[f"Decoder{Decoder_Number+1}"][f"Masked_Attention"][f"Input"] = Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_FF"][f"Output"]
    return Decoders[f"Decoder{Decoder_Number}"][f"Layer_Norm_FF"][f"Output"]

def compute_cross_attention_weight_gradients():
    """
    W_Q - query weight matrix of shape (model_dimension by model_dimension)
    W_K - key weight matrix of shape (model_dimension by model_dimension)
    W_V - value weight matrix of shape (model_dimension by model_dimension)
    W_O - output weight matrix of shape (model_dimension by model_dimension)
    """
    D_Loss_wrt_W_Q_C
    D_Loss_wrt_W_K
    D_Loss_wrt_W_V
    D_Loss_wrt_W_O
def compute_encoder_self_attention_weight_gradients():
    """

    """
def compute_masked_attention_weight_gradients():
    """
    """
def compute_Decoder_FF_weight_gradients():
    """
    """
def compute_Encoder_FF_weight_gradients():
    """
    """

def compute_Linear_layer_weight_gradients():
    """
    """


def forward_pass(Encoders, Decoders, Final_Layers, Num_Of_Encoders, Num_Of_Decoders):
    """
    Input:
    Encoders - with Encoder input (input to self attention) for the first encoder layer already set
    Decoders - with Decoder input (input to masked attention) for the first decoder layer already set
    Final_Layers - dictionary containing

    """

    #Encoder

    for i in range(1, Num_Of_Encoders + 1):
        Encoder_Layer = i
        Encoder(Encoders, Encoder_Layer, Num_Of_Encoders)


    #Decoder
    Final_Encoder_Layer_Output = Encoders[f"Encoder{Num_Of_Encoders}"][f"Layer_Norm_FF"][f"Output"]

    for i in range(1, Num_Of_Decoders+1):
        Decoder_Layer = i
        Decoder(Decoders, Final_Encoder_Layer_Output, Decoder_Layer, Num_Of_Decoders)

    Final_Layers[f"Linear"]["Input"] = Decoders[f"Decoder{Num_Of_Decoders}"][f"Layer_Norm_FF"][f"Output"]



    # Linear
    X =  Final_Layers[f"Linear"]["Input"]

    W_L = Final_Layers[f"Linear"]["Weights"]
    Final_Layers[f"Softmax"][f"Input"] = Linear(X, W_L)
    # Softmax
    Final_Layers[f"Softmax"][f"Output"] = Softmax(Final_Layers[f"Softmax"][f"Input"])
    # Loss
    X = Final_Layers[f"Softmax"][f"Output"]
    Y = Final_Layers[f"True_Labels"]
    Loss = Cross_entropy_loss(X, Y)
    return Loss, X, Y
"""
def Convert_Jacobian_To_Weight_Matrix_Shape(J , W):
    ""
    J - Jacobain Matrix (1 by rowsW times columnsW)
    W - weight matrix
    ""
    rowsW , colsW = W.shape
    rowsJ, colsJ = J.shape
    J_In_Shape_W = np.zeros((rowsW, colsW))
    rows_J_In_Shape_W = 0
    cols_J_In_Shape_W = 0
    for i in range(colsJ):
        J_In_Shape_W[rows_J_In_Shape_W][cols_J_In_Shape_W] = J[0][i]
        cols_J_In_Shape_W += 1
        if cols_J_In_Shape_W >= colsW:
            cols_J_In_Shape_W = 0
            rows_J_In_Shape_W += 1
            if rows_J_In_Shape_W >= rowsW:
                break
    return J_In_Shape_W

"""
def Convert_Jacobian_To_Weight_Matrix_Shape(J_Flattened: cp.ndarray, W: cp.ndarray) -> cp.ndarray:
    """
    Reshapes a CuPy row vector of shape (1, m*n) into the shape of a given CuPy matrix (m, n).

    Parameters:
        J_Flattened (cp.ndarray): A CuPy row vector of shape (1, m*n).
        W (cp.ndarray): A CuPy 2D matrix of shape (m, n) to infer the target shape.

    Returns:
        cp.ndarray: Reshaped matrix of shape (m, n).
    """
    return J_Flattened.reshape(W.shape)

def J_Prime(Threshhold, J):  # For Gradient Clipping
    """
    Input:
    J is a row vector representing the gradient
    Output:
    J_Prime is a row vector scaled or the same depending on L2 norm
    """
    J_Prime = None
    L2_Norm_J = np.linalg.norm(J, ord=2)  # Computes L2 norm for row vector J

    if L2_Norm_J <= Threshhold:
        J_Prime = J
    else:
        J_Prime = (Threshhold / L2_Norm_J) * J
    return J_Prime

# Default: lr=0.001, beta1=0.9, beta2=0.97, eps=1e-6, weight_decay=0.01
#def adamw(weight, grad, m, v, t, lr=0.0005, beta1=0.8, beta2=0.95, eps=1e-6, weight_decay=0.01):   # this works mid training
#def adamw(weight, grad, m, v, t, lr=0.0005, beta1=0.8, beta2=0.97,  eps=1e-7,  weight_decay=0.001):
def adamw(weight, grad, m, v, t, lr=0.001, beta1=0.0, beta2=0.0, eps=1e-6, weight_decay=0.0000):
    """
    AdamW optimizer implementation in CuPy.

    Parameters:
    - weight: cp.ndarray, the weight matrix to be updated.
    - grad: cp.ndarray, the gradient of the weight matrix.
    - m: cp.ndarray, first moment vector (moving average of gradients).
    - v: cp.ndarray, second moment vector (moving average of squared gradients).
    - t: int, timestep (incremented by 1 at each update).
    - lr: float, learning rate.
    - beta1: float, exponential decay rate for the first moment.
    - beta2: float, exponential decay rate for the second moment.
    - eps: float, small constant to avoid division by zero.
    - weight_decay: float, L2 regularization coefficient.

    Returns:
    - updated_weight: cp.ndarray, the updated weight matrix.
    - m: cp.ndarray, updated first moment.
    - v: cp.ndarray, updated second moment.
    """
    # Update biased first moment estimate
    m = beta1 * m + (1 - beta1) * grad

    # Update biased second moment estimate
    v = beta2 * v + (1 - beta2) * (grad ** 2)

    # Bias-corrected first and second moments
    m_hat = m / (1 - beta1 ** t)
    v_hat = v / (1 - beta2 ** t)

    # Apply weight decay
    weight -= lr * weight_decay * weight

    # Update weight
    weight_update = lr * m_hat / (np.sqrt(v_hat).astype(np.float16) + eps)
    updated_weight = weight - weight_update

    updated_weight = updated_weight.astype(np.float16)
    return updated_weight, m, v

def average_3d_to_2d(matrix_3d):
    # Calculate the mean along the first axis (depth), resulting in a 2D matrix
    return np.mean(matrix_3d, axis=0)



def positional_encoding(x):
    batch_size, seq_len, dim = x.shape

    # Position encoding computation
    position = np.arange(seq_len).reshape(-1, 1)
    div_term = np.exp(np.arange(0, dim, 2) * (-np.log(10000.0) / dim))

    # Creating the positional encoding tensor
    pe = np.zeros((seq_len, dim), dtype=np.float32)
    pe[:, 0::2] = np.sin(position * div_term)
    pe[:, 1::2] = np.cos(position * div_term)

    return x + pe[None, :, :]

# Define the numerical Jacobian function
def numerical_jacobian(f, x, eps=1e-5):
    batch_size, seq_len, dim = x.shape
    input_size = seq_len * dim
    jacobian = np.zeros((batch_size, input_size, input_size), dtype=np.float32)

    # Flatten the input tensor to shape (batch_size, seq_len * dim)
    x_flat = x.reshape(batch_size, -1)

    # Perturb the entire input tensor at once
    x_pos = x_flat + eps
    x_neg = x_flat - eps

    # Reshape perturbed tensors back to (batch_size, seq_len, dim)
    x_pos = x_pos.reshape(batch_size, seq_len, dim)
    x_neg = x_neg.reshape(batch_size, seq_len, dim)

    # Compute the outputs for perturbed tensors
    pos_output = f(x_pos)
    neg_output = f(x_neg)

    # Compute the gradient estimate for all elements at once
    grad_estimate = (pos_output - neg_output) / (2 * eps)

    # Reshape grad_estimate to match the Jacobian shape
    # Flatten the output gradients to (batch_size, seq_len * dim)
    grad_estimate_flat = grad_estimate.reshape(batch_size, -1)

    # Fill the Jacobian matrix using broadcasting
    jacobian[:, :, :] = grad_estimate_flat[:, :, None] * np.eye(input_size)

    return jacobian

def positional_encoding_derivative(x):
    return numerical_jacobian(positional_encoding, x)

def embed_tokens(token_indices, embedding_matrix):
    """
    Maps token indices to the embedding matrix.

    Args:
        token_indices (np.ndarray): A (batch_size, sequence_length) tensor containing token indices.
        embedding_matrix (np.ndarray): A (vocab_size, model_dimension) tensor containing embeddings.

    Returns:
        np.ndarray: A (batch_size, sequence_length, model_dimension) tensor containing embedded representations.
    """
    return embedding_matrix[token_indices]

def embedding_jacobian(token_indices):
    """
    Computes the full Jacobian of the embedding layer output w.r.t. the embedding matrix weights.

    Args:
        token_indices (np.ndarray): A (batch_size, sequence_length) tensor containing token indices.
        vocab_size (int): Vocabulary size.
        model_dimension (int): Model dimension.

    Returns:
        np.ndarray: A (batch_size, sequence_length * model_dimension, vocab_size * model_dimension) Jacobian matrix.
    """
    batch_size, sequence_length = token_indices.shape

    # Compute row indices for placing identity matrices
    row_indices = np.arange(sequence_length * model_dimension).reshape(sequence_length, model_dimension)

    # Compute column indices in embedding matrix space
    col_indices = (token_indices[..., None] * model_dimension) + np.arange(model_dimension)

    # Flatten for indexing
    row_indices = row_indices.ravel()
    col_indices = col_indices.reshape(batch_size, -1)

    # Create sparse identity blocks
    jacobian = np.zeros((batch_size, sequence_length * model_dimension, vocab_size * model_dimension), dtype=np.float32)
    jacobian[np.arange(batch_size)[:, None], row_indices, col_indices] = 1.0

    return jacobian

def backward_pass(Decoders, Decoder_Layer, Final_Layers, Encoders, Encoder_Layer, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, D_WFF1_wrt_WFF1_Nonjacobian, D_WFF2_wrt_WFF2_Nonjacobian, D_X_wrt_X_nonjacobian_MACA, D_XT_wrt_X_MA, D_X_wrt_X_nonjacobian_SA, D_XT_wrt_X_SA, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian, t):
    """
      t - current epoch
    """

    start_time = time.time()  # Record start time

    # WL Linear update (check)
    J = D_Lavg_wrt_WL(Final_Layers)
    J = average_3d_to_2d(J)
    #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
    W = Final_Layers[f"Linear"][f"Weights"]
    J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
    m = Final_Layers[f"First_Moment"][f"mW"]
    v = Final_Layers[f"Second_Moment"][f"vW"]
    updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
    Final_Layers[f"First_Moment"][f"mW"] = m
    Final_Layers[f"Second_Moment"][f"vW"] = v
    Final_Layers[f"Linear"][f"Weights"] = updated_weight



    #Final_Layers[f"Linear"][f"Weights"] += (-Learning_rate) * (J_In_Shape_W)

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for linear layer weight update: {elapsed_time:.6f} seconds")

    #Learning_rate = CLR_Triangular_Mode(t)
    Learning_rate = 0.01
    Threshhold = 1

    Encoder_Num = Encoder_Layer
    J_In_Shape_W_List = []
    J = None
    start_time1 = time.time()  # Record start time


    """
    # Preliminary Calculation for derivatives for Encoders ################
    # Cupy Implementation
    M = model_dimension
    S = sequence_length

    # Total number of ones (one per (i,j) pair)
    total_elements = M * S

    # Generate indices corresponding to the row positions.
    # In the original loop, rows are filled in the order:
    # for i in range(M): for j in range(S): row_index = i * S + j.
    rows = np.arange(total_elements)

    # Compute the corresponding (i, j) pairs for each row.
    # 'i' repeats S times, 'j' cycles every S elements.
    i_idx = np.repeat(np.arange(M), S)
    j_idx = np.tile(np.arange(S), M)

    # The column index for each (i, j) pair is given by: col = j * M + i.
    col_indices = j_idx * M + i_idx

    # Create the zero matrix and assign 1's at the calculated indices.
    D_XT_wrt_X_SA = np.zeros((total_elements, S * M))
    D_XT_wrt_X_SA[rows, col_indices] = 1


    D_X_wrt_X = np.eye(sequence_length * model_dimension) # identity matrix
    Rows = sequence_length
    Cols = model_dimension
    #D_X_wrt_X_nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols) #Numpy
    D_X_wrt_X_nonjacobian_SA = reshape_cupy_2d_to_3d(D_X_wrt_X, Rows, Cols) # Cupy
    """
    #################################################################
    # Calculate LN, FF , and SA derivatives for Encoders

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:

        start_time_T = time.time()  # Record start time
        D_OSAi_wrt_Xi = compute_D_SA_wrt_itsInput(Encoders, Encoder_Layer_Current, D_XT_wrt_X_SA, D_X_wrt_X_nonjacobian_SA) # SA derivative wrt its input
        end_time_T = time.time()  # Record end time
        elapsed_time = end_time_T - start_time_T  # Calculate elapsed time
        #print(f"Elapsed time for D_LNSAi_wrt_OSAi: {elapsed_time:.6f} seconds")

        start_time = time.time()  # Record start time

        O = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Output"]
        X = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Input"]
        D_LNSAi_wrt_OSAi = compute_layernorm_derivative(O, X) # layer norm derivative

        end_time = time.time()  # Record end time
        elapsed_time = end_time - start_time  # Calculate elapsed time
        #print(f"Elapsed time for D_LNSAi_wrt_OSAi: {elapsed_time:.6f} seconds")

        Decoder = False
        W_FF1 =  Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        W_FF2 = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        ReLu_Input = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"ReLu_Input"]
        FF_Input = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Input"]
        D_FFi_wrt_LNSAi = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input, FF_Input) # feed forward derivative

        start_time = time.time()  # Record start time

        O = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Output"]
        X = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Input"]
        D_LNFFi_wrt_FFi = compute_layernorm_derivative(O, X) # layer norm derivative

        end_time = time.time()  # Record end time
        elapsed_time = end_time - start_time  # Calculate elapsed time
        #p rint(f"Elapsed time for D_LNFFi_wrt_FFi: {elapsed_time:.6f} seconds")

        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_OSAi_wrt_Xi"] = D_OSAi_wrt_Xi
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_LNSAi_wrt_OSAi"] = D_LNSAi_wrt_OSAi
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_FFi_wrt_LNSAi"] = D_FFi_wrt_LNSAi
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_LNFFi_wrt_FFi"] = D_LNFFi_wrt_FFi

        Encoder_Layer_Current -= 1

    end_time1 = time.time()  # Record end time
    elapsed_time = end_time1 - start_time1  # Calculate elapsed time
    #print(f"Elapsed time for Encoder Layers Derivatives: {elapsed_time:.6f} seconds")
    """
    ########## Preliminary Calculations for Decoders derivatives ################
    # Cupy Implementation
    M = model_dimension
    S = sequence_length_O

    # Total number of ones (one per (i,j) pair)
    total_elements = M * S

    # Generate indices corresponding to the row positions.
    # In the original loop, rows are filled in the order:
    # for i in range(M): for j in range(S): row_index = i * S + j.
    rows = np.arange(total_elements)

    # Compute the corresponding (i, j) pairs for each row.
    # 'i' repeats S times, 'j' cycles every S elements.
    i_idx = np.repeat(np.arange(M), S)
    j_idx = np.tile(np.arange(S), M)

    # The column index for each (i, j) pair is given by: col = j * M + i.
    col_indices = j_idx * M + i_idx

    # Create the zero matrix and assign 1's at the calculated indices.
    D_XT_wrt_X_MA = np.zeros((total_elements, S * M))
    D_XT_wrt_X_MA[rows, col_indices] = 1



    # Calculate D_X_wrt_X_nonjacobian
    D_X_wrt_X = np.eye(sequence_length_O * model_dimension) # identity matrix

    Rows = sequence_length_O
    Cols = model_dimension
    #D_X_wrt_X_nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols) # numpy
    D_X_wrt_X_nonjacobian_MACA = reshape_cupy_2d_to_3d(D_X_wrt_X, Rows, Cols) # cupy

    """

    ####################################
    # Calculate LN, CA, MA dereivatives for Decoders
    start_time = time.time()  # Record start time
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:

        D_OMAi_wrt_Xi = compute_D_MaskedMAO_wrt_itsInput(Decoders, Decoder_Layer_Current, D_X_wrt_X_nonjacobian_MACA, D_XT_wrt_X_MA) # Derivative of output of MA wrt its input

        D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Decoder_Layer_Current, D_X_wrt_X_nonjacobian_MACA) # Derivative of OC wrt input from masked attention

        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Input"]
        D_LNMAi_wrt_OMAi = compute_layernorm_derivative(O, X) # layernorm derivative

        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Input_Decoder"]
        D_LNCAi_wrt_OCAi = compute_layernorm_derivative(O, X) # layernorm derivative

        Decoder = True
        W_FF1 = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        W_FF2 = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        ReLu_Input = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"ReLu_Input"]
        FF_Input = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Input"]
        D_FFi_wrt_LNCAi = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input, FF_Input) # feed forward derivative

        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Input"]
        D_LNFFi_wrt_FFi = compute_layernorm_derivative(O, X) # layernorm derivative

        # Save to Decoders dictionary
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_OMAi_wrt_Xi"] = D_OMAi_wrt_Xi
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_LNMAi_wrt_OMAi"] = D_LNMAi_wrt_OMAi
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_OCAi_wrt_LNMAi"] = D_OCAi_wrt_LNMAi
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_LNCAi_wrt_OCAi"] = D_LNCAi_wrt_OCAi
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_FFi_wrt_LNCAi"] = D_FFi_wrt_LNCAi
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_LNFFi_wrt_FFi"] = D_LNFFi_wrt_FFi

        Decoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for Decoder Layers Derivatives: {elapsed_time:.6f} seconds")

    start_time = time.time()  # Record start time





    # Calculate D_Lavg_wrt_DOF_
    D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)

    # Calculate D_Lavg_wrt_DOi for each decoder layer

    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:

        F = Number_Of_Decoder_Layers
        i = Decoder_Layer_Current
        m = F - i  # m is number of decoders layers in front of decoder layer i
        D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
        D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_Lavg_wrt_DOi"] = D_Lavg_wrt_DOi    # CuPy Matrix
        Decoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_DOi: {elapsed_time:.6f} seconds")

    start_time_WKDS = time.time()  # Record start time

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        for i in range(1, number_of_heads+1):

            A_Num = i
            P = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] #Softmax_S for the ith attention head from encoders
            D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, P)
            Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"D_Softmax_S_wrt_S"] = D_Softmax_S_wrt_S #Softmax_S for the ith attention head from encoders

        Encoder_Layer_Current -= 1

    end_time_WKDS = time.time()  # Record end time
    elapsed_time = end_time_WKDS - start_time_WKDS  # Calculate elapsed time
    #print(f"Elapsed time for D_Softmax_S_wrt_S for D_Lavg_wrt_WKSi_: {elapsed_time:.6f} seconds")



    ##################
    """
    ####### Test ###########
    start_time_TT = time.time()  # Record start time
    #P = np.random.rand(sequence_length, sequence_length)
    P = Encoders[f"Encoder{3}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{1}"][f"Softmax_S"] #Softmax_S for the ith attention head from encoders
    D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, P)
    end_time_TT = time.time()  # Record end time
    elapsed_time = end_time_TT - start_time_TT  # Calculate elapsed time
    #print(f"Elapsed time for WKSI Attention i (Before Before Before Before): {elapsed_time:.6f} seconds")
    ##################
    """












    start_time = time.time()  # Record start time


    #############################
    # WFF2 Decoder update (checked!)

    Encoder_Layer_ = 0
    D_Lavg_wrt_EOF_ = 0
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WFF2i(D_WFF2_wrt_WFF2_Nonjacobian, D_Lavg_wrt_EOF_, Encoder_Layer_, Decoder_Layer_Current, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, Final_Layers, Encoders, Decoders, False, True)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"] = updated_weight
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"] = v



        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Decoder_Layer_Current -= 1







    # WFF1 Decoder update (checked!)


    Encoder_Layer_ = 0
    D_Lavg_wrt_EOF_ = 0
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WFF1i(D_WFF1_wrt_WFF1_Nonjacobian, D_Lavg_wrt_EOF_, Encoder_Layer_, Decoder_Layer_Current, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, Final_Layers, Encoders, Decoders, False, True)
        J = average_3d_to_2d(J)

        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"] = updated_weight
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"] = v


        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Decoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for decoder FF weights updates: {elapsed_time:.6f} seconds")

    # Calculate D_Lavg_wrt_OCAi
    start_time = time.time()  # Record start time

    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        D_Lavg_wrt_DOi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_Lavg_wrt_DOi"]  # CuPy Matrix
        D_DOi_wrt_OCAi_ = D_DOi_wrt_OCAi(Decoders, Decoder_Layer_Current)
        D_Lavg_wrt_OCAi = D_Lavg_wrt_DOi @ D_DOi_wrt_OCAi_
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"D_Lavg_wrt_OCAi"] = D_Lavg_wrt_OCAi
        Decoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_OCAi: {elapsed_time:.6f} seconds")

    start_time_CA = time.time()  # Record start time

    start_time_WOCi = time.time()  # Record start time
    # WO Cross update (checked!)
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WOCi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mO"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vO"]

        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mO"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vO"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"] = updated_weight



        #J_In_Shape_W_List.append(J_In_Shape_W)
        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        Decoder_Layer_Current -= 1
    end_time_WOCi = time.time()  # Record end time
    elapsed_time = end_time_WOCi - start_time_WOCi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WOCi: {elapsed_time:.6f} seconds")


    start_time_WVCi = time.time()  # Record start time
    # WV Cross update (checked!)
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WVCi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Value_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mV"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vV"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mV"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vV"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Value_Weight_Matrix"] = updated_weight





        #J_In_Shape_W_List.append(J_In_Shape_W)
        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Value_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        Decoder_Layer_Current -= 1
    end_time_WVCi = time.time()  # Record end time
    elapsed_time = end_time_WVCi - start_time_WVCi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WVCi: {elapsed_time:.6f} seconds")

    start_time_WKCi = time.time()  # Record start time
    # WK Cross update (checked!)
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WKCi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mK"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vK"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mK"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vK"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Key_Weight_Matrix"] = updated_weight






        #J_In_Shape_W_List.append(J_In_Shape_W)
        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Key_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        Decoder_Layer_Current -= 1
    end_time_WKCi = time.time()  # Record end time
    elapsed_time = end_time_WKCi - start_time_WKCi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WKCi: {elapsed_time:.6f} seconds")


    start_time_WQCi = time.time()  # Record start time
    # WQ Cross update (checked!)
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Loss_wrt_W_Q_C_i(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mQ"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vQ"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"First_Moment"][f"mQ"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Second_Moment"][f"vQ"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"] = updated_weight




        #J_In_Shape_W_List.append(J_In_Shape_W)
        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        Decoder_Layer_Current -= 1

    end_time_WQCi = time.time()  # Record end time
    elapsed_time = end_time_WQCi - start_time_WQCi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WQCi: {elapsed_time:.6f} seconds")


    end_time_CA = time.time()  # Record end time
    elapsed_time = end_time_CA - start_time_CA  # Calculate elapsed time
    #print(f"Elapsed time for cross updates: {elapsed_time:.6f} seconds")



    start_time = time.time()  # Record start time
    # Calculate D_Lavg_wrt_OMAi for each decoder layer
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:

        #D_Lavg_wrt_OCAi_ = D_Lavg_wrt_OCAi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        """
        D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Decoder_Layer_Current) # OC derivative wrt X (decoder input)

        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Input"]
        D_LNMAi_wrt_OMAi =  compute_layernorm_derivative(O, X) #layernorm derivative
        """
        #D_OCAi_wrt_LNMAi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_OCAi_wrt_LNMAi"]
        #D_OCAi_wrt_OMAi = D_OCAi_wrt_LNMAi @ D_LNMAi_wrt_OMAi

        """
        Decoder = True
        W_FF1 = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        W_FF2 = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        ReLu_Input = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"ReLu_Input"]
        D_FFDi_wrt_LNCAi = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input)# Feedforward derivative

        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Feed_Forward"][f"Input"]
        D_LNFFi_wrt_FFDi = compute_layernorm_derivative(O, X)# Layernorm derivative
        """
        D_FFDi_wrt_LNCAi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_FFi_wrt_LNCAi"]
        D_LNFFi_wrt_FFDi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_LNFFi_wrt_FFi"]

        D_LNFFi_wrt_LNCAi = D_LNFFi_wrt_FFDi @ D_FFDi_wrt_LNCAi
        """
        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Cross_Attention"][f"Input_Decoder"]
        D_LNCAi_wrt_OCAi = compute_layernorm_derivative(O, X) # Layernorm derivative
        """
        D_LNCAi_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_LNCAi_wrt_OCAi"]
        D_LNFFi_wrt_OCAi = D_LNFFi_wrt_LNCAi @ D_LNCAi_wrt_OCAi
        """
        D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Decoder_Layer_Current) # derivative of Cross attention output wrt input from MA
        """
        D_OCAi_wrt_LNMAi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_OCAi_wrt_LNMAi"]
        D_LNFFi_wrt_LNMAi = D_LNFFi_wrt_OCAi @ D_OCAi_wrt_LNMAi
        """
        O = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Input"]
        D_LNMAi_wrt_OMAi = compute_layernorm_derivative(O, X) # Layernorm derivative
        """
        D_LNMAi_wrt_OMAi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_LNMAi_wrt_OMAi"]

        D_DOi_wrt_OMAi = D_LNFFi_wrt_LNMAi @ D_LNMAi_wrt_OMAi

        D_Lavg_wrt_DOi = Decoders[f"Decoder{Decoder_Layer_Current}"][f"D_Lavg_wrt_DOi"]  # CuPy Matrix
        D_Lavg_wrt_OMAi = D_Lavg_wrt_DOi @ D_DOi_wrt_OMAi
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"D_Lavg_wrt_OMAi"] = D_Lavg_wrt_OMAi

        Decoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_OMAi: {elapsed_time:.6f} seconds")

    start_time_MA = time.time()  # Record start time


    # WO Masked Update (Checked!)
    start_time_WOMi = time.time()  # Record start time
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WOMi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mO"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vO"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mO"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vO"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"] = updated_weight




        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Decoder_Layer_Current -= 1

    end_time_WOMi = time.time()  # Record end time
    elapsed_time = end_time_WOMi - start_time_WOMi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WOMi: {elapsed_time:.6f} seconds")



    # WV Masked Update (Checked!)
    start_time_WVMi = time.time()  # Record start time
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WVMi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mV"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vV"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mV"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vV"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"] = updated_weight




        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Decoder_Layer_Current -= 1
    end_time_WVMi = time.time()  # Record end time
    elapsed_time = end_time_WVMi - start_time_WVMi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WVMi: {elapsed_time:.6f} seconds")



    # WK Masked Update (Checked!)
    start_time_WKMi = time.time()  # Record start time
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WKMi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mK"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vK"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mK"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vK"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"] = updated_weight




        #J_In_Shape_W_List.append(J_In_Shape_W)
        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        Decoder_Layer_Current -= 1
    end_time_WKMi = time.time()  # Record end time
    elapsed_time = end_time_WKMi - start_time_WKMi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WKMi: {elapsed_time:.6f} seconds")



    # WQ Masked update (checked!)
    start_time_WQMi = time.time()  # Record start time
    Decoder_Layer_Current = Number_Of_Decoder_Layers
    while Decoder_Layer_Current > 0:
        J = D_Lavg_wrt_WQMi(Decoders, Decoder_Layer_Current, Final_Layers, Number_Of_Decoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mQ"]
        v = Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vQ"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"First_Moment"][f"mQ"] = m
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Second_Moment"][f"vQ"] = v
        Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"] = updated_weight



        #J_In_Shape_W_List.append(J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        #Decoders[f"Decoder{Decoder_Layer_Current}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        Decoder_Layer_Current -= 1
    end_time_WQMi = time.time()  # Record end time
    elapsed_time = end_time_WQMi - start_time_WQMi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WQMi: {elapsed_time:.6f} seconds")

    end_time_MA = time.time()  # Record end time
    elapsed_time = end_time_MA - start_time_MA  # Calculate elapsed time
    #print(f"Elapsed time for masked updates: {elapsed_time:.6f} seconds")
    """
    # Calculating D_Lavg_wrt_EOF_
    start_time = time.time()  # Record start time
    D_Lavg_wrt_EOF_ = D_Lavg_wrt_E(Number_Of_Decoder_Layers, Decoders, Final_Layers, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian)
    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_EOF_: {elapsed_time:.6f} seconds")





    # Calculate D_Lavg_wrt_EOi for each encoder layer
    start_time = time.time()  # Record start time

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        F = Number_Of_Encoder_Layers
        m = F - Encoder_Layer_Current
        D_EOF_wrt_EOi_ = D_EOF_wrt_EOi(m, F, Encoders)
        D_Lavg_wrt_EOi = D_Lavg_wrt_EOF_ @ D_EOF_wrt_EOi_   # EOi = LNFFi = Layernorm FF of ith Encoder Layer
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Lavg_wrt_EOi"] = D_Lavg_wrt_EOi
        Encoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_EOi: {elapsed_time:.6f} seconds")

    # calculating D_Lavg_wrt_SOi's
    start_time = time.time()  # Record start time
    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        D_EOi_wrt_SOi_ = D_EOi_wrt_SOi(Encoders, Encoder_Layer_Current)
        D_Lavg_wrt_EOi = Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Lavg_wrt_EOi"]
        D_Lavg_wrt_SOi = D_Lavg_wrt_EOi @ D_EOi_wrt_SOi_

        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Lavg_wrt_SOi"] = D_Lavg_wrt_SOi
        Encoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for calculating D_Lavg_wrt_SOi's: {elapsed_time:.6f} seconds")
    """




    start_time_SA = time.time()  # Record start time

    start_time12 = time.time()  # Record start time

    P = Final_Layers[f"Softmax"][f"Output"] # P = Softmax(S)
    S = Final_Layers[f"Linear"][f"Output"]

    D_Softmax_wrt_logits = Derivative_SoftmaxS_wrt_S(P, S) # derivative of softmax wrt its input
    Final_Layers[f"D_Softmax_wrt_logits"] = D_Softmax_wrt_logits

    end_time12 = time.time()  # Record end time
    elapsed_time = end_time12 - start_time12  # Calculate elapsed time
    #print(f"Elapsed time for D_Softmax_wrt_logits: {elapsed_time:.6f} seconds")
    #######################################################@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
    # Something unusual goes on when I run this before the functions above
    ##print("D_ET_wrt_E_nonjacobian shape: ", D_ET_wrt_E_nonjacobian.shape)
    ##print("D_E_wrt_E_Nonjacobian shape: ", D_E_wrt_E_Nonjacobian.shape)



    ###########################
    np.cuda.Device(0).synchronize()
    # Call This Section 2
    start_time1 = time.time()  # Record start time
    D_Concat_wrt_WQS_Nonjacobian = None
    D_Concat_wrt_WKS_Nonjacobian = None
    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        """
        A = []
        for i in range(1, number_of_heads+1):
            A_Num = i

            D_Attention_i_wrt_WQS_Nonjacobian_ = D_Attention_i_wrt_WQS_Nonjacobian(Encoders, A_Num, Encoder_Layer_Current)

            A.append(D_Attention_i_wrt_WQS_Nonjacobian_)

        D_Concat_wrt_WQS_Nonjacobian = concatenate_cupy_matrices(A)


        A1 = []
        for i in range(1, number_of_heads+1):

            A_Num = i


            D_Attention_i_wrt_WKS_Nonjacobian_ =  D_Attention_i_wrt_WKS_Nonjacobian(Encoders, A_Num, Encoder_Layer_Current) #D_Attention_i_wrt_WKS_Nonjacobian(Encoders, A_Num, Encoder_Num)
            A1.append(D_Attention_i_wrt_WKS_Nonjacobian_)

        D_Concat_wrt_WKS_Nonjacobian = concatenate_cupy_matrices(A1) # Cupy Implementation
        """
        split_K = split(Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"QKV"][f"Key"])# Key matrix for Self attention from encoders)
        split_V = split(Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"QKV"][f"Value"])# Key matrix for Self attention from encoders)
        A = [D_Attention_i_wrt_WQS_Nonjacobian(Encoders, i, Encoder_Layer_Current, split_K, split_V) for i in range(1, number_of_heads+1)]
        D_Concat_wrt_WQS_Nonjacobian = concatenate_cupy_matrices(A)

        split_Q = split(Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"QKV"][f"Query"])# Key matrix for Self attention from encoders)
        split_V = split(Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"QKV"][f"Value"])# Key matrix for Self attention from encoders)
        A1 = [D_Attention_i_wrt_WKS_Nonjacobian(Encoders, i, Encoder_Layer_Current, split_Q, split_V) for i in range(1, number_of_heads+1)]
        D_Concat_wrt_WKS_Nonjacobian = concatenate_cupy_matrices(A1)

        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Concat_wrt_WQS_Nonjacobian"] = D_Concat_wrt_WQS_Nonjacobian
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Concat_wrt_WKS_Nonjacobian"] = D_Concat_wrt_WKS_Nonjacobian


        Encoder_Layer_Current -= 1
    end_time1 = time.time()  # Record end time
    elapsed_time = end_time1 - start_time1  # Calculate elapsed time
    #print(f"Elapsed time for Concat: {elapsed_time:.6f} seconds")
    ####################
    ###############
    start_time_sync = time.time()  # Record start time

    np.cuda.Device(0).synchronize()

    end_time_sync = time.time()  # Record end time
    elapsed_time = end_time_sync - start_time_sync  # Calculate elapsed time
    #print(f"Elapsed time for cp.cuda.Device(0).synchronize(): {elapsed_time:.6f} seconds")

    start_time_free_all_blocks = time.time()  # Record start time

    np._default_memory_pool.free_all_blocks()

    end_time_free_all_blocks = time.time()  # Record end time
    elapsed_time = end_time_free_all_blocks - start_time_free_all_blocks  # Calculate elapsed time
    #print(f"Elapsed time for _free_all_blocks: {elapsed_time:.6f} seconds")

    # Call This Section 1
    # Calculating D_Lavg_wrt_EOF_
    start_time = time.time()  # Record start time
    D_Lavg_wrt_EOF_ = D_Lavg_wrt_E(Number_Of_Decoder_Layers, Decoders, Final_Layers, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian)
    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_EOF_: {elapsed_time:.6f} seconds")
    ####################
    # Calculate D_Lavg_wrt_EOi for each encoder layer
    start_time_EOi = time.time()  # Record start time

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        F = Number_Of_Encoder_Layers
        m = F - Encoder_Layer_Current
        D_EOF_wrt_EOi_ = D_EOF_wrt_EOi(m, F, Encoders)
        D_Lavg_wrt_EOi = D_Lavg_wrt_EOF_ @ D_EOF_wrt_EOi_   # EOi = LNFFi = Layernorm FF of ith Encoder Layer
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Lavg_wrt_EOi"] = D_Lavg_wrt_EOi
        Encoder_Layer_Current -= 1

    end_time_EOi = time.time()  # Record end time
    elapsed_time = end_time_EOi - start_time_EOi  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_EOi: {elapsed_time:.6f} seconds")



    # calculating D_Lavg_wrt_SOi's
    start_time_SOi = time.time()  # Record start time
    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        D_EOi_wrt_SOi_ = D_EOi_wrt_SOi(Encoders, Encoder_Layer_Current)
        D_Lavg_wrt_EOi = Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Lavg_wrt_EOi"]
        D_Lavg_wrt_SOi = D_Lavg_wrt_EOi @ D_EOi_wrt_SOi_

        Encoders[f"Encoder{Encoder_Layer_Current}"][f"D_Lavg_wrt_SOi"] = D_Lavg_wrt_SOi
        Encoder_Layer_Current -= 1

    end_time_SOi = time.time()  # Record end time
    elapsed_time = end_time_SOi - start_time_SOi  # Calculate elapsed time
    #print(f"Elapsed time for calculating D_Lavg_wrt_SOi's: {elapsed_time:.6f} seconds")

    start_time_WK = time.time()  # Record start time

    #############################################################################@@@@@@@@@@@@@@@@@@@@@@2222
    # WK Self Attention update (checked!)

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:

        J = D_Lavg_wrt_WKSi(Encoders, Decoders, Final_Layers, Encoder_Layer_Current, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mK"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vK"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mK"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vK"] = v
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"] = updated_weight



        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1

    end_time_WK = time.time()  # Record end time
    elapsed_time = end_time_WK - start_time_WK  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WKSi: {elapsed_time:.6f} seconds")

    start_time_WOSA = time.time()  # Record start time
    # WO Self Attention update (checked!)
    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:


        J = D_Lavg_wrt_WOSi(Encoders, Decoders, Final_Layers, Encoder_Layer_Current, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers)
        J = average_3d_to_2d(J)


        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"]

        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)

        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mO"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vO"]

        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)




        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mO"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vO"] = v
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"] = updated_weight






        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1


    end_time_WOSA = time.time()  # Record end time
    elapsed_time = end_time_WOSA - start_time_WOSA  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WOSi: {elapsed_time:.6f} seconds")

    start_time_WV = time.time()  # Record start time

    # WV Self Attention update (checked!)

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        start_time_WOSA_Adam = time.time()  # Record start time
        J =  D_Lavg_wrt_WVSi(Encoders, Decoders, Final_Layers, Encoder_Layer_Current, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers)
        J = average_3d_to_2d(J)

        end_time_WOSA_Adam = time.time()  # Record end time
        elapsed_time = end_time_WOSA_Adam - start_time_WOSA_Adam  # Calculate elapsed time
        #print(f"Elapsed time for D_Lavg_wrt_WVSi_Adam: {elapsed_time:.6f} seconds")
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mV"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vV"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mV"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vV"] = v
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"] = updated_weight





        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1


    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time_WV  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WVSi: {elapsed_time:.6f} seconds")

    """
    start_time_WK = time.time()  # Record start time

    # WK Self Attention update (checked!)

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:

        J = D_Lavg_wrt_WKSi(Encoders, Decoders, Final_Layers, Encoder_Layer_Current, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers, D_Lavg_wrt_EOF_)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mK"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vK"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mK"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vK"] = v
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"] = updated_weight
        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time_WK  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WKSi: {elapsed_time:.6f} seconds")
    """


    start_time_WQ = time.time()  # Record start time
    # WQ Self Attention update (checked!)

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:

        J = D_Lavg_wrt_WQSi(Encoders, Decoders, Final_Layers, Encoder_Layer_Current, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"]
        start_time_JinSHape = time.time()  # Record start time

        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)

        end_time_JinSHape = time.time()  # Record end time
        elapsed_time = end_time_JinSHape - start_time_JinSHape  # Calculate elapsed time
        #print(f"Elapsed time for J_In_Shape_W: {elapsed_time:.6f} seconds")

        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mQ"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vQ"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"First_Moment"][f"mQ"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Second_Moment"][f"vQ"] = v
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"] = updated_weight




        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"] += (-Learning_rate) * (J_In_Shape_W)
        J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time_WQ  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WQSi: {elapsed_time:.6f} seconds")


    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time_SA  # Calculate elapsed time
    #print(f"Elapsed time for SA updates: {elapsed_time:.6f} seconds")


    start_time = time.time()  # Record start time


    ####### Preliminary Calculations for FF weights for encoder ##########
    """
    # Cupy
    # Get dimensions
    #S, F = ReLu_Input.shape  # sequence_length_I and feed_forward_dimension
    S = sequence_length
    F = feed_forward_dimension

    # Compute the ReLU derivative mask
    D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)


    # Flatten the indices for the output 3D matrix
    ColR = np.arange(S * F)


    # Create an empty 3D matrix
    D_ReLuInput_wrt_Input_nonjacobian = np.zeros((S * F, S, F))


    # Populate the matrix with the computed values
    D_ReLuInput_wrt_Input_nonjacobian[ColR, ColR // F, ColR % F] = D_ReLuXij_wrt_Xij.flatten()

    #D_ReLuInput_wrt_Input = To_Jacobian(D_ReLuInput_wrt_Input_nonjacobian) # Numpy
    D_ReLuR_wrt_R = reshape_cupy_3d_to_2d(D_ReLuInput_wrt_Input_nonjacobian)
    """



    ############################
    # WFF2 Encoder update (checked!)

    Decoder_Layer_Current = 0

    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        J = D_Lavg_wrt_WFF2i(D_WFF2_wrt_WFF2_Nonjacobian, D_Lavg_wrt_EOF_, Encoder_Layer_Current, Decoder_Layer_Current, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, Final_Layers, Encoders, Decoders, True, False)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"] = updated_weight
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"] = v


        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF2"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1

    # WFF1 Encoder update (checked!)
    Decoder_Layer_Current = 0
    Encoder_Layer_Current = Number_Of_Encoder_Layers
    while Encoder_Layer_Current > 0:
        J = D_Lavg_wrt_WFF1i(D_WFF1_wrt_WFF1_Nonjacobian, D_Lavg_wrt_EOF_, Encoder_Layer_Current, Decoder_Layer_Current, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, Final_Layers, Encoders, Decoders, True, False)
        J = average_3d_to_2d(J)
        #J_Prime_ = J_Prime(Threshhold, J) # for gradient clipping
        W = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
        m = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"]
        v = Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"]
        updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"] = updated_weight
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"] = m
        Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"] = v




        #Encoders[f"Encoder{Encoder_Layer_Current}"][f"Feed_Forward"][f"Weights"][f"WFF1"] += (-Learning_rate) * (J_In_Shape_W)
        #J_In_Shape_W_List.append(J_In_Shape_W)
        Encoder_Layer_Current -= 1


    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for FF weights encoder update: {elapsed_time:.6f} seconds")








    #Update Embedding matrix

    D_OSAi_wrt_Xi = Encoders[f"Encoder{1}"][f"D_OSAi_wrt_Xi"]
    D_Lavg_wrt_OSAi = Encoders[f"Encoder{1}"][f"D_Lavg_wrt_SOi"]

    D_Lavg_wrt_Xi_E = D_Lavg_wrt_OSAi @ D_OSAi_wrt_Xi

    D_OMAi_wrt_Xi = Decoders[f"Decoder{1}"][f"D_OMAi_wrt_Xi"]
    D_Lavg_wrt_OMAi = Decoders[f"Decoder{1}"][f"Masked_Attention"][f"D_Lavg_wrt_OMAi"]
    D_Lavg_wrt_Xi_D = D_Lavg_wrt_OMAi @ D_OMAi_wrt_Xi




    Input_E = Encoders[f"Encoder{1}"][f"Self_Attention"][f"Input"]
    Input_D = Decoders[f"Decoder{1}"][f"Masked_Attention"][f"Input"]
    token_indices_E = Final_Layers[f"Encoder_Token_Indices"]
    token_indices_D = Final_Layers[f"Decoder_Token_Indices"]
    D_PE_wrt_Input_E = positional_encoding_derivative(Input_E)
    D_PE_wrt_Input_D = positional_encoding_derivative(Input_D)
    D_Embedding_wrt_weights_E = embedding_jacobian(token_indices_E)
    D_Embedding_wrt_weights_D = embedding_jacobian(token_indices_D)

    D_PE_wrt_Embedding_Weights_E = D_PE_wrt_Input_E @ D_Embedding_wrt_weights_E
    D_PE_wrt_Embedding_Weights_D = D_PE_wrt_Input_D @ D_Embedding_wrt_weights_D

    D_Lavg_wrt_Embedding_Weights_D = D_Lavg_wrt_Xi_D @ D_PE_wrt_Embedding_Weights_D
    D_Lavg_wrt_Embedding_Weights_E = D_Lavg_wrt_Xi_E @ D_PE_wrt_Embedding_Weights_E

    J_D = average_3d_to_2d(D_Lavg_wrt_Embedding_Weights_D)
    J_E = average_3d_to_2d(D_Lavg_wrt_Embedding_Weights_E)
    J = np.stack([J_D, J_E], axis=0)
    J = average_3d_to_2d(J)

    W = Final_Layers[f"Embedding_Matrix"]
    m = Final_Layers[f"First_Moment_EM"]
    v = Final_Layers[f"Second_Moment_EM"]
    J_In_Shape_W = Convert_Jacobian_To_Weight_Matrix_Shape(J , W)
    updated_weight, m, v = adamw(W, J_In_Shape_W, m, v, t)
    Final_Layers[f"Embedding_Matrix"] = updated_weight
    Final_Layers[f"First_Moment_EM"] = m
    Final_Layers[f"Second_Moment_EM"] = v







    return J_In_Shape_W_List, J

 # Don't delete cell


def compute_jacobian_q_wiq(X, WiQ):
    """
    Computes the Jacobian matrix of the derivative of Q with respect to WiQ.

    Parameters:
    - X (numpy.ndarray): Input matrix of shape (sequence_length, model_dimension)
    - WiQ (numpy.ndarray): Weight matrix of shape (model_dimension, dk)

    Returns:
    - jacobian (numpy.ndarray): Jacobian matrix of shape
      (sequence_length, dk, model_dimension, dk)
    """
    seq_len, model_dim = X.shape
    model_dim, dk = WiQ.shape

    # Initialize the Jacobian tensor
    jacobian = np.zeros((seq_len * dk, model_dim * dk))

    # Fill in the Jacobian
    for i in range(seq_len):  # Iterate over the sequence length
        for j in range(dk):  # Iterate over dk (columns of WiQ)
            for m in range(model_dimension):
                for k in range(dk):  # Iterate over rows of WiQ
                    D_qij_wrt_Wmk = None
                    if k == j:
                        D_qij_wrt_Wmk = X[i,m]
                    else:
                        D_qij_wrt_Wmk = 0
                    jacobian[rowJ, colJ] = D_qij_wrt_Wmk
                    colJ += 1
                    if colJ >= (model_dim * dk):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (seq_len * dk):
                            break

    return jacobian



def compute_jacobian_s_qi(Q, K, dk):
    """
    Computes the Jacobian of S with respect to Q using efficient matrix multiplication.

    Parameters:
    - Q (numpy.ndarray): Query matrix of shape (sequence_length, dk)
    - K (numpy.ndarray): Key matrix of shape (sequence_length, dk)
    - dk (int): Dimension of the key (scaling factor)

    Returns:
    - jacobian_s_qi (numpy.ndarray): Jacobian of S w.r.t. Q of shape
      (sequence_length * sequence_length, sequence_length * dk)
    """
    seq_len, _ = Q.shape

    # Scaling factor for S
    scale = 1 / np.sqrt(dk).astype(np.float16)

    # Reshape K for broadcasting
    jacobian_s_qi = np.zeros((seq_len * seq_len, seq_len * dk))
    rowJ = 0
    colJ = 0
    for i in range(seq_len):
        for j in range(seq_len):
            for m in range(sequence_length):
                for k in range(dk):
                    D_Sij_wrt_qmk = None
                    if i == m:
                        D_Sij_wrt_qmk = K_Transposed[k,j] / np.sqrt(dk).astype(np.float16)
                    else:
                        D_Sij_wrt_qmk = 0

                    jacobian_s_qi[rowJ, colJ] = D_Sij_wrt_qmk
                    colJ += 1
                    if colJ >= (sequence_length * dk):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (seq_len * seq_len):
                            break


    return jacobian_s_qi

def compute_jacobian_attention_s(S, V):
    """
    Computes the Jacobian of Attention with respect to S using matrix multiplication.

    Parameters:
    - S (numpy.ndarray): Scaled dot-product matrix of shape (sequence_length, sequence_length)
    - V (numpy.ndarray): Value matrix of shape (sequence_length, dk)

    Returns:
    - jacobian_attention_s (numpy.ndarray): Jacobian of Attention w.r.t. S of shape
      (sequence_length * dk, sequence_length * sequence_length)
    """


    # Initialize the Jacobian matrix
    jacobian_attention_s = np.zeros((sequence_length * dk, seq_len * seq_len))
    rowJ = 0
    colJ = 0
    for i in range(seq_len):
        for j in range(dk):
            for m in range(sequence_length):
                for k in range(sequence_length):
                    D_Aij_wrt_Smk = 0
                    for l in range(sequence_length):
                        D_Pil_wrt_Smk = None
                        if i==m and l==k:
                            D_Pil_wrt_Smk = P[i,l] * (1-P[m,k])
                        if i==m and l!=k:
                            D_Pil_wrt_Smk = -P[m,k] * P[i,l]
                        else:
                            D_Pil_wrt_Smk = 0

                        term = V[l,j] * D_Pil_wrt_Smk
                        D_Aij_wrt_Smk += term
                    jacobian_attention_s[rowJ,colJ] = D_Aij_wrt_Smk
                    colJ += 1
                    if colJ >= (sequence_length * sequence_length):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length * dk):
                            break


    return jacobian_attention_s

def compute_derivative_attention_qi(Q, K, V, dk):
    """
    Computes the derivative of Attention with respect to Q using matrix multiplication.

    Parameters:
    - Q (numpy.ndarray): Query matrix of shape (sequence_length, dk)
    - K (numpy.ndarray): Key matrix of shape (sequence_length, dk)
    - V (numpy.ndarray): Value matrix of shape (sequence_length, dk)
    - dk (int): Dimension of the key (scaling factor)

    Returns:
    - derivative (numpy.ndarray): Derivative of Attention w.r.t. Q of shape
      (sequence_length, dk, sequence_length, dk)
    """
    seq_len, _ = Q.shape

    # Compute S
    S = np.dot(Q, K.T) / np.sqrt(dk).astype(np.float16)

    # Compute the Jacobians
    jacobian_s_qi = compute_jacobian_s_qi(Q, K, dk)  # Shape: (seq_len^2, seq_len * dk)
    jacobian_attention_s = compute_jacobian_attention_s(S, V)  # Shape: (seq_len * dk, seq_len^2)

    # Chain rule: Combine the Jacobians using matrix multiplication
    jacobian_combined = np.dot(jacobian_attention_s, jacobian_s_qi)  # Shape: (seq_len * dk, seq_len * dk)

    # Reshape to the final form
    #derivative = jacobian_combined.reshape((seq_len, dk, seq_len, dk))

    return jacobian_combined




def compute_derivative_o_attention_i(Attention_i, W_o, h, sequence_length, dk, model_dimension, i):
    """
    Computes the derivative of O with respect to Attention sub i.

    Parameters:
    - Attention_i (numpy.ndarray): Attention matrix for the i-th head of shape (sequence_length, dk)
    - W_o (numpy.ndarray): Output weight matrix of shape (model_dimension, model_dimension)
    - h (int): Number of attention heads
    - sequence_length (int): Length of the input sequence
    - dk (int): Dimension of the key/query vectors (for each attention head)
    - model_dimension (int): Overall model dimension
    - i (int): Attention head number between 1 and h

    Returns:
    - jacobian (numpy.ndarray): Jacobian of O w.r.t. Attention_i of shape
      (sequence_length * model_dimension, sequence_length * dk)
    """
    Z = (i - 1) * dk  # Starting index for the i-th attention head in the concatenated matrix
    concat_size = sequence_length * model_dimension
    attention_size = sequence_length * dk

    # Initialize the jacobian matrix with zeros
    jacobian = np.zeros((concat_size, attention_size))

    # Iterate through the rows and columns of the jacobian
    rowJ = 0
    colJ = 0
    for i in range(sequence_length):
        for j in range(model_dimension):
            # O_ij corresponds to O[row, col] in the concatenated matrix
            for m in range(sequence_length):
                for k in range(dk):
                    # Calculate the column index in Attention_i

                    if i == m:  # If i == m, we calculate the derivative as W_o
                        W_superscript_O_row = Z + (k-1)
                        jacobian[rowJ, colJ] = W_o[W_superscript_O_row , j]
                    else:
                        jacobian[rowJ, colJ] = 0
                    colJ += 1
                    if colJ >= (sequence_length * dk):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length * model_dimension):
                            break


    return jacobian




"""

def compute_layernorm_derivative(O, X):  # refer to page 5
    ""
    Compute the derivative of LayerNorm with respect to O where LayerNorm = LayerNorm(O+X).

    Parameters:
    - O (numpy.ndarray): Output of the previous layer, shape (sequence_length or sequence_length_O, model_dimension)
    - X (numpy.ndarray): Input to the previous layer, shape (sequence_length or sequence_length_O, model_dimension)
    - sequence_length (int): Length of the input sequence
    - model_dimension (int): Model dimension

    Returns:
    - jacobian (numpy.ndarray): Jacobian of LayerNorm w.r.t O, shape
      (sequence_length * model_dimension, sequence_length * model_dimension)
    ""
    # Compute S = O + X (residual connection)
    S = O + X

    # Compute mean (mu_i) and std (sigma_i) for each row for the ith row
    mu = np.mean(S, axis=1)  # Shape: (sequence_length, 1)


    # Compute the squared sum for each row using loops
    sigmas = []
    variance = []
    for i in range(S.shape[0]):  # Iterate over each row
        squared_sum = 0
        for value in S[i]:  # Iterate over each element in the ith row
            squared_sum += (value - mu[i]) ** 2
        squared_sum /= (model_dimension)

        variance.append(squared_sum)  # First element corresponds to first row variance, second corresponds to second row variance, etc...
        sigma_i = np.sqrt(squared_sum)
        sigmas.append(sigma_i)       #note sigmas are lowercase sigmas not to be confused with summation sigma





    # Initialize the Jacobian matrix

    jacobian_layernorm_s = np.zeros((O.shape[0] * model_dimension, O.shape[0] * model_dimension))

    # Compute the Jacobian of LayerNorm w.r.t S (i.e. O+X)
    rowJ = 0
    colJ = 0
    for i in range(O.shape[0]):
        for j in range(model_dimension):
            for m in range(O.shape[0]):
                for k in range(model_dimension):
                    if j == k and i == m:
                        d_S_mu = 1 - (1 / model_dimension)

                        summation = 0
                        for n in range(model_dimension):
                          if n == k:
                            summation += 2 * (S[i,k] - mu[i]) * (1 - (1/(model_dimension)))

                          else:

                            summation += 2 * (S[i,n] - mu[i]) * (- (1/(model_dimension)))

                        if np.sqrt(variance[i]) == 0:
                            #print("S = ", S)
                            #print(" ***** Division by Zero Error ***** ")
                            exit()

                        d_sigma = ( 1/(2 * np.sqrt(variance[i])) ) * (1/(model_dimension)) * (summation)

                        Lij_wrt_Smk = ((sigmas[i]) * (d_S_mu) - (S[i,j] - mu[i]) * (d_sigma))/(sigmas[i]) ** 2

                        jacobian_layernorm_s[rowJ, colJ] = Lij_wrt_Smk
                    if i == m and j != k:
                        d_S_mu = - (1 / model_dimension)
                        summation = 0
                        for n in range(model_dimension):
                          if n == k:
                            summation += 2 * (S[i,k] - mu[i]) * (1 - (1/(model_dimension)))
                          else:
                            summation += 2 * (S[i,k] - mu[i]) * (- (1/(model_dimension)))

                        d_sigma = ( 1/(2 * np.sqrt(variance[i])) ) * (1/(model_dimension)) * (summation)
                        Lij_wrt_Smk = ((sigmas[i]) * (d_S_mu) - (S[i,j] - mu[i]) * (d_sigma))/(sigmas[i]) ** 2


                        jacobian_layernorm_s[rowJ, colJ] = Lij_wrt_Smk
                    else:
                        jacobian_layernorm_s[rowJ, colJ] = 0

                    colJ += 1
                    if colJ >= (O.shape[0] * model_dimension):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (O.shape[0] * model_dimension):
                            break




    # Compute the Jacobian of (O+X) w.r.t O
    jacobian_S_O = np.eye(O.shape[0] * model_dimension)


    # Combine the Jacobians to compute LayerNorm w.r.t O
    jacobian = jacobian_layernorm_s @ jacobian_S_O

    return jacobian

"""

def compute_layernorm_derivative(m, X, epsilon=1e-5):
    """
    (MAIN)
    Compute the 2D Jacobian matrix of LayerNorm's output with respect to its input in CuPy,
    for multiple batches.

    Parameters:
    - B: Number of batches
    - N: Number of samples per batch
    - D: Feature dimension
    - epsilon: Small value for numerical stability

    Returns:
    - jacobian: 3D tensor of Jacobians per batch, shape (B, N*D, N*D)
    """

    # Random input tensor X of shape (B, N, D)
    B, N, D = X.shape

    # Random scale parameter gamma of shape (D,)
    gamma = np.random.randn(D).astype(np.float32)

    # Compute mean and variance per sample (B, N, 1)
    mean = X.mean(axis=2, keepdims=True)
    var = X.var(axis=2, keepdims=True)
    std_inv = 1.0 / np.sqrt(var + epsilon).astype(np.float16)  # Shape: (B, N, 1)

    # Center the input
    X_centered = X - mean  # Shape: (B, N, D)

    # Identity matrix and all-ones matrix for computation
    I = np.eye(D, dtype=X.dtype)  # (D, D)
    ones = np.ones((D, D), dtype=X.dtype) / D  # (D, D)

    # Compute Jacobian per sample in batch (B, N, D, D)
    jacobian_per_sample = std_inv[:, :, None] * (
        I[None, None, :, :] - ones[None, None, :, :]
        - (X_centered[:, :, :, None] * X_centered[:, :, None, :]) * (std_inv[:, :, None, None]**2).squeeze(-1)
    )


    # Scale by gamma
    jacobian_per_sample *= gamma[None, None, :, None]

    # Construct block diagonal efficiently (B, N, D, N, D)
    jacobian = np.zeros((B, N, D, N, D), dtype=X.dtype)
    diag_idx = np.arange(N)
    jacobian[:, diag_idx, :, diag_idx, :] = jacobian_per_sample.transpose(1,0,2,3)

    # Reshape to final 2D shape per batch (B, N*D, N*D)
    jacobian = jacobian.reshape(B, N * D, N * D)

    return jacobian
"""
def compute_layernorm_derivative(m, X, epsilon=1e-5):
    ""
    Compute the 2D Jacobian matrix of LayerNorm's output with respect to its input in CuPy, without a for loop.

    Parameters:
    - X: Input to LayerNorm, shape (N, D)
    - gamma: Scale parameter, shape (D,)
    - epsilon: Small value for numerical stability

    Returns:
    - jacobian: 2D Jacobian of LayerNorm output w.r.t. input X, shape (N*D, N*D)
    ""


    N, D = X.shape

    # Random scale parameter gamma with shape (D,)
    gamma = np.random.randn(D).astype(cp.float32)

    # Compute mean and variance (along feature dimension D)
    mean = X.mean(axis=1, keepdims=True)  # Shape: (N, 1)
    var = X.var(axis=1, keepdims=True)    # Shape: (N, 1)
    std_inv = 1.0 / np.sqrt(var + epsilon).astype(np.float16)  # Shape: (N, 1)

    # Normalize X
    X_centered = X - mean  # Centered X, shape (N, D)

    I = np.eye(D, dtype=X.dtype)  # Identity matrix of shape (D, D)
    ones = np.ones((D, D), dtype=X.dtype) / D  # (D, D) matrix filled with 1/D

    # Compute Jacobian per sample (N, D, D)
    jacobian_per_sample = std_inv[:, :, None] * (
        I - ones - (X_centered[:, :, None] * X_centered[:, None, :]) * std_inv[:, :, None]**2
    )

    # Scale by gamma
    jacobian_per_sample *= gamma[None, :, None]  # Shape: (N, D, D)

    # Construct block diagonal in a fully vectorized way
    jacobian = np.zeros((N, D, N, D), dtype=X.dtype)
    diag_idx = np.arange(N)
    jacobian[diag_idx, :, diag_idx, :] = jacobian_per_sample  # Insert blocks efficiently

    # Reshape to final 2D shape (N*D, N*D)
    jacobian = jacobian.reshape(N * D, N * D)

    return jacobian

"""
def compute_derivative_attention_i_wrt_X_for_Encoder_self_attention( ):
    """
    Compute the derivative of attention i output w.r.t. the input X.

    Parameters:


    Returns:
    - nonjacobian: Derivative of attention i w.r.t. X (sequence_length x dk)
    """
    # Calculate the Derivative of O w.r.t. Attention sub i

    # Calculate the Derivative of Attention sub i w.r.t. X

    jacobian_Ai_x =   np.zeros((sequence_length * dk, sequence_length * model_dimension))

      # Calculate the derivative of Aij w.r.t. Xmk
    rowJ = 0
    colJ = 0
    for i in range(sequence_length):
        for j in range(dk):
            for m in range(sequence_length):
                for k in range(model_dimension):
                    D_Aij_wrt_Xmk = 0

                    for t in range(sequence_length):

                        f = e_to_power_Sit
                        e_to_power_Sit = np.exp(Sit)
                        Sit = S[i,t]
                        g = sigma_e_to_power_S

                        sigma_e_to_power_S = 0

                        # Calculate g_prime
                        g_prime = 0
                        for t1 in range(sequence_length):
                            sigma_e_to_power_S += np.exp(S[i,t1])   # to calculate for g
                            D_e_to_Sit1_wrt_Xmk = np.exp(S[i,t1]) * (D_Sit1_wrt_Xmk)
                            Sigma_z1_zdk_of_D_qiz_times_kzt1_wrt_Xmk = 0
                            for z in range(dk):
                                qiz = Q[i,z]
                                K_transposed = K.T
                                kzt1 = K_transposed[z,t1]
                                D_kzt1_wrt_Xmk = None
                                if t1 == m:
                                    D_kzt1_wrt_Xmk = W_K_kz
                                    W_K_kz = W_K[k,z]
                                else:
                                    D_kzt1_wrt_Xmk = W_K_kz = 0
                                D_qiz_wrt_Xmk = None
                                if i == m:
                                    D_qiz_wrt_Xmk = W_Q_kz
                                    W_Q_kz = W_Q[k,z]
                                else:
                                    D_qiz_wrt_Xmk = 0
                                D_qiz_times_kzt1_wrt_Xmk = (D_qiz_wrt_Xmk)*(kzt1) + (qiz)*(D_kzt1_wrt_Xmk)
                                Sigma_z1_zdk_of_D_qiz_times_kzt1_wrt_Xmk += D_qiz_times_kzt1_wrt_Xmk
                            D_Sit1_wrt_Xmk = (1/np.sqrt(dk).astype(np.float16)) * Sigma_z1_zdk_of_D_qiz_times_kzt1_wrt_Xmk
                            D_e_to_Sit1_wrt_Xmk = np.exp(S[i,t1]) * D_Sit1_wrt_Xmk
                            g_prime += D_e_to_Sit1_wrt_Xmk

                        # Calculate f_prime
                        f_prime = 0
                        D_e_to_Sit_wrt_Xmk = np.exp(S[i,t]) * (D_Sit_wrt_Xmk)
                        Sigma_z1_zdk_of_D_qiz_times_kzt_wrt_Xmk = 0
                        for z in range(dk):
                            qiz = Q[i,z]
                            K_transposed = K.T
                            kzt = K_transposed[z,t]
                            D_kzt_wrt_Xmk = None
                            if t == m:
                                D_kzt_wrt_Xmk = W_K_kz
                                W_K_kz = W_K[k,z]
                            else:
                                D_kzt_wrt_Xmk = W_K_kz = 0
                            D_qiz_wrt_Xmk = None
                            if i == m:
                                D_qiz_wrt_Xmk = W_Q_kz
                                W_Q_kz = W_Q[k,z]
                            else:
                                D_qiz_wrt_Xmk = 0
                            D_qiz_times_kzt1_wrt_Xmk = (D_qiz_wrt_Xmk)*(kzt) + (qiz)*(D_kzt_wrt_Xmk)
                            Sigma_z1_zdk_of_D_qiz_times_kzt_wrt_Xmk += D_qiz_times_kzt1_wrt_Xmk
                        D_Sit_wrt_Xmk = (1/np.sqrt(dk).astype(np.float16)) * Sigma_z1_zdk_of_D_qiz_times_kzt_wrt_Xmk
                        D_e_to_Sit_wrt_Xmk = np.exp(S[i,t]) * D_Sit_wrt_Xmk
                        f_prime = D_e_to_Sit_wrt_Xmk

                        D_Pit_wrt_Xmk = ( (g)*(f_prime) - (f)*(g_prime) ) / (g)^2
                        Vtj = V[t,j]
                        Pit = Softmax_S[i,t]
                        D_Vtj_wrt_Xmk = None
                        if t == m:
                            D_Vtj_wrt_Xmk = W_V[k,j]
                        else:
                            D_Vtj_wrt_Xmk = 0
                        D_Pit_times_Vtj_wrt_Xmk = (D_Pit_wrt_Xmk)*(Vtj) + (Pit)*(D_Vtj_wrt_Xmk)
                        D_Aij_wrt_Xmk += D_Pit_times_Vtj_wrt_Xmk
                    jacobian_Ai_x[rowJ, colJ] = D_Aij_wrt_Xmk
                    colJ += 1
                    if colJ >= (sequence_length * model_dimension):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length * dk):
                            break



    jacobian_Ai_x_NonJacobian = To_Nonjacobian(jacobian_Ai_x, sequence_length, dk)
    return  jacobian_Ai_x_NonJacobian
def Compute_D_O_wrt_X_for_Encoder_Sefl_Attention():
    """

    """
    A = []
    for i in range(number_of_heads):
        A.append(compute_derivative_attention_i_wrt_X_for_Encoder_self_attention())
    D_Concat_wrt_X_nonjacobian = concatenate(A)
    W_O_Numpy = W_O.get()
    D_O_wrt_X_nonjacobian = D_Concat_wrt_X_nonjacobian @ W_O_Numpy
    D_O_wrt_X = To_Jacobian(D_O_wrt_X_nonjacobian)
    return D_O_wrt_X

def compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input, FF_Input):   # refer to page 9 and 10
    """
    md - model_dimension
    fd - feed forward dimension
    Compute the derivative of FF wrt input X.
    W_FF1 - feed forward weights for FF which derivative is being computed for  (md, fd)
    W_FF2 - feed forward weights for FF which derivative is being computed for  (fd, md)
    ReLu_Input - input to relu function which for multiple sequences should be (B, sq or sq_O, fd)  sq if Decoder is false and sq_O if Decoder is true
    FF_Input - input to feed forward layer which should have a shape (B, sq or sq_O, md) sq if Decoder is false and sq_O if Decoder is true
    Decoder - boolean variable where True means FF derivative referring to decoder and False means FF derivative referring to encoder

    Parameters:


    Returns:
    - jacobian: Derivative of FF wrt input X (sequence_length times model_dimension) by (sequence_length times model_dimension)
    """
    sequence_length_I = None
    if Decoder: # Decoder is a boolean variable
        sequence_length_I = sequence_length_O
    else:
        sequence_length_I = sequence_length

    # Get dimensions (including batch size)
    batch_size, S, F = ReLu_Input.shape

    # Compute the ReLU derivative mask
    D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)


    # Flatten the indices for the output 4D matrix
    ColR = np.arange(batch_size * S * F)






    # Create an empty 4D matrix
    D_ReLuInput_wrt_Input_nonjacobian = np.zeros((batch_size, S * F, S, F))

    # Populate the matrix with the computed values
    # Adjust the indexing to handle the batch dimension

    a0 = np.repeat(np.arange(batch_size), S * F)
    a1 = ColR % (S * F)
    #a2 = np.tile(np.arange(S), batch_size * F)
    if((S*F)%2!=0):
        print("Error: S*F is not even")
        exit("Goodbye")
    if((batch_size * F)%2!=0):
        print("Error: batch_size * F is not even")
        exit("Goodbye")


    a2 = np.tile(np.repeat(np.arange(S), F), batch_size)
    a3 = np.tile(np.arange(F), batch_size * S)
    val = D_ReLuXij_wrt_Xij.flatten()

    shapes = {a0.shape, a1.shape, a2.shape, a3.shape, val.shape}  # Convert to a set
    if len(shapes) > 1:  # All shapes are unique
        print("All shapes are different!")
        exit("Goodbye")
    D_ReLuInput_wrt_Input_nonjacobian[
        a0,
        a1,
        a2,
        a3

    ] = val








    #D_ReLuInput_wrt_Input = To_Jacobian(D_ReLuInput_wrt_Input_nonjacobian) # Numpy
    D_ReLuInput_wrt_Input = reshape_cupy_4d_to_3d(D_ReLuInput_wrt_Input_nonjacobian)

    D_X_wrt_X = np.stack([np.eye(sequence_length_I * model_dimension) for _ in range(batch_size)])
    B, Rows, Cols = FF_Input.shape
    #D_X_wrt_X_Nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols)  # Numpy
    D_X_wrt_X_Nonjacobian = reshape_cupy_3d_to_4d(D_X_wrt_X, Rows, Cols)


    #W_FF1_Numpy = W_FF1.get()
    W_FF1_Numpy = W_FF1
    D_Input_wrt_X_Nonjacobian = D_X_wrt_X_Nonjacobian @ W_FF1_Numpy
    #D_Input_wrt_X = To_Jacobian(D_Input_wrt_X_Nonjacobian) # Numpy
    D_Input_wrt_X = reshape_cupy_4d_to_3d(D_Input_wrt_X_Nonjacobian) # Cupy


    D_ReLuInput_wrt_X = D_ReLuInput_wrt_Input @ D_Input_wrt_X
    B, Rows, Cols = ReLu_Input.shape
    #D_ReLuInput_wrt_X_Nonjacobian = To_Nonjacobian(D_ReLuInput_wrt_X, Rows, Cols)  # numpy
    D_ReLuInput_wrt_X_Nonjacobian = reshape_cupy_3d_to_4d(D_ReLuInput_wrt_X, Rows, Cols)
    #W_FF2_Numpy = W_FF2.get()
    W_FF2_Numpy = W_FF2
    D_FF_wrt_X_Nonjacobian = D_ReLuInput_wrt_X_Nonjacobian @ W_FF2_Numpy
    #D_FF_wrt_X = To_Jacobian(D_FF_wrt_X_Nonjacobian) # Numpy
    D_FF_wrt_X = reshape_cupy_4d_to_3d(D_FF_wrt_X_Nonjacobian)

    return D_FF_wrt_X
"""
def compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input, FF_Input):   # refer to page 9 and 10
    ""
    Compute the derivative of FF wrt input X.
    W_FF1 - feed forward weights for FF which derivative is being computed for
    W_FF2 - feed forward weights for FF which derivative is being computed for
    input - input to the ReLu function when computing feed forward
    Decoder - boolean variable where True means FF derivative referring to decoder and False means FF derivative referring to encoder

    Parameters:


    Returns:
    - jacobian: Derivative of FF wrt input X (sequence_length times model_dimension) by (sequence_length times model_dimension)
    ""
    sequence_length_I = None
    if Decoder: # Decoder is a boolean variable
        sequence_length_I = sequence_length_O
    else:
        sequence_length_I = sequence_length
    ""
    #Numpy
    D_ReLuInput_wrt_Input_nonjacobian =   cp.empty((sequence_length_I , feed_forward_dimension), dtype=object)

    # (sequence_length_I * feed_forward_dimension, sequence_length_I , feed_forward_dimension)
    rowJ = 0
    colJ = 0
    for i in range(sequence_length_I):
        for j in range(feed_forward_dimension):

            D_ReLuXij_wrt_ReLuInput = cp.zeros((1, (sequence_length_I * feed_forward_dimension) ))
            D_ReLuXij_wrt_Xij = None
            input = ReLu_Input[i,j]
            if input < 0:
                D_ReLuXij_wrt_Xij = 0
            else:
                D_ReLuXij_wrt_Xij = 1
            ColR = i * (feed_forward_dimension) + j
            D_ReLuXij_wrt_ReLuInput[0,ColR] = D_ReLuXij_wrt_Xij
            D_ReLuInput_wrt_Input_nonjacobian[i,j] = D_ReLuXij_wrt_ReLuInput


            ""
            for m in range(sequence_length_I):
                for k in range(model_dimension):
                    D_fij_wrt_Xmk = 0
                    for r in range(feed_forward_dimension):
                        W_FF2_rj = W_FF2[r,j]


                        D_ReLu_of_input_wrt_input = None
                        input = ReLu_Input[i,r]
                        if input < 0:
                             D_ReLu_of_input_wrt_input = 0
                        else:
                            D_ReLu_of_input_wrt_input = 1

                        D_input_wrt_Xmk = None
                        if i == m:
                            D_input_wrt_Xmk = W_FF1[k,r]
                        else:
                            D_input_wrt_Xmk = 0

                        D_ReLu_of_input_wrt_Xmk = D_ReLu_of_input_wrt_input * D_input_wrt_Xmk
                        D_fij_wrt_Xmk += D_ReLu_of_input_wrt_Xmk * W_FF2_rj

                    jacobian_FF_x[rowJ, colJ] = D_fij_wrt_Xmk
                    colJ += 1
                    if colJ >= (sequence_length_I * model_dimension):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length_I * model_dimension):
                            break
            ""
    ""
    # Cupy
    # Get dimensions
    S, F = ReLu_Input.shape  # sequence_length_I and feed_forward_dimension

    # Compute the ReLU derivative mask
    D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)

    # Flatten the indices for the output 3D matrix
    ColR = np.arange(S * F)

    # Create an empty 3D matrix
    D_ReLuInput_wrt_Input_nonjacobian = np.zeros((S * F, S, F))

    # Populate the matrix with the computed values
    D_ReLuInput_wrt_Input_nonjacobian[ColR, ColR // F, ColR % F] = D_ReLuXij_wrt_Xij.flatten()
    #############
    #D_ReLuInput_wrt_Input = To_Jacobian(D_ReLuInput_wrt_Input_nonjacobian) # Numpy
    D_ReLuInput_wrt_Input = reshape_cupy_3d_to_2d(D_ReLuInput_wrt_Input_nonjacobian)


    D_X_wrt_X = np.eye(sequence_length_I * model_dimension)
    Rows, Cols = FF_Input.shape
    #D_X_wrt_X_Nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols)  # Numpy
    D_X_wrt_X_Nonjacobian = reshape_cupy_2d_to_3d(D_X_wrt_X, Rows, Cols)


    #W_FF1_Numpy = W_FF1.get()
    W_FF1_Numpy = W_FF1
    D_Input_wrt_X_Nonjacobian = D_X_wrt_X_Nonjacobian @ W_FF1_Numpy
    #D_Input_wrt_X = To_Jacobian(D_Input_wrt_X_Nonjacobian) # Numpy
    D_Input_wrt_X = reshape_cupy_3d_to_2d(D_Input_wrt_X_Nonjacobian) # Cupy


    D_ReLuInput_wrt_X = D_ReLuInput_wrt_Input @ D_Input_wrt_X
    Rows, Cols = ReLu_Input.shape
    #D_ReLuInput_wrt_X_Nonjacobian = To_Nonjacobian(D_ReLuInput_wrt_X, Rows, Cols)  # numpy
    D_ReLuInput_wrt_X_Nonjacobian = reshape_cupy_2d_to_3d(D_ReLuInput_wrt_X, Rows, Cols)
    #W_FF2_Numpy = W_FF2.get()
    W_FF2_Numpy = W_FF2
    D_FF_wrt_X_Nonjacobian = D_ReLuInput_wrt_X_Nonjacobian @ W_FF2_Numpy
    #D_FF_wrt_X = To_Jacobian(D_FF_wrt_X_Nonjacobian) # Numpy
    D_FF_wrt_X = reshape_cupy_3d_to_2d(D_FF_wrt_X_Nonjacobian)

    return D_FF_wrt_X

"""

def compute_derivative_Linear(W_L):
    """
    Compute the derivative of Linear wrt input X efficiently using broadcasting.

    Parameters:
    - W_L: (model_dimension, vocab_size) linear layer weight matrix
    - sequence_length_O: int, sequence length
    - vocab_size: int, vocabulary size
    - model_dimension: int, model dimension

    Returns:
    - jacobian: (sequence_length_O * vocab_size, sequence_length_O * model_dimension) Jacobian matrix
    """

    # Create the block-diagonal Jacobian using Kronecker product
    identity_seq = np.eye(sequence_length_O)  # Identity matrix of size sequence_length_O
    jacobian = np.kron(identity_seq, W_L.T)  # Kronecker product to distribute W_L along the sequence

    return jacobian
"""
def compute_derivative_Linear(W_L): # refer to page 11
    ""
    Compute the derivative of Linear wrt input X.
    W_L - linear layer weight matrix
    Parameters:


    Returns:
    - jacobian: Derivative of Linear wrt input X (sequence_length times vocab size) by (sequence_length times model_dimension)
    ""
    jacobian_Li_x =   np.zeros((sequence_length_O * vocab_size, sequence_length_O * model_dimension))
    rowJ = 0
    colJ = 0

    for i in range(sequence_length_O):
        for j in range(vocab_size):
            for m in range(sequence_length_O):
                for k in range(model_dimension):
                    D_Lij_wrt_Xmk = None
                    if i == m:
                        D_Lij_wrt_Xmk = W_L[k,j]
                    else:
                        D_Lij_wrt_Xmk = 0

                    jacobian_Li_x[rowJ, colJ] = D_Lij_wrt_Xmk
                    colJ += 1
                    if colJ >= (sequence_length_O * model_dimension):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length_O * vocab_size):
                            break

    return jacobian_Li_x

"""

def compute_Attention_i_wrt_ki():
    """
    Compute the derivative of Attention i wrt ki.

    Parameters:


    Returns:
    - jacobian: Derivative of Attention i wrt input ki (sequence_length times dk) by (dk times sequence_length )
     """
    jacobian_Ai_ki =   np.zeros((sequence_length * dk, dk * sequence_length))
    rowJ = 0
    colJ = 0
    for i in range(sequence_length):
        for j in range(dk):
            for k in range(sequence_length):
                for m in range(dk):
                    D_Aij_wrt_Kmk = 0
                    for t in range(sequence_length):
                        f = np.exp(S[i,t])
                        g = 0
                        for t1 in range(sequence_length):
                            g += np.exp(S[i,t1])
                        g_prime = None
                        for t1 in range(sequence_length):
                            sigma_z1_zdk_of_qiz_times_D_Kzt1_wrt_Kmk = 0
                            for z in range(dk):
                                D_Kzt1_wrt_Kmk = None
                                if z == m and t1 == k:
                                    D_Kzt1_wrt_Kmk = 1
                                else:
                                    D_Kzt1_wrt_Kmk = 0
                                term = Q[i,z] * (D_Kzt1_wrt_Kmk)
                                sigma_z1_zdk_of_qiz_times_D_Kzt1_wrt_Kmk += term
                            D_Sit1_wrt_Kmk = (1/np.sqrt(dk).astype(np.float16)) * (sigma_z1_zdk_of_qiz_times_D_Kzt1_wrt_Kmk)
                            D_e_to_Sit1_wrt_Kmk = np.exp(S[i,t1]) * D_Sit1_wrt_Kmk
                            g_prime += D_e_to_Sit1_wrt_Kmk

                        f_prime = None
                        sigma_z1_zdk_of_qiz_times_D_Kzt1_wrt_Kmk = 0
                        for z in range(dk):
                            D_Kzt1_wrt_Kmk = None
                            if z == m and t == k:
                                D_Kzt1_wrt_Kmk = 1
                            else:
                                D_Kzt1_wrt_Kmk = 0
                            term = Q[i,z] * (D_Kzt1_wrt_Kmk)
                            sigma_z1_zdk_of_qiz_times_D_Kzt1_wrt_Kmk += term
                        D_Sit1_wrt_Kmk = (1/np.sqrt(dk).astype(np.float16)) * (sigma_z1_zdk_of_qiz_times_D_Kzt1_wrt_Kmk)
                        D_e_to_Sit1_wrt_Kmk = np.exp(S[i,t]) * D_Sit1_wrt_Kmk
                        f_prime = D_e_to_Sit1_wrt_Kmk


                        D_Pit_wrt_Kmk = ((g)*(f_prime) - (f)*(g_prime)) / (g)^2
                        vtj_times_D_Pit_wrt_Kmk = V[t,j] * D_Pit_wrt_Kmk
                        D_Aij_wrt_Kmk += vtj_times_D_Pit_wrt_Kmk
                    jacobian_Ai_ki[rowJ, colJ]
                    colJ += 1
                    if colJ >= (dk * sequence_length):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length * dk):
                            break

    return jacobian_Ai_ki


def compute_Attention_i_wrt_vi():
    """
    Compute the derivative of Attention i wrt vi.

    Parameters:


    Returns:
    - jacobian: Derivative of Attention i wrt vi (sequence_length times dk) by (sequence_length times dk)
     """
    jacobian_Ai_vi =   np.zeros((sequence_length * dk, sequence_length * dk))
    rowJ = 0
    colJ = 0
    for i in range(sequence_length):
        for j in range(dk):
            for m in range(sequence_length):
                for k in range(dk):
                    D_Aij_wrt_Vmk = 0
                    for z in range(sequence_length):
                        D_Vzj_wrt_Vmk = None
                        if j == k and z == m:
                            D_Vzj_wrt_Vmk = 1
                        else:
                            D_Vzj_wrt_Vmk = 0
                        term = P[i,z] * D_Vzj_wrt_Vmk
                        D_Aij_wrt_Vmk += term
                    jacobian_Ai_vi[rowJ, colJ] = D_Aij_wrt_Vmk
                    colJ += 1
                    if colJ >= (sequence_length * dk):
                        colJ = 0
                        rowJ += 1
                        if rowJ >= (sequence_length * dk):
                            break
    return jacobian_Ai_vi
def D_Lavg_wrt_X(X, Y):
    """
    Compute the Jacobian matrix of the derivative of Lavg with respect to X for a batch.

    Parameters:
    X - shape (batch_size, sequence_length_O, vocab_size)
    Y - shape (batch_size, sequence_length_O, vocab_size)

    Returns:
    jacobian_Lavg_wrt_X of shape (batch_size, 1, sequence_length_O * vocab_size)
    """
    batch_size, sequence_length_O, vocab_size = X.shape

    # Compute the derivative for all elements at once
    D_ln_X = np.where(X > 0, 1 / X, 0)  # Compute log derivative safely
    D_Lz1_wrt_X = -np.sum(Y * D_ln_X, axis=2, keepdims=True)  # Summing over vocab_size

    # Compute final Jacobian
    D_Lavg_wrt_X = (1 / sequence_length_O) * D_Lz1_wrt_X * Y

    # Reshape to the required (batch_size, 1, sequence_length_O * vocab_size)
    return D_Lavg_wrt_X.reshape(batch_size, 1, -1)
"""
def D_Lavg_wrt_X(X, Y):
    ""
    Compute the Jacobian matrix of the derivative of Lavg with respect to X.

    Parameters:
    X - shape (sequence_length_O, vocab_size)
    Y - shape (sequence_length_O, vocab_size)

    Returns:
    jacobian_Lavg_wrt_X of shape (1, sequence_length_O * vocab_size)
    ""
    sequence_length_O, vocab_size = X.shape

    # Compute the derivative for all elements at once
    D_ln_X = np.where(X > 0, 1 / X, 0)  # Compute log derivative safely
    D_Lz1_wrt_X = -np.sum(Y * D_ln_X, axis=1, keepdims=True)  # Summing over vocab_size

    # Compute final Jacobian
    D_Lavg_wrt_X = (1 / sequence_length_O) * D_Lz1_wrt_X * Y

    # Reshape to the required (1, sequence_length_O * vocab_size)
    return D_Lavg_wrt_X.ravel().reshape(1, -1)

"""
"""

def D_Lavg_wrt_X(X, Y): # refer to page 15 and 12
    ""
    Input:
    X - shape (sequence length output by vocab size)
    Y - true output labels of shape (sequence length output by vocab size)
    Output:
    jacobian matrix of the derivative of Lavg wrt X of shape (1 by (sequence length output times vocab size) )
    ""
    jacobian_Lavg_wrt_X = np.zeros((1, sequence_length_O * vocab_size))

    ColJ = 0
    for i in range(sequence_length_O):
        for j in range(vocab_size):
            D_Lavg_wrt_Xij = None
            sum = 0
            for z in range(sequence_length_O):
                D_Lz1_wrt_Xij = None
                sumt = 0
                for t in range(vocab_size):
                     D_ln_Xzt_wrt_Xij = None
                     if z == i and t == j:
                         D_ln_Xzt_wrt_Xij = (1/X[i,j])
                         #p rint("X[i,j] = ", X[i,j])
                     else:
                         D_ln_Xzt_wrt_Xij = 0
                     sumt += Y[z,t] * D_ln_Xzt_wrt_Xij
                #p rint("sumt = ", sumt)
                D_Lz1_wrt_Xij = -sumt
                sum += D_Lz1_wrt_Xij
            #p rint("sum = ", sum)
            D_Lavg_wrt_Xij = (1/sequence_length_O) * (sum)
            #p rint("D_Lavg_wrt_Xij = ", D_Lavg_wrt_Xij)
            jacobian_Lavg_wrt_X[0, ColJ] = D_Lavg_wrt_Xij
            ColJ += 1
            if ColJ >= (sequence_length_O * vocab_size):
                ColJ = 0
                break
    return jacobian_Lavg_wrt_X
"""
"""
def Derivative_SoftmaxS_wrt_S(P, S): # refer to page 3
    ""
    S
    P = Softmax(S) - matrix of shape Rows_Softmax_S by Columns_Softmax_S where each element is P[row,col]
    P[row,col] = element in Softmax(S)

    Output:
    Jacobian of Softmax S wrt to S
    ""

    Rows_Softmax_S = P.shape[0]
    Columns_Softmax_S = P.shape[1]
    Rows_S = S.shape[0]
    Columns_S = S.shape[1]
    Jacobian_SoftmaxS_wrt_S = np.zeros(( (Rows_Softmax_S * Columns_Softmax_S),(Rows_S * Columns_S)  ))
    RowJ = 0
    ColJ = 0
    for i in range(Rows_Softmax_S):
        for j in range(Columns_Softmax_S):
            for m in range(Rows_S):
                for k in range(Columns_S):
                    if i == m and j == k:
                        D_Pij_wrt_Smk = P[i,j] * (1 - P[m,k])
                    if i == m and j != k:
                        D_Pij_wrt_Smk = -P[m,k] * P[i,j]
                    if i != m:
                        D_Pij_wrt_Smk = 0
                    #p rint("D_Pij_wrt_Smk = ", D_Pij_wrt_Smk)
                    Jacobian_SoftmaxS_wrt_S[RowJ, ColJ] = D_Pij_wrt_Smk
                    ColJ += 1
                    if ColJ >= (Rows_S * Columns_S):
                        ColJ = 0
                        RowJ += 1
                        if RowJ >= (Rows_Softmax_S * Columns_Softmax_S):
                            break
                if RowJ >= (Rows_Softmax_S * Columns_Softmax_S):
                    break
            if RowJ >= (Rows_Softmax_S * Columns_Softmax_S):
                break
        if RowJ >= (Rows_Softmax_S * Columns_Softmax_S):
            break
    return Jacobian_SoftmaxS_wrt_S
"""

def Derivative_SoftmaxS_wrt_S(S, R):
    """
    Compute the full 2D Jacobian of softmax for a batch of 2D CuPy arrays without a for loop.
    S: (batch_size, m, n) softmax output
    Returns: (batch_size, m*n, m*n) Jacobian matrices
    """
    batch_size, m, n = S.shape
    S_expanded = S[..., None]  # Shape (batch_size, m, n, 1)

    # Create batch of identity matrices
    I_n = np.eye(n, dtype=S.dtype)[None, None, :, :]  # Shape (1, 1, n, n), broadcastable

    # Compute batched Jacobians (batch_size, m, n, n)
    J_blocks = I_n * S[..., None] - S_expanded @ S_expanded.transpose(0, 1, 3, 2)

    # Efficient block-diagonal placement
    J_full = np.zeros((batch_size, m * n, m * n), dtype=S.dtype)

    # Get indices for placing each block in the full matrix
    block_indices = np.arange(m)[:, None] * n  # Shape (m, 1)

    # Precompute row_idx and col_idx
    row_idx = np.arange(n).repeat(n)  # (n * n,)
    col_idx = np.tile(np.arange(n), n)  # (n * n,)

    # Compute batch-wise indices
    row_indices = (block_indices + row_idx).reshape(1, m, n * n)  # Shape (1, m, n*n)
    col_indices = (block_indices + col_idx).reshape(1, m, n * n)  # Shape (1, m, n*n)

    # Expand batch dimension
    row_indices = np.tile(row_indices, (batch_size, 1, 1))  # Shape (batch_size, m, n*n)
    col_indices = np.tile(col_indices, (batch_size, 1, 1))  # Shape (batch_size, m, n*n)

    # Reshape J_blocks to match the expected shape
    J_blocks = J_blocks.reshape(batch_size, m, n * n)

    # Use np.add.at for efficient assignment
    np.add.at(J_full, (np.arange(batch_size)[:, None, None], row_indices, col_indices), J_blocks)

    return J_full

"""
def Derivative_SoftmaxS_wrt_S(S, R):
    ""
    Compute the full 2D Jacobian of softmax for a 2D CuPy array without a for loop.
    S: (m, n) softmax output
    Returns: (m*n, m*n) Jacobian matrix
    ""
    start_time_T = time.time()  # Record start time
    m, n = S.shape
    S_expanded = S[..., None]  # Shape (m, n, 1)

    # Create a batch of identity matrices (broadcasted over m samples)
    I_n = np.eye(n, dtype=S.dtype)[None, :, :]  # Shape (1, n, n), broadcastable

    # Compute batched Jacobians (m, n, n)
    J_blocks = I_n * S[..., None] - S_expanded @ S_expanded.transpose(0, 2, 1)

    # Efficient block-diagonal placement:
    J_full = np.zeros((m * n, m * n), dtype=S.dtype)

    end_time_T = time.time()  # Record end time
    elapsed_time = end_time_T - start_time_T  # Calculate elapsed time
    ##print(f"Elapsed time for SoftmaxS wrt S: {elapsed_time:.6f} seconds")

    # Get indices for placing each block in the full matrix
    block_indices = np.arange(m) * n

    # Create CuPy events for GPU timing
    start_event = np.cuda.Event()
    end_event = np.cuda.Event()

    # Record the start of GPU operation
    start_event.record()
    ###########################################################

    # Precompute row_idx and col_idx once, outside any loops
    row_idx = np.arange(n).repeat(n)  # (n * n,)
    col_idx = np.tile(np.arange(n), n)  # (n * n,)

    # Compute batch-wise indices with optimized broadcasting
    batch_offsets = block_indices[:, None]  # Shape (m, 1)
    row_indices = batch_offsets + row_idx  # Broadcasting
    col_indices = batch_offsets + col_idx  # Broadcasting

    # Use cp.add.at for more efficient assignment
    np.add.at(J_full, (row_indices.ravel(), col_indices.ravel()), J_blocks.ravel())
    #####################################3
    # Record the end of GPU operation
    end_event.record()

    # Wait for the events to complete
    end_event.synchronize()


    #!export LC_ALL=C.UTF-8
    #!export LANG=C.UTF-8
    #!nvidia-smi


    # Measure elapsed GPU time
    elapsed_gpu_time = np.cuda.get_elapsed_time(start_event, end_event) / 1000  # Convert to seconds
    ##print(np.get_default_memory_pool().used_bytes())

    ##print(f"Elapsed time for SoftmaxS wrt S (GPU): {elapsed_gpu_time:.6f} seconds")



    return J_full
"""
"""
def Derivative_SoftmaxS_wrt_S(S, R):
    ""
    Compute the full 2D Jacobian of softmax for a 2D CuPy array.
    S: (m, n) softmax output
    Returns: (m*n, m*n) Jacobian matrix
    ""
    m, n = S.shape
    S_expanded = S[..., None]  # Shape (m, n, 1)

    # Create a batch of identity matrices
    I_n = np.tile(np.eye(n, dtype=S.dtype)[None, :, :], (m, 1, 1))  # Shape (m, n, n)

    # Compute batched Jacobians (m, n, n)
    J_blocks = I_n * S[..., None] - S_expanded @ S_expanded.transpose(0, 2, 1)

    # Convert to full 2D Jacobian (m*n, m*n) as a block diagonal matrix
    J_full = np.zeros((m * n, m * n), dtype=S.dtype)
    start_time_T = time.time()  # Record start time
    for i in range(m):
        J_full[i*n:(i+1)*n, i*n:(i+1)*n] = J_blocks[i]  # Insert blocks along diagonal

    end_time_T = time.time()  # Record end time
    elapsed_time = end_time_T - start_time_T  # Calculate elapsed time
    #print(f"Elapsed time for SoftmaxS wrt S: {elapsed_time:.6f} seconds")
    return J_full


"""
def To_Jacobian(NJ):
    """
    Input:
    NJ - NonJacobian Matrix where each element is a row vector of shape (1 x ColsJ)
    Rows - number of rows in NonJacobian (leibnitz distributed)
    Cols - number of columns in NonJacobian (leibnitz distributed)
    ColsJ - number of columns in Jacobian
    Output:
    Jacobian - shape is ((Rows * Cols) , ColsJ)   (returns cupy matrix)
    """
    Rows = NJ.shape[0]
    Cols = NJ.shape[1]
    ColsJ = NJ[0,0].shape[1]  # obtain number of columns for the first element
    Jacobian = np.zeros(((Rows * Cols), ColsJ))
    RowJ = 0
    for i in range(Rows):
        for j in range(Cols):
            NJ_ij = NJ[i, j]
            for k in range(ColsJ):
                Jacobian[RowJ, k] = NJ_ij[0, k]
            RowJ += 1
            if RowJ >= (Rows * Cols):
                break
    return Jacobian

def To_Nonjacobian(J,Rows, Cols):
    """
    Input:
    J - Jacobian Matrix
    X - what is being differentiated (for example J is the jacobian matrix of D_X_wrt_input)
    Rows - number of rows in NonJacobian (leibnitz distributed)
    Cols - number of columns in NonJacobian (leibnitz distributed)
    ColsJ - number of columns in Jacobian
    Output:
    NonJacobian - shape is what you differentiating to  (returns numpy matrix)
    """


    ColsJ = J.shape[1]
    NonJacobian = cp.empty((Rows , Cols), dtype=object)
    Current_RowJ = 0
    for i in range(Rows):
        for j in range(Cols):
            Element_ij_NonJacobian = cp.zeros((1,ColsJ))
            for k in range(ColsJ):
                Element_ij_NonJacobian[0,k] = J[Current_RowJ, k]
            NonJacobian[i,j] = Element_ij_NonJacobian
            Current_RowJ += 1
            if Current_RowJ >= (Rows * Cols):
                break

    return NonJacobian
######################
def compute_D_Attention_i_wrt_X_NonJacobianForm_SA(P, S, W_Q_i, W_K_i, W_V_i, Q_i, K_i, V_i, X, D_XT_wrt_X, D_X_wrt_X_nonjacobian):       # refer to page 24 (similar to MA)
    """
    Helper function for compute_D_MaskedMAO_wrt_itsInput()
    Input:
    W_Q_i - query weight matrix for the ith attention head
    W_K_i - key weight matrix for the ith attention head
    W_V_i - value weight matrix for the ith attention head
    Q_i - query matrix for the ith attention head
    K_i - key marix for the ith attention head
    V_i - value marix for the ith attention head
    P = Softmax(S)


    Output:
    NonJacobian of the Derivative of Attention i wrt its input X (shape ((sequence_length_O * model_dimension) by (sequence_length_O * model_dimension)) )
    """
    """
    K_i_T = K_i.T
    # Calculate D_Attention_i_wrt_X_NonJacobianForm
    D_Attention_i_wrt_X_NonJacobianForm = cp.empty((sequence_length , dk), dtype=object) #np.zeros((sequence_length, dk))

    start_time = time.time()  # Record start time

    for i in range(sequence_length):
        for j in range(dk):


            D_Aij_wrt_X = cp.zeros((1, sequence_length * model_dimension ))   # back to cp later
            col_D_Aij_wrt_X = 0
            for m in range(sequence_length):
                for k in range(model_dimension):



                    sum = 0   # D_Aij_wrt_Xmk = sum
                    for z in range(sequence_length):
                        # calculate g_prime




                        g_prime = 0
                        for t1 in range(sequence_length):
                            sum_D_Sit1_wrt_Xmk = 0
                            for r in range(dk):
                                D_qir_wrt_Xmk = None
                                if i==m:
                                    D_qir_wrt_Xmk = W_Q_i[k,r]
                                else:
                                    D_qir_wrt_Xmk = 0
                                D_KiTrt1_wrt_Xmk = None
                                if m == t1:
                                    D_KiTrt1_wrt_Xmk = W_K_i[k,r]
                                else:
                                    D_KiTrt1_wrt_Xmk = 0
                                term_D_Sit1_wrt_Xmk = ( D_qir_wrt_Xmk * K_i_T[r,t1] ) + (Q_i[i,r] * D_KiTrt1_wrt_Xmk)

                                sum_D_Sit1_wrt_Xmk += term_D_Sit1_wrt_Xmk
                            D_Sit1_wrt_Xmk = (1/(cp.sqrt(dk))) * (sum_D_Sit1_wrt_Xmk)
                            term_g_prime = cp.exp(S[i,t1]) * D_Sit1_wrt_Xmk
                            g_prime += term_g_prime



                        # calculate f_prime
                        f_prime = 0
                        sum_D_Siz_wrt_Xmk = 0
                        for r in range(dk):
                            D_qir_wrt_Xmk = None
                            if i==m:
                                D_qir_wrt_Xmk = W_Q_i[k,r]
                            else:
                                D_qir_wrt_Xmk = 0
                            D_KiTrz_wrt_Xmk = None
                            if m == z:
                                D_KiTrz_wrt_Xmk = W_K_i[k,r]
                            else:
                                D_KiTrz_wrt_Xmk = 0
                            term_D_Siz_wrt_Xmk = ( D_qir_wrt_Xmk * K_i_T[r,z] ) + (Q_i[i,r] * D_KiTrz_wrt_Xmk)
                            sum_D_Siz_wrt_Xmk += term_D_Siz_wrt_Xmk
                        D_Siz_wrt_Xmk = (1/(np.sqrt(dk).astype(np.float16))) * (sum_D_Siz_wrt_Xmk)
                        f_prime = np.exp(S[i,z]) * D_Siz_wrt_Xmk



                        # f and g defined
                        f = cp.exp(S[i,z])
                        g = 0
                        for t1 in range(sequence_length):
                            g += cp.exp(S[i,t1])

                        # calculate D_Vzj_wrt_Xmk
                        D_Vzj_wrt_Xmk = None
                        if z == m:
                            D_Vzj_wrt_Xmk = W_V_i[k,j]
                        else:
                            D_Vzj_wrt_Xmk = 0

                        D_Piz_wrt_Xmk = ( ( (g)*(f_prime) )- ( (f)*(g_prime) ) ) / (g)**2
                        term = ( (D_Piz_wrt_Xmk) * (V_i[z,j]) ) + ( (P[i,z]) * (D_Vzj_wrt_Xmk) )
                        sum += term

                    D_Aij_wrt_Xmk = sum



                    D_Aij_wrt_X[0,col_D_Aij_wrt_X] =  D_Aij_wrt_Xmk
                    col_D_Aij_wrt_X += 1
                    if col_D_Aij_wrt_X >= (sequence_length_O * model_dimension):
                        col_D_Aij_wrt_X = 0
                        break


            D_Attention_i_wrt_X_NonJacobianForm[i, j] = D_Aij_wrt_X




    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Attention_i_wrt_X_NonJacobianForm : {elapsed_time:.6f} seconds")
    """
    """
    D_X_wrt_X = np.eye(sequence_length * model_dimension) # identity matrix
    Rows = sequence_length
    Cols = model_dimension
    #D_X_wrt_X_nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols) #Numpy
    D_X_wrt_X_nonjacobian = reshape_cupy_2d_to_3d(D_X_wrt_X, Rows, Cols) # Cupy
    """


    #W_i_Q_Numpy = W_Q_i.get()
    W_i_Q_Numpy = W_Q_i
    D_Qi_wrt_X_nonjacobian = D_X_wrt_X_nonjacobian @ W_i_Q_Numpy

    W_K_iT = W_K_i.T

    #WiKT_Numpy = W_K_iT.get()
    WiKT_Numpy = W_K_iT





    #XT = X.T
    XT = np.transpose(X, (0, 2, 1))
    B, Rows, Cols = XT.shape

    #D_XT_wrt_X_Nonjacobian = To_Nonjacobian(D_XT_wrt_X, Rows, Cols)  # Numpy

    D_XT_wrt_X_Nonjacobian = reshape_cupy_3d_to_4d(D_XT_wrt_X, Rows, Cols)  # Cupy
    D_KiT_wrt_X_nonjacobian = WiKT_Numpy @ D_XT_wrt_X_Nonjacobian

    #KiT = K_i.T
    KiT = np.transpose(K_i, (0, 2, 1))
    #KiT_numpy = KiT.get()
    #Q_i_Numpy = Q_i.get()

    KiT_numpy = KiT
    KiT_numpy = KiT_numpy[:,None,:,:]
    Q_i_Numpy = Q_i
    Q_i_Numpy = Q_i_Numpy[:,None,:,:]

    A = Q_i_Numpy @ D_KiT_wrt_X_nonjacobian


    D_S_wrt_X_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * ( (D_Qi_wrt_X_nonjacobian @ KiT_numpy) + (Q_i_Numpy @ D_KiT_wrt_X_nonjacobian) )
    #D_S_wrt_X = To_Jacobian(D_S_wrt_X_Nonjacobian)  # Numpy
    D_S_wrt_X = reshape_cupy_4d_to_3d(D_S_wrt_X_Nonjacobian)  # Cupy

    D_SoftmaxS_wrt_S = Derivative_SoftmaxS_wrt_S(P, S)# softmax derivative

    D_SoftmaxS_wrt_X = D_SoftmaxS_wrt_S @ D_S_wrt_X
    B, Rows, Cols = P.shape
    #D_SoftmaxS_wrt_X_nonjacobian = To_Nonjacobian(D_SoftmaxS_wrt_X, Rows, Cols)  # numpy

    D_SoftmaxS_wrt_X_nonjacobian = reshape_cupy_3d_to_4d(D_SoftmaxS_wrt_X, Rows, Cols)  # cupy
    #SoftmaxS_Numpy = P.get() # Numpy
    #V_i_Numpy = V_i.get() # Numpy

    SoftmaxS_Numpy = P
    SoftmaxS_Numpy = SoftmaxS_Numpy[:,None,:,:]
    V_i_Numpy = V_i
    V_i_Numpy = V_i_Numpy[:,None,:,:]
    #WiV_numpy = W_V_i.get()
    WiV_numpy = W_V_i
    D_Vi_wrt_X_Nonjacobian = D_X_wrt_X_nonjacobian @ WiV_numpy
    D_Attention_i_wrt_X_NonJacobianForm = (D_SoftmaxS_wrt_X_nonjacobian @ V_i_Numpy) + (SoftmaxS_Numpy @ D_Vi_wrt_X_Nonjacobian)

    return D_Attention_i_wrt_X_NonJacobianForm
def compute_D_SA_wrt_itsInput(Encoders,Encoder_Layer, D_XT_wrt_X, D_X_wrt_X_nonjacobian): # refer to page 23 (similarly)
    """
    Input:
    Encoder_Layer = layer number which SA layer resides in for which the derivative is being computed

    Output:

    Jacobian of the Derivative of MaskedMAO wrt its input X (shape ((sequence_length_O * model_dimension) by (sequence_length_O * model_dimension)) )
    Input:
    P = Softmax(S)
    S = ( (Qi @ KiT) / sqrt(dk) ) + mask
    W_V - value weight matrix for Masked Multihead attention layer
    W_Q - query weight matrix for Masked Multihead attention layer
    W_K - key weight matrix for Masked Multihead attention layer
    W_O - output weight matrix for Masked Multihead attention layer
    Q - Query matrix for Masked Multihead attention layer
    K - Key matrix for Masked Multihead attention layer
    V - Value matrix for Masked Multihead attention layer

    """
    """
    D_XT_wrt_X = np.zeros((  (model_dimension * sequence_length), (sequence_length * model_dimension)  ))  # refer to page 23 + 2
    RowJ = 0
    ColJ = 0
    for i in range(model_dimension):
        for j in range(sequence_length):
            ""
            for m in range(sequence_length):
                for k in range(model_dimension):
                    D_XTij_wrt_Xmk = None
                    if j == m and i == k:
                        D_XTij_wrt_Xmk = 1
                    else:
                        D_XTij_wrt_Xmk = 0
                    D_XT_wrt_X[RowJ , ColJ] = D_XTij_wrt_Xmk
                    ColJ += 1
                    if ColJ >= (sequence_length * model_dimension):
                        ColJ = 0
                        RowJ += 1
                        if RowJ >= (model_dimension * sequence_length):
                            break
                if RowJ >= (model_dimension * sequence_length):
                    break
            ""
            ###########

            ColJ = j * (model_dimension) + i
            D_XT_wrt_X[RowJ , ColJ] = 1
            RowJ += 1
            if RowJ >= (model_dimension * sequence_length):
                break
        if RowJ >= (model_dimension * sequence_length):
            break

    """
    #########



    ###########

    X = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Input"]
    W_Q = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_K = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"]
    W_V = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"]
    W_O = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    Q = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"QKV"][f"Query"]
    K = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"QKV"][f"Key"]
    V = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"QKV"][f"Value"]

    split_K = split(K) # split_K is a list of matrices split along the model dimension of K into dk length
    split_Q = split(Q) # similarly
    split_V = split(V) # similarly
    split_W_V = split(W_V) # similarly
    split_W_Q = split(W_Q) # similarly
    split_W_K = split(W_K) # similarly



    A = []     # A is a list of matrices
    for i in range(number_of_heads):
        W_Q_i = split_W_Q[i]
        W_K_i = split_W_K[i]
        W_V_i = split_W_V[i]
        Q_i = split_Q[i]
        K_i = split_K[i]
        V_i = split_V[i]
        A_Num = i+1
        P = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
        S = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]
        A.append(compute_D_Attention_i_wrt_X_NonJacobianForm_SA(P, S, W_Q_i, W_K_i, W_V_i, Q_i, K_i, V_i, X, D_XT_wrt_X, D_X_wrt_X_nonjacobian))




    #D_Concat_wrt_X_NonJacobianForm = concatenate(A) # Numpy
    D_Concat_wrt_X_NonJacobianForm = concatenate_cupy_matrices(A) # Cupy

    #W_O_Numpy = W_O.get() # Numpy
    W_O_Numpy = W_O
    D_SA_wrt_X_NonJacobianForm = D_Concat_wrt_X_NonJacobianForm @ W_O_Numpy
    #D_SA_wrt_X = To_Jacobian(D_SA_wrt_X_NonJacobianForm) # Numpy
    D_SA_wrt_X = reshape_cupy_4d_to_3d(D_SA_wrt_X_NonJacobianForm) # Cupy

    return D_SA_wrt_X

####################
def compute_D_Attention_i_wrt_X_NonJacobianForm(P, S, W_Q_i, W_K_i, W_V_i, Q_i, K_i, V_i, X, D_X_wrt_X_nonjacobian, D_XT_wrt_X):
    """
    Helper function for compute_D_MaskedMAO_wrt_itsInput()
    Input:
    W_Q_i - query weight matrix for the ith attention head
    W_K_i - key weight matrix for the ith attention head
    W_V_i - value weight matrix for the ith attention head
    Q_i - query matrix for the ith attention head
    K_i - key marix for the ith attention head
    V_i - value marix for the ith attention head
    P = Softmax(S)


    Output:
    NonJacobian of the Derivative of Attention i wrt its input X (shape ((sequence_length_O * model_dimension) by (sequence_length_O * model_dimension)) )
    """

    """
    K_i_T = K_i.T
    # Calculate D_Attention_i_wrt_X_NonJacobianForm
    D_Attention_i_wrt_X_NonJacobianForm =  cp.empty((sequence_length_O , dk), dtype=object)   #np.zeros((sequence_length_O, dk)

    for i in range(sequence_length_O):
        for j in range(dk):
            D_Aij_wrt_X = cp.zeros((1, sequence_length_O * model_dimension ))
            col_D_Aij_wrt_X = 0
            for m in range(sequence_length_O):
                for k in range(model_dimension):


                    sum = 0   # D_Aij_wrt_Xmk = sum
                    for z in range(sequence_length_O):
                        # calculate g_prime
                        g_prime = 0
                        for t1 in range(sequence_length_O):
                            sum_D_Sit1_wrt_Xmk = 0
                            for r in range(dk):
                                D_qir_wrt_Xmk = None
                                if i==m:
                                    D_qir_wrt_Xmk = W_Q_i[k,r]
                                else:
                                    D_qir_wrt_Xmk = 0
                                D_KiTrt1_wrt_Xmk = None
                                if m == t1:
                                    D_KiTrt1_wrt_Xmk = W_K_i[k,r]
                                else:
                                    D_KiTrt1_wrt_Xmk = 0
                                term_D_Sit1_wrt_Xmk = ( D_qir_wrt_Xmk * K_i_T[r,t1] ) + (Q_i[i,r] * D_KiTrt1_wrt_Xmk)
                                sum_D_Sit1_wrt_Xmk += term_D_Sit1_wrt_Xmk
                            D_Sit1_wrt_Xmk = (1/(np.sqrt(dk).astype(np.float16))) * (sum_D_Sit1_wrt_Xmk)
                            term_g_prime = np.exp(S[i,t1]) * D_Sit1_wrt_Xmk
                            g_prime += term_g_prime
                        # calculate f_prime
                        f_prime = 0
                        sum_D_Siz_wrt_Xmk = 0
                        for r in range(dk):
                            D_qir_wrt_Xmk = None
                            if i==m:
                                D_qir_wrt_Xmk = W_Q_i[k,r]
                            else:
                                D_qir_wrt_Xmk = 0
                            D_KiTrz_wrt_Xmk = None
                            if m == z:
                                D_KiTrz_wrt_Xmk = W_K_i[k,r]
                            else:
                                D_KiTrz_wrt_Xmk = 0
                            term_D_Siz_wrt_Xmk = ( D_qir_wrt_Xmk * K_i_T[r,z] ) + (Q_i[i,r] * D_KiTrz_wrt_Xmk)
                            sum_D_Siz_wrt_Xmk += term_D_Siz_wrt_Xmk
                        D_Siz_wrt_Xmk = (1/(np.sqrt(dk).astype(np.float16))) * (sum_D_Siz_wrt_Xmk)
                        f_prime = np.exp(S[i,z]) * D_Siz_wrt_Xmk

                        # f and g defined
                        f = cp.exp(S[i,z])
                        g = 0
                        for t1 in range(sequence_length_O):
                            g += cp.exp(S[i,t1])

                        # calculate D_Vzj_wrt_Xmk
                        D_Vzj_wrt_Xmk = None
                        if z == m:
                            D_Vzj_wrt_Xmk = W_V_i[k,j]
                        else:
                            D_Vzj_wrt_Xmk = 0

                        D_Piz_wrt_Xmk = ( ( (g)*(f_prime) )- ( (f)*(g_prime) ) ) / (g)**2
                        term = ( (D_Piz_wrt_Xmk) * (V_i[z,j]) ) + ( (P[i,z]) * (D_Vzj_wrt_Xmk) )
                        sum += term
                    D_Aij_wrt_Xmk = sum

                    D_Aij_wrt_X[0,col_D_Aij_wrt_X] =  D_Aij_wrt_Xmk
                    col_D_Aij_wrt_X += 1
                    if col_D_Aij_wrt_X >= (sequence_length_O * model_dimension):
                        col_D_Aij_wrt_X = 0
                        break

            D_Attention_i_wrt_X_NonJacobianForm[i, j] = D_Aij_wrt_X
    """
    """
    D_X_wrt_X = np.eye(sequence_length_O * model_dimension) # identity matrix
    #Rows, Cols = X.shape
    Rows = sequence_length_O
    Cols = model_dimension
    D_X_wrt_X_nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols)
    """


    #W_i_Q_Numpy = W_Q_i.get()
    W_i_Q_Numpy = W_Q_i
    D_Qi_wrt_X_nonjacobian = D_X_wrt_X_nonjacobian @ W_i_Q_Numpy

    """
    D_XT_wrt_X = np.zeros((  (model_dimension * sequence_length_O), (sequence_length_O * model_dimension)  ))
    RowJ = 0
    ColJ = 0
    for i in range(model_dimension):
        for j in range(sequence_length_O):
            ""
            for m in range(sequence_length_O):
                for k in range(model_dimension):
                    D_XTij_wrt_Xmk = None
                    if j == m and i == k:
                        D_XTij_wrt_Xmk = 1
                    else:
                        D_XTij_wrt_Xmk = 0
                    D_XT_wrt_X[RowJ , ColJ] = D_XTij_wrt_Xmk
                    ColJ += 1
                    if ColJ >= (sequence_length_O * model_dimension):
                        ColJ = 0
                        RowJ += 1
                        if RowJ >= (model_dimension * sequence_length_O):
                            break
                if RowJ >= (model_dimension * sequence_length_O):
                    break

            ""
            ColJ = j * (model_dimension) + i
            D_XT_wrt_X[RowJ , ColJ] = 1
            RowJ += 1
            if RowJ >= (model_dimension * sequence_length_O):
                break
        if RowJ >= (model_dimension * sequence_length_O):
            break
    """

    #XT = X.T
    XT = np.transpose(X, (0, 2, 1))

    B, Rows, Cols = XT.shape
    #D_XT_wrt_X_Nonjacobian = To_Nonjacobian(D_XT_wrt_X, Rows, Cols)  # Numpy

    D_XT_wrt_X_Nonjacobian = reshape_cupy_3d_to_4d(D_XT_wrt_X, Rows, Cols)  # Cupy
    W_K_iT = W_K_i.T
    #WiKT_Numpy = W_K_iT.get()
    WiKT_Numpy = W_K_iT
    D_KiT_wrt_X_nonjacobian = WiKT_Numpy @ D_XT_wrt_X_Nonjacobian

    #KiT = K_i.T
    KiT = np.transpose(K_i, (0, 2, 1))
    #KiT_numpy = KiT.get()

    #Q_i_Numpy = Q_i.get()
    KiT_numpy = KiT
    Q_i_Numpy = Q_i
    Q_i_Numpy = Q_i_Numpy[:,None,:,:]
    KiT_numpy = KiT_numpy[:,None,:,:]



    D_S_wrt_X_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * ( (D_Qi_wrt_X_nonjacobian @ KiT_numpy) + (Q_i_Numpy @ D_KiT_wrt_X_nonjacobian) )
    #D_S_wrt_X = To_Jacobian(D_S_wrt_X_Nonjacobian)  #  numpy
    D_S_wrt_X = reshape_cupy_4d_to_3d(D_S_wrt_X_Nonjacobian)  # cupy


    D_SoftmaxS_wrt_S = Derivative_SoftmaxS_wrt_S(P, S)# softmax derivative
    D_SoftmaxS_wrt_X = D_SoftmaxS_wrt_S @ D_S_wrt_X

    B, Rows, Cols = P.shape
    #D_SoftmaxS_wrt_X_nonjacobian = To_Nonjacobian(D_SoftmaxS_wrt_X, Rows, Cols)  # Numpy
    D_SoftmaxS_wrt_X_nonjacobian = reshape_cupy_3d_to_4d(D_SoftmaxS_wrt_X, Rows, Cols)  # Cupy
    #SoftmaxS_Numpy = P.get()
    #V_i_Numpy = V_i.get()

    SoftmaxS_Numpy = P
    SoftmaxS_Numpy = SoftmaxS_Numpy[:, None, :, :]
    V_i_Numpy = V_i
    V_i_Numpy = V_i_Numpy[:, None, :, :]

    #WiV_numpy = W_V_i.get()
    WiV_numpy = W_V_i


    D_Vi_wrt_X_Nonjacobian = D_X_wrt_X_nonjacobian @ WiV_numpy
    D_Attention_i_wrt_X_NonJacobianForm = (D_SoftmaxS_wrt_X_nonjacobian @ V_i_Numpy) + (SoftmaxS_Numpy @ D_Vi_wrt_X_Nonjacobian)

    return D_Attention_i_wrt_X_NonJacobianForm
def compute_D_MaskedMAO_wrt_itsInput(Decoders, Decoder_Layer, D_X_wrt_X_nonjacobian, D_XT_wrt_X): # refer to page 23
    """
    Input:
    Decodeer_Layer = layer number which MA layer resides in for which the derivative is being computed

    Output:

    Jacobian of the Derivative of MaskedMAO wrt its input X (shape ((sequence_length_O * model_dimension) by (sequence_length_O * model_dimension)) )
    Input:
    P = Softmax(S)
    S = ( (Qi @ KiT) / sqrt(dk) ) + mask
    W_V - value weight matrix for Masked Multihead attention layer
    W_Q - query weight matrix for Masked Multihead attention layer
    W_K - key weight matrix for Masked Multihead attention layer
    W_O - output weight matrix for Masked Multihead attention layer
    Q - Query matrix for Masked Multihead attention layer
    K - Key matrix for Masked Multihead attention layer
    V - Value matrix for Masked Multihead attention layer

    """
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Input"]
    W_Q = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_K = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"]
    W_V = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"]
    W_O = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    Q = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Query"]
    K = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Key"]
    V = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Value"]

    Jacobian_D_MaskedMAO_wrt_X = np.zeros(((sequence_length_O * model_dimension),(sequence_length_O * model_dimension)))
    rowJ = 0
    colJ = 0


    split_K = split(K) # split_K is a list of matrices split along the model dimension of K into dk length
    split_Q = split(Q) # similarly
    split_V = split(V) # similarly
    split_W_V = split(W_V) # similarly
    split_W_Q = split(W_Q) # similarly
    split_W_K = split(W_K) # similarly


    A = []     # A is a list of matrices
    for i in range(number_of_heads):
        W_Q_i = split_W_Q[i]
        W_K_i = split_W_K[i]
        W_V_i = split_W_V[i]
        Q_i = split_Q[i]
        K_i = split_K[i]
        V_i = split_V[i]
        A_Num = i+1
        P = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
        S = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]
        A.append(compute_D_Attention_i_wrt_X_NonJacobianForm(P, S, W_Q_i, W_K_i, W_V_i, Q_i, K_i, V_i, X, D_X_wrt_X_nonjacobian, D_XT_wrt_X))
    #D_Concat_wrt_X_NonJacobianForm = concatenate(A)  #  numpy
    D_Concat_wrt_X_NonJacobianForm = concatenate_cupy_matrices(A)  #  cupy
    #W_O_Numpy = W_O.get()
    W_O_Numpy = W_O
    D_MaskedMAO_wrt_X_NonJacobianForm = D_Concat_wrt_X_NonJacobianForm @ W_O_Numpy
    #D_MaskedMAO_wrt_X = To_Jacobian(D_MaskedMAO_wrt_X_NonJacobianForm)  # Numpy
    D_MaskedMAO_wrt_X = reshape_cupy_4d_to_3d(D_MaskedMAO_wrt_X_NonJacobianForm)  # Cupy
    return D_MaskedMAO_wrt_X



def D_EO_wrt_EI(Encoders, Encoder_Layer): # refer to page 44
    """
    i = Encoder_Layer in notes
    Output:
    Derivative of Encoder Output wrt Encoder Input in Jacobian form for the ith encoder layer
    """



    """
    D_SAi_wrt_Xi = compute_D_SA_wrt_itsInput(Encoders, Encoder_Layer) # SA derivative wrt its input


    O = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Output"]
    X = Encoders[f"Encoder{Encoder_Layer}"][f"Self_Attention"][f"Input"]
    D_LNiSA_wrt_SAi = compute_layernorm_derivative(O, X) # layer norm derivative

    """
    D_SAi_wrt_Xi = Encoders[f"Encoder{Encoder_Layer}"][f"D_OSAi_wrt_Xi"]
    D_LNiSA_wrt_SAi = Encoders[f"Encoder{Encoder_Layer}"][f"D_LNSAi_wrt_OSAi"]

    D_LNiSA_wrt_Xi = D_LNiSA_wrt_SAi @ D_SAi_wrt_Xi

    """
    Decoder = False
    W_FF1 =  Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
    D_FFi_wrt_LNiSA = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input) # feed forward derivative
    """
    D_FFi_wrt_LNiSA = Encoders[f"Encoder{Encoder_Layer}"][f"D_FFi_wrt_LNSAi"]

    D_FFi_wrt_Xi = D_FFi_wrt_LNiSA @ D_LNiSA_wrt_Xi

    """
    O = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Output"]
    X = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Input"]
    D_LNiFF_wrt_FFi = compute_layernorm_derivative(O, X) # layer norm derivative
    """
    D_LNiFF_wrt_FFi = Encoders[f"Encoder{Encoder_Layer}"][f"D_LNFFi_wrt_FFi"]

    D_EO_wrt_EI = D_LNiFF_wrt_FFi @ D_FFi_wrt_Xi



    return D_EO_wrt_EI

def D_DO_wrt_DI(Decoders, Decoder_Layer):   # refer to page 44
    """

    Output:
    Derivative of Decoder Output wrt Decoder Input in Jacobian form
    """

    """
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Input"]
    D_LNMAi_wrt_OMAi = compute_layernorm_derivative(O, X) # layernorm derivative

    """
    """
    D_OMAi_wrt_Xi = compute_D_MaskedMAO_wrt_itsInput(Decoders, Decoder_Layer) # Derivative of output of MA wrt its input
    """




    D_OMAi_wrt_Xi = Decoders[f"Decoder{Decoder_Layer}"][f"D_OMAi_wrt_Xi"]
    D_LNMAi_wrt_OMAi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNMAi_wrt_OMAi"]

    D_LNMAi_wrt_Xi = D_LNMAi_wrt_OMAi @ D_OMAi_wrt_Xi

    D_OCAi_wrt_LNMAi = Decoders[f"Decoder{Decoder_Layer}"][f"D_OCAi_wrt_LNMAi"]
    """
    D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Decoder_Layer) # Derivative of OC wrt input from masked attention
    """
    D_OCAi_wrt_Xi = D_OCAi_wrt_LNMAi @ D_LNMAi_wrt_Xi
    """
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    D_LNCAi_wrt_OCAi = compute_layernorm_derivative(O, X) # layernorm derivative
    """
    D_LNCAi_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNCAi_wrt_OCAi"]

    D_LNCAi_wrt_Xi = D_LNCAi_wrt_OCAi @ D_OCAi_wrt_Xi

    """
    Decoder = True
    W_FF1 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
    D_FFi_wrt_LNCAi = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input) # feed forward derivative
    """
    D_FFi_wrt_LNCAi = Decoders[f"Decoder{Decoder_Layer}"][f"D_FFi_wrt_LNCAi"]


    D_FFi_wrt_Xi = D_FFi_wrt_LNCAi @ D_LNCAi_wrt_Xi
    """
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Input"]
    D_LNFFi_wrt_FFi = compute_layernorm_derivative(O, X) # layernorm derivative
    """
    D_LNFFi_wrt_FFi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNFFi_wrt_FFi"]
    D_DO_wrt_DI = D_LNFFi_wrt_FFi @ D_FFi_wrt_Xi
    return D_DO_wrt_DI


def Compute_D_X_wrt_previousOC(Decoders, Decoder_Layer):
    """
    Note: Decoder Layer >= 2 (should be)
    Input:
    Decoder Layer - Decoder Layer Number which X or LNMA after previous mask resides in

    Computes Compute_D_X_wrt_previousOC (Derivative of X wrt previous cross attention output) for d(Output of Cross Attention)/dE and returns it in jacobian form

    """
    shape = (sequence_length_O * model_dimension)
    D_LN_after_previous_Mask_wrt_LN_after_previous_Mask = np.identity(shape)
    D_X_wrt_LN_after_previous_Mask = D_LN_after_previous_Mask_wrt_LN_after_previous_Mask   # this will be an identity matrix
    """
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Input"]
    D_LN_after_previous_Mask_wrt_previous_Mask = compute_layernorm_derivative(O, X)
    """
    D_LN_after_previous_Mask_wrt_previous_Mask = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNMAi_wrt_OMAi"]

    D_X_wrt_previous_Mask = D_X_wrt_LN_after_previous_Mask @ D_LN_after_previous_Mask_wrt_previous_Mask
    """
    D_previous_Mask_wrt_LN_after_previous_Decoder_FF = compute_D_MaskedMAO_wrt_itsInput(Decoders, Decoder_Layer)
    """
    D_previous_Mask_wrt_LN_after_previous_Decoder_FF = Decoders[f"Decoder{Decoder_Layer}"][f"D_OMAi_wrt_Xi"]
    D_X_wrt_LN_after_previous_Decoder_FF = D_X_wrt_previous_Mask @ D_previous_Mask_wrt_LN_after_previous_Decoder_FF
    """
    O = Decoders[f"Decoder{Decoder_Layer-1}"][f"Feed_Forward"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer-1}"][f"Feed_Forward"][f"Input"]
    D_LN_after_previous_Decoder_FF_wrt_previous_Decoder_FF = compute_layernorm_derivative(O, X)
    """
    D_LN_after_previous_Decoder_FF_wrt_previous_Decoder_FF = Decoders[f"Decoder{Decoder_Layer-1}"][f"D_LNFFi_wrt_FFi"]

    D_X_wrt_previous_Decoder_FF = D_X_wrt_LN_after_previous_Decoder_FF @ D_LN_after_previous_Decoder_FF_wrt_previous_Decoder_FF

    """
    Decoder = True
    W_FF1 = Decoders[f"Decoder{Decoder_Layer-1}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Decoders[f"Decoder{Decoder_Layer-1}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Decoders[f"Decoder{Decoder_Layer-1}"][f"Feed_Forward"][f"ReLu_Input"]
    D_previous_Decoder_FF_wrt_LayerNorm_After_Previous_OC = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input)
    """
    D_previous_Decoder_FF_wrt_LayerNorm_After_Previous_OC = Decoders[f"Decoder{Decoder_Layer-1}"][f"D_FFi_wrt_LNCAi"]

    D_X_wrt_LayerNorm_After_Previous_OC = D_X_wrt_previous_Decoder_FF @ D_previous_Decoder_FF_wrt_LayerNorm_After_Previous_OC
    """
    O = Decoders[f"Decoder{Decoder_Layer-1}"][f"Cross_Attention"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer-1}"][f"Cross_Attention"][f"Input_Decoder"]
    D_LayerNorm_After_Previous_OC_wrt_previousOC = compute_layernorm_derivative(O, X)
    """
    D_LayerNorm_After_Previous_OC_wrt_previousOC = Decoders[f"Decoder{Decoder_Layer-1}"][f"D_LNCAi_wrt_OCAi"]

    D_X_wrt_previousOC = D_X_wrt_LayerNorm_After_Previous_OC @ D_LayerNorm_After_Previous_OC_wrt_previousOC

    return D_X_wrt_previousOC
def D_DOF_wrt_DOi(m, F, Decoders):   # refer to page 44
    """
    Input:
    m - number of decoder layers in front of ith decoder layer
    Output:
    Derivative of Decoder Output of Final decoder layer wrt Decoder Output of the ith decoder layer in Jacobian form
    note: if i = F where F is final decoder layer number then it is just an identity matrix
    """
    shape = (sequence_length_O * model_dimension)
    D_DO_wrt_DI_Prev = np.identity(shape)
    Current_Decoder_Layer = F
    while m > 0:
        D_DO_wrt_DI_Current =  D_DO_wrt_DI(Decoders, Current_Decoder_Layer)
        D_DO_wrt_DI_Prev = D_DO_wrt_DI_Prev @ D_DO_wrt_DI_Current
        m -= 1
        Current_Decoder_Layer -= 1
    D_DOF_wrt_DOi = D_DO_wrt_DI_Prev
    return D_DOF_wrt_DOi

def D_EOF_wrt_EOi(m, F, Encoders):
    """
    Input:
    m - number of encoder layers in front of ith encoder layer
    F = Number of Encoder Layers
    Output:
    Derivative of encoder Output of Final encoder layer wrt encoder Output of the ith encoder layer in Jacobian form
    note: if i = F where F is final decoder layer number then it is just an identity matrix
    """
    shape = (sequence_length * model_dimension)
    D_EO_wrt_EI_Prev = np.identity(shape)
    Current_Encoder_Layer = F
    while m > 0:
        D_EO_wrt_EI_Current =  D_EO_wrt_EI(Encoders, Current_Encoder_Layer)
        D_EO_wrt_EI_Prev = D_EO_wrt_EI_Prev @ D_EO_wrt_EI_Current
        m -= 1
        Current_Encoder_Layer -= 1
    D_EOF_wrt_EOi = D_EO_wrt_EI_Prev
    return D_EOF_wrt_EOi
def D_attention_i_wrt_E_NonJacobianForm(P, K_i, V_i, Q_i, W_Q_i, W_K_i, W_V_i, Layer_Num, A_Num, Decoders, D_previousOC_wrt_E, D_X_wrt_previousOC, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian):
    """
    Input:
    P = Softmax(S)
    K_i - key marix for the ith attention head
    V_i - value marix for the ith attention head
    Q_i - query matrix for the ith attention head
    W_Q_i - query weight matrix for the ith attention head
    W_K_i - key weight matrix for the ith attention head
    W_V_i - value weight matrix for the ith attention head
    A_Num - Attenton head number
    Layer_Num - Layer number for Decoder which the attention heads resides in

    Output:
    D_attention_i_wrt_E_NonJacobianForm
    """


    Softmax_S = P


    # calculate D_KiT_wrt_E_NonJacobian
    W_K_iT = W_K_i.T
    #W_K_iT_Numpy = W_K_iT.get()
    W_K_iT_Numpy = W_K_iT

    D_KiT_wrt_E_NonJacobian = W_K_iT_Numpy @ D_ET_wrt_E_nonjacobian
    # calculate D_S_wrt_E_NonJacobianForm
    D_S_wrt_E_NonJacobianForm = None
    Decoder_layer = Layer_Num
    if Decoder_layer == 1:
        #Q_i_Numpy = Q_i.get()

        Q_i_Numpy = Q_i
        Q_i_Numpy = Q_i_Numpy[:, None, :, :]
        D_S_wrt_E_NonJacobianForm = (1/(np.sqrt(dk).astype(np.float16))) *    ( (Q_i_Numpy) @ (D_KiT_wrt_E_NonJacobian) )  # if Cross Attention is in first decoder layer

    else:

        Decoder_layer -= 1
        """
        D_previousOC_wrt_E = derivative_cross_attention_output_wrt_encoder_output( Decoder_layer, Decoders)
        D_X_wrt_previousOC = Compute_D_X_wrt_previousOC(Decoders, Layer_Num)
        """



        D_X_wrt_E = D_X_wrt_previousOC @ D_previousOC_wrt_E

        #D_X_wrt_E_NonJacobian = To_Nonjacobian(D_X_wrt_E, sequence_length_O, model_dimension) # Numpy
        D_X_wrt_E_NonJacobian = reshape_cupy_3d_to_4d(D_X_wrt_E, sequence_length_O, model_dimension) # Cupy
        #W_Q_i_Numpy = W_Q_i.get()
        W_Q_i_Numpy = W_Q_i
        D_Qi_wrt_E_NonJacobian = D_X_wrt_E_NonJacobian @ W_Q_i_Numpy
        #K_iT = K_i.T
        K_iT = np.transpose(K_i, (0, 2, 1))
        #K_iT_Numpy = K_iT.get()
        #Q_i_Numpy = Q_i.get()

        K_iT_Numpy = K_iT
        Q_i_Numpy = Q_i
        Q_i_Numpy = Q_i_Numpy[:, None, :, :]
        K_iT_Numpy = K_iT_Numpy[:, None, :, :]

        D_S_wrt_E_NonJacobianForm = (1/(np.sqrt(dk).astype(np.float16))) * ( ( (D_Qi_wrt_E_NonJacobian) @ (K_iT_Numpy) ) + ( (Q_i_Numpy) @ (D_KiT_wrt_E_NonJacobian) ) ) # if Cross attention is not first decoder layer

    # calculate D_Vi_wrt_E_NonJacobianForm           refer to page 18


    D_Vi_wrt_E_NonJacobianForm = None
    """
    D_E_wrt_E = cp.empty((sequence_length, model_dimension), dtype=object)
    for i in range(sequence_length):
        for j in range(model_dimension):
            D_eij_wrt_E = cp.zeros((1, sequence_length * model_dimension))
            ""
            col_D_eij_wrt_E = 0
            for m in range(sequence_length):
                for k in range(model_dimension):
                    D_eij_wrt_emk = None
                    if i == m and j == k:
                        D_eij_wrt_emk = 1
                    else:
                        D_eij_wrt_emk = 0
                    D_eij_wrt_E[0,col_D_eij_wrt_E] = D_eij_wrt_emk
                    col_D_eij_wrt_E += 1
                    if col_D_eij_wrt_E >= (sequence_length * model_dimension):
                        col_D_eij_wrt_E = 0
                        break
            ""
            Col = i * model_dimension + j
            D_eij_wrt_E[0,Col] = 1

            D_E_wrt_E[i,j] = D_eij_wrt_E

    """

    #W_V_i_Numpy = W_V_i.get()
    W_V_i_Numpy = W_V_i



    D_Vi_wrt_E_NonJacobianForm = D_E_wrt_E_Nonjacobian @ W_V_i_Numpy



    # Calculate D_Attentioni_wrt_E_NonJacobianForm
    #D_S_wrt_E = To_Jacobian(D_S_wrt_E_NonJacobianForm) #  numpy
    D_S_wrt_E = reshape_cupy_4d_to_3d(D_S_wrt_E_NonJacobianForm) #  cupy






    P = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    S =  Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]




    # Record start time
    start_time4 = time.time()
    #dummy_S = np.random.rand(2, 2)  # Small input
    #dummy_R = np.zeros_like(dummy_S)
    #Derivative_SoftmaxS_wrt_S(dummy_S, dummy_R)  # Warm-up call

    D_SoftmaxS_wrt_S = Derivative_SoftmaxS_wrt_S(P, S)

    # Record end time
    end_time4 = time.time()

    # Calculate and #print the elapsed time
    elapsed_time = end_time4 - start_time4
    #print(f"Elapsed time for D_SoftmaxS_wrt_S with dummy for A_Num {A_Num}: {elapsed_time} seconds")

    start_time3 = time.time()
    D_SoftmaxS_wrt_E = D_SoftmaxS_wrt_S @ D_S_wrt_E
    #D_SoftmaxS_wrt_E_NonJacobianForm = To_Nonjacobian(D_SoftmaxS_wrt_E, sequence_length_O, sequence_length) # Numpy
    D_SoftmaxS_wrt_E_NonJacobianForm = reshape_cupy_3d_to_4d(D_SoftmaxS_wrt_E, sequence_length_O, sequence_length) # Cupy
    #V_i_Numpy = V_i.get()
    #Softmax_S_Numpy = Softmax_S.get()

    V_i_Numpy = V_i
    Softmax_S_Numpy = Softmax_S
    V_i_Numpy = V_i_Numpy[:, None, :, :]
    Softmax_S_Numpy = Softmax_S_Numpy[:, None, :, :]

    D_Attentioni_wrt_E_NonJacobianForm = (D_SoftmaxS_wrt_E_NonJacobianForm @ V_i_Numpy) + (Softmax_S_Numpy @ D_Vi_wrt_E_NonJacobianForm)

    end_time3 = time.time()

    # Calculate and #print the elapsed time
    elapsed_time = end_time3 - start_time3
    ##print(f"Elapsed time (FINAL): {elapsed_time} seconds")
    return D_Attentioni_wrt_E_NonJacobianForm



def derivative_cross_attention_output_wrt_encoder_output(Layer_Num, Decoders, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian): # refer to page 17
    """

    P = Softmax(S) - shape sequence_length_O by sequence_length
    S - S = (Qi @ KiT) / sqrt(dk)
    Layer_Num is decoder layer
    K - key marix
    V - value marix
    Q - query matrix
    W_Q - query weight matrix
    W_K - key weight matrix
    W_V - value weight matrix
    W_O - output weight matrix

    Decoder_layer - current decoder layer
    Outputs:
    d(Output of Cross Attention)/dE  in jacobian form i.e D_OC_wrt_E
    """
    start_time8 = time.time()  # Record start time

    Decoder_layer = Layer_Num
    D_previousOC_wrt_E = None
    D_X_wrt_previousOC = None
    if Decoder_layer == 1:
        D_previousOC_wrt_E = None
        D_X_wrt_previousOC = None
    else:

        Decoder_layer -= 1

        D_previousOC_wrt_E = derivative_cross_attention_output_wrt_encoder_output( Decoder_layer, Decoders, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian)
        D_X_wrt_previousOC = Compute_D_X_wrt_previousOC(Decoders, Layer_Num)



    end_time8 = time.time()  # Record end time
    elapsed_time = end_time8 - start_time8  # Calculate elapsed time
    #print(f"Elapsed time for D_attention_i_wrt_E_NonJacobianForm0 for Decoder Layer {Layer_Num}: {elapsed_time:.6f} seconds")


    K = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"QKV"][f"Key"]
    V = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"QKV"][f"Value"]
    Q = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"QKV"][f"Query"]
    W_Q = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_K = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Key_Weight_Matrix"]
    W_V = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Value_Weight_Matrix"]
    W_O = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]

    split_K = split(K)
    split_Q = split(Q)
    split_V = split(V)
    split_W_Q = split(W_Q)
    split_W_K = split(W_K)
    split_W_V = split(W_V)
    start_time = time.time()  # Record start time
    A = []
    for i in range(number_of_heads):
        K_i = split_K[i]
        V_i = split_V[i]
        Q_i = split_Q[i]
        W_Q_i = split_W_Q[i]
        W_K_i = split_W_K[i]
        W_V_i = split_W_V[i]

        A_Num = i+1
        P = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]



        A.append(D_attention_i_wrt_E_NonJacobianForm(P, K_i, V_i, Q_i, W_Q_i, W_K_i, W_V_i, Layer_Num, A_Num, Decoders,  D_previousOC_wrt_E, D_X_wrt_previousOC, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian))


        ##print(f"For Decoder Layer = ", Layer_Num)
    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_attention_i_wrt_E_NonJacobianForm for Decoder Layer {Layer_Num}: {elapsed_time:.6f} seconds")

    start_time7 = time.time()  # Record start time
    #D_Concat_wrt_E_NonJacobianForm = concatenate(A) # Numpy
    D_Concat_wrt_E_NonJacobianForm = concatenate_cupy_matrices(A)
    #W_O_Numpy = W_O.get()
    W_O_Numpy = W_O
    D_OC_wrt_E_NonJacobianForm = D_Concat_wrt_E_NonJacobianForm @ W_O_Numpy
    #D_OC_wrt_E = To_Jacobian(D_OC_wrt_E_NonJacobianForm ) # Numpy
    D_OC_wrt_E = reshape_cupy_4d_to_3d(D_OC_wrt_E_NonJacobianForm)


    end_time7 = time.time()  # Record end time
    elapsed_time = end_time7 - start_time7  # Calculate elapsed time
    #print(f"Elapsed time for D_attention_i_wrt_E_NonJacobianForm2 for Decoder Layer {Layer_Num}: {elapsed_time:.6f} seconds")
    return D_OC_wrt_E


def Compute_D_Attentioni_wrt_X_for_OC_nonjacobian(K_i, V_i, W_Q_i, Decoders, Layer_Num, A_Num, D_X_wrt_X_nonjacobian):  # refer to page 25
    """
    helper function for  Compute_D_OC_wrt_X()
    Input:
    K_i - key marix for the ith attention head
    V_i - value marix for the ith attention head
    W_Q_i - query weight matrix for the ith attention head

    Outputs:
    D_Attentioni_wrt_X_for_OC_nonjacobian
    """
    """
    D_X_wrt_X_nonjacobian = cp.empty((sequence_length_O, model_dimension), dtype=object)
    for i in range(sequence_length_O):
        for j in range(model_dimension):
            D_Xij_wrt_X = cp.zeros((1, sequence_length_O * model_dimension))
            col_D_Xij_wrt_X = 0
            for m in range(sequence_length_O):
                for k in range(model_dimension):
                    D_Xij_wrt_Xmk = None
                    if i == m and j == k:
                        D_Xij_wrt_Xmk = 1
                    else:
                        D_Xij_wrt_Xmk = 0
                    D_Xij_wrt_X[0, col_D_Xij_wrt_X] = D_Xij_wrt_Xmk
                    col_D_Xij_wrt_X += 1
                    if col_D_Xij_wrt_X >= (sequence_length_O * model_dimension):
                        col_D_Xij_wrt_X = 0
                        break
            D_X_wrt_X_nonjacobian[i, j] = D_Xij_wrt_X
    """
    #W_Q_i_Numpy = W_Q_i.get()
    W_Q_i_Numpy = W_Q_i
    D_Qi_wrt_X_NonJacobianForm = D_X_wrt_X_nonjacobian @ W_Q_i_Numpy
    #K_iT = K_i.T
    K_iT = np.transpose(K_i, (0,2,1))
    #K_iT_Numpy = K_iT.get()
    K_iT_Numpy = K_iT
    K_iT_Numpy = K_iT_Numpy[:, None, :, :]
    D_S_wrt_X_NonJacobianForm = D_Qi_wrt_X_NonJacobianForm @ K_iT_Numpy
    #D_S_wrt_X = To_Jacobian(D_S_wrt_X_NonJacobianForm)  # Numpy
    D_S_wrt_X = reshape_cupy_4d_to_3d(D_S_wrt_X_NonJacobianForm)
    P = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    S =  Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]

    D_SoftmaxS_wrt_S = Derivative_SoftmaxS_wrt_S(P, S)
    D_SoftmaxS_wrt_X = D_SoftmaxS_wrt_S @ D_S_wrt_X
    #D_SoftmaxS_wrt_X_NonJacobianForm = To_Nonjacobian(D_SoftmaxS_wrt_X, sequence_length_O, sequence_length) #Numpy
    D_SoftmaxS_wrt_X_NonJacobianForm = reshape_cupy_3d_to_4d(D_SoftmaxS_wrt_X, sequence_length_O, sequence_length)
    #V_i_Numpy = V_i.get()
    V_i_Numpy = V_i
    V_i_Numpy = V_i_Numpy[:, None, :, :]
    D_Attentioni_wrt_X_nonjacobian = D_SoftmaxS_wrt_X_NonJacobianForm @ V_i_Numpy
    return D_Attentioni_wrt_X_nonjacobian

def Compute_D_OC_wrt_X(Decoders, Layer_Num, D_X_wrt_X_nonjacobian):
    """
    Computes the derivative of the cross attention output wrt input X used to create the query matrix (i.e from output sequence)
    Layer_Num - Decoder Layer
    K
    V
    W_Q
    W_O

    """
    K = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"QKV"][f"Key"]
    V = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"QKV"][f"Value"]

    W_Q = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"]
    W_O = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]

    split_K = split(K) # split_K is a list of matrices split along the model dimension of K into dk length
    split_V = split(V) # similarly
    split_W_Q = split(W_Q) # similarly
    A = []   # list of matrices where each matrix is the derivative of Attention i wrt X

    for i in range(number_of_heads):
        K_i = split_K[i]
        V_i = split_V[i]
        W_Q_i = split_W_Q[i]
        A_Num = i+1
        A.append(Compute_D_Attentioni_wrt_X_for_OC_nonjacobian(K_i, V_i, W_Q_i, Decoders, Layer_Num, A_Num, D_X_wrt_X_nonjacobian))


    #D_Concat_wrt_X_NonJacobianForm = concatenate(A) # Cupy
    D_Concat_wrt_X_NonJacobianForm = concatenate_cupy_matrices(A)
    #W_O_Numpy = W_O.get()
    W_O_Numpy = W_O
    D_OC_wrt_X_NonJacobianForm = D_Concat_wrt_X_NonJacobianForm @ W_O_Numpy
    #D_OC_wrt_X = To_Jacobian(D_OC_wrt_X_NonJacobianForm)  # Numpy
    D_OC_wrt_X = reshape_cupy_4d_to_3d(D_OC_wrt_X_NonJacobianForm)

    return D_OC_wrt_X
def D_Attention_i_wrt_WKC_NonJacobian(X, V_i, K_i, A_num, Decoders, Layer_Num):  # Refer to page 27
    """
    Helper function for D_OC_wrt_WQC()
    input:
    V_i - value weight matrix for the ith attention head
    K_i - key weight matrix for the ith attention head
    X - input matrix to compute for Query matrix for Cross attention
    A_num - Attention head number
    Layer_Num - decoder layer number

    Outputs:
    Output:
    output:
    Nonjacobian of D_Attention_i_wrt_WKC
    """



  ###
    """
    # Numpy Implementation
    D_WiKT_wrt_WKC_Nonjacobian = cp.empty((dk, model_dimension), dtype=object)
    for i in range(dk):
        for j in range(model_dimension):
            D_WiKTj_wrt_WKC = cp.zeros((1,(model_dimension * model_dimension) ))
            D_WiKTj_wrt_WKC_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiKTj_wrt_WKCmk = None

                    if k ==  (( dk * (A_Num - 1) ) +  i )  and j == m:
                        D_WiKTj_wrt_WKCmk = 1
                    else:
                        D_WiKTj_wrt_WKCmk = 0
                    D_WiKTj_wrt_WKC[0, D_WiKTj_wrt_WKC_Col] = D_WiKTj_wrt_WKCmk
                    D_WiKTj_wrt_WKC_Col += 1
                    if D_WiKTj_wrt_WKC_Col >= (model_dimension * model_dimension):
                        D_WiKTj_wrt_WKC_Col = 0
                        break
                if D_WiKTj_wrt_WKC_Col >= (model_dimension * model_dimension):
                    D_WiKTj_wrt_WKC_Col = 0
                    break
            D_WiKT_wrt_WKC_Nonjacobian[i,j] = D_WiKTj_wrt_WKC

    """
    """
    # Cupy implementation

    D_WiKT_wrt_WKC_Nonjacobian = np.zeros((model_dimension * model_dimension, dk, model_dimension), dtype=cp.float32)

    # Compute indices for assignment
    i_indices = np.arange(dk).reshape(dk, 1)
    j_indices = np.arange(model_dimension).reshape(1, model_dimension)
    Col_indices = j_indices * model_dimension + (dk * (A_Num - 1) + i_indices)

    # Use advanced indexing to assign 1s
    D_WiKT_wrt_WKC_Nonjacobian[Col_indices, i_indices, j_indices] = 1
    """
    A_Num = A_num + 1
    D_WiKT_wrt_WKC_Nonjacobian = Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WKTi_wrt_WK_Nonjacobian"]


    E = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Input_Encoder"]
    split_Q = split(Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"QKV"][f"Query"])
    split_Index = A_num - 1
    Q_i = split_Q[split_Index] #split
    #ET = E.T
    ET = np.transpose(E, (0,2,1))
    #ET_Numpy = ET.get()  # Numpy
    ET_Numpy = ET # Cupy
    ET_Numpy = ET_Numpy[:, None, :, :]
    D_KiT_wrt_WKC_Nonjacobian = D_WiKT_wrt_WKC_Nonjacobian @ ET_Numpy
    #Q_i_Numpy = Q_i.get() #  Numpy
    Q_i_Numpy = Q_i  # Cupy
    Q_i_Numpy = Q_i_Numpy[:, None, :, :]
    D_S_wrt_WKC_nonjacobian = ( 1/np.sqrt(dk).astype(np.float16)) * (Q_i_Numpy @ D_KiT_wrt_WKC_Nonjacobian)

    #D_S_wrt_WKC = To_Jacobian(D_S_wrt_WKC_nonjacobian)  # Numpy
    D_S_wrt_WKC = reshape_cupy_4d_to_3d(D_S_wrt_WKC_nonjacobian)


    P = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    S =  Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]

    D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, S) # softmax derivative funtion wrt to its input
    D_Softmax_S_wrt_WKC = D_Softmax_S_wrt_S @ D_S_wrt_WKC
    Softmax_S = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    B, Rows, Cols = Softmax_S.shape

    #D_Softmax_S_wrt_WKC_nonjacobian = To_Nonjacobian(D_Softmax_S_wrt_WKC, Rows, Cols)  # numpy
    D_Softmax_S_wrt_WKC_nonjacobian = reshape_cupy_3d_to_4d(D_Softmax_S_wrt_WKC, Rows, Cols)

    #V_i_Numpy = V_i.get()  # Numpy
    V_i_Numpy = V_i
    V_i_Numpy = V_i_Numpy[:, None, :, :]
    D_Attention_i_wrt_WKC_NonJacobian = D_Softmax_S_wrt_WKC_nonjacobian @ V_i_Numpy
    return D_Attention_i_wrt_WKC_NonJacobian

def D_OC_wrt_WKC(X, V, K, Decoders, Layer_Num):  #refer to page 26
    """
    X - input matrix to compute query weight matrix
    V - value weight matrix
    K - key weight matrix
    helper function for D_Lavg_wrt_WKCi()
    output: jacobian of Derivative of OC wrt Key weight matrix for cross attention


    """
    split_V = split(V)
    split_K = split(K)

    A = []
    for i in range(number_of_heads):
        V_i = split_V[i]
        K_i = split_K[i]
        A_num = i
        A.append(D_Attention_i_wrt_WKC_NonJacobian(X, V_i, K_i, A_num, Decoders, Layer_Num))
    #D_concat_wrt_WKC = concatenate(A) # numpy
    D_concat_wrt_WKC = concatenate_cupy_matrices(A)


    W_O = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    #W_O_Numpy = W_O.get() #numpy
    W_O_Numpy = W_O
    D_OC_wrt_WKC_nonjacobian = D_concat_wrt_WKC @ W_O_Numpy
    #D_OC_wrt_WKC = To_Jacobian(D_OC_wrt_WKC_nonjacobian)  # Numpy
    D_OC_wrt_WKC = reshape_cupy_4d_to_3d(D_OC_wrt_WKC_nonjacobian)
    return D_OC_wrt_WKC
def D_Attention_i_wrt_WVC_NonJacobian(X, V_i, K_i, A_num, Decoders, Layer_Num):
    """
    """


    """
    # Numpy Implementation
    D_WiV_wrt_WVC_Nonjacobian = cp.empty((model_dimension, dk), dtype=object)
    for i in range(model_dimension):
        for j in range(dk):
            D_WiVij_wrt_WVC = cp.zeros((1, model_dimension * model_dimension))
            D_WiVij_wrt_WVC_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiVij_wrt_WVCmk = None
                    if i == m and k == ( dk * (A_Num - 1)  +  j  ):
                        D_WiVij_wrt_WVCmk = 1
                    else:
                        D_WiVij_wrt_WVCmk = 0
                    D_WiVij_wrt_WVC[0, D_WiVij_wrt_WVC_Col] = D_WiVij_wrt_WVCmk
                    D_WiVij_wrt_WVC_Col += 1
                    if D_WiVij_wrt_WVC_Col >= (model_dimension * model_dimension):
                        D_WiVij_wrt_WVC_Col = 0
                        break

                if D_WiVij_wrt_WVC_Col >= (model_dimension * model_dimension):
                    D_WiVij_wrt_WVC_Col = 0
                    break


            D_WiV_wrt_WVC_Nonjacobian[i,j] = D_WiVij_wrt_WVC
    """
    ##################
    """
    # Cupy implementation
    A_Num = A_num + 1
    # Create an empty array with the correct shape
    D_WiV_wrt_WVC_Nonjacobian = np.zeros((model_dimension * model_dimension, model_dimension, dk))

    # Compute indices efficiently
    rows = np.arange(model_dimension)[:, None]  # Shape (model_dimension, 1)
    cols = dk * (A_Num - 1) + np.arange(dk)    # Shape (dk,)

    # Compute the flattened row indices
    final_rows = rows * model_dimension + cols  # Shape (model_dimension, dk)

    # Assign 1s in a vectorized manner
    D_WiV_wrt_WVC_Nonjacobian[final_rows, np.arange(model_dimension)[:, None], np.arange(dk)] = 1

    #####################
    """
    A_Num = A_num + 1
    D_WiV_wrt_WVC_Nonjacobian = Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WVi_wrt_WV_Nonjacobian"]
    #############

    E = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Input_Encoder"]
    #E_Numpy = E.get() # Numpy
    E_Numpy = E
    E_Numpy = E_Numpy[:, None, :, :]


    D_Vi_wrt_WVC_Nonjacobian = E_Numpy @ D_WiV_wrt_WVC_Nonjacobian
    Softmax_S = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
    #Softmax_S_Numpy = Softmax_S.get()
    Softmax_S_Numpy = Softmax_S
    Softmax_S_Numpy = Softmax_S_Numpy[:, None, :, :]

    D_Attention_i_wrt_WVC_NonJacobian = Softmax_S_Numpy @ D_Vi_wrt_WVC_Nonjacobian
    return D_Attention_i_wrt_WVC_NonJacobian
def D_Lavg_wrt_WOCi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layer):   # refer to page 26 and 45
    """
    Output:
    Jacobian of Derivative of Loss average wrt output weight matrix of cross attention of the ith decoder layer

    """

    """
    # Numpy
    D_WOC_wrt_WOC_Nonjacobian = cp.empty((model_dimension, model_dimension), dtype=object)
    for i in range(model_dimension):
        for j in range(model_dimension):
            D_WOCij_wrt_WOC = cp.zeros((1, model_dimension * model_dimension))
            col_D_WOCij_wrt_WOC = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WOCij_wrt_WOCmk = None
                    if i == m and j == k:
                        D_WOCij_wrt_WOCmk = 1
                    else:
                        D_WOCij_wrt_WOCmk = 0
                    D_WOCij_wrt_WOC[0,col_D_WOCij_wrt_WOC] = D_WOCij_wrt_WOCmk
                    col_D_WOCij_wrt_WOC += 1
                    if col_D_WOCij_wrt_WOC >= (model_dimension * model_dimension):
                        col_D_WOCij_wrt_WOC = 0
                        break
                if col_D_WOCij_wrt_WOC >= (model_dimension * model_dimension):
                    col_D_WOCij_wrt_WOC = 0
                    break
            D_WOC_wrt_WOC_Nonjacobian[i,j] = D_WOCij_wrt_WOC
    """

    D_WOC_wrt_WOC_Nonjacobian = Decoders[f"D_WO_wrt_WO_Nonjacobian"]
    Concat = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Concat"]



    #Concat_Numpy = Concat.get()
    Concat_Numpy = Concat
    Concat_Numpy = Concat_Numpy[:, None, :, :]


    D_OC_wrt_WOC_Nonjacobian = Concat_Numpy @ D_WOC_wrt_WOC_Nonjacobian
    #D_OCAi_wrt_WOCi_ = To_Jacobian(D_OC_wrt_WOC_Nonjacobian)  # Numpy
    D_OCAi_wrt_WOCi_ = reshape_cupy_4d_to_3d(D_OC_wrt_WOC_Nonjacobian) # Cupy
    """
    D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
    F = Number_Of_Decoder_Layers
    i = Decoder_Layer
    m = F - i  # m is number of decoders layers in front of decoder layer i
    D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
    D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_


    D_DOi_wrt_OCAi_ = D_DOi_wrt_OCAi(Decoders, Decoder_Layer)

    D_DOi_wrt_WOCi = D_DOi_wrt_OCAi_ @ D_OCAi_wrt_WOCi_
    """
    D_Lavg_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"D_Lavg_wrt_OCAi"]

    D_Lavg_wrt_WOCi = D_Lavg_wrt_OCAi @ D_OCAi_wrt_WOCi_


    return D_Lavg_wrt_WOCi

def D_OC_wrt_WVC(X, V, K, Decoders, Layer_Num):
    """
    """
    split_V = split(V)
    split_K = split(K)

    A = []
    for i in range(number_of_heads):
        V_i = split_V[i]
        K_i = split_K[i]
        A_num = i
        A.append(D_Attention_i_wrt_WVC_NonJacobian(X, V_i, K_i, A_num, Decoders, Layer_Num))

    #D_concat_wrt_WVC = concatenate(A) #Numpy
    D_concat_wrt_WVC = concatenate_cupy_matrices(A)

    W_O = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    #W_O_Numpy = W_O.get()  #Cupy
    W_O_Numpy = W_O

    D_OC_wrt_WVC_nonjacobian = D_concat_wrt_WVC @ W_O_Numpy
    #D_OC_wrt_WVC = To_Jacobian(D_OC_wrt_WVC_nonjacobian) # Numpy
    D_OC_wrt_WVC = reshape_cupy_4d_to_3d(D_OC_wrt_WVC_nonjacobian)
    return D_OC_wrt_WVC
def D_Lavg_wrt_WVCi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers): # refer to page 38 and 45
    """
    output:
    jacobian of Derivative of loss wrt value weight matrix for cross attention
    """



    ######
    """
    D_Lavg_wrt_WQCi = None

    D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
    F = Number_Of_Decoder_Layers
    i = Decoder_Layer
    m = F - i  # m is number of decoders layers in front of decoder layer i
    D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
    D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_

    ######

    D_DOi_wrt_OCAi_ = D_DOi_wrt_OCAi(Decoders, Decoder_Layer)
    """

    X = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    V = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"QKV"][f"Value"]
    K = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"QKV"][f"Key"]


    D_OCAi_wrt_WVCi = D_OC_wrt_WVC(X, V, K, Decoders, Decoder_Layer)


    D_Lavg_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"D_Lavg_wrt_OCAi"]

    D_Lavg_wrt_WVCi = D_Lavg_wrt_OCAi @ D_OCAi_wrt_WVCi


    return D_Lavg_wrt_WVCi


def D_Lavg_wrt_WKCi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers): # refer to page 37 and 45
    """
    Decoder_Layer - for which WKCi resides in
    output:
    jacobian of Derivative of loss wrt Key weight matrix for cross attention
    """
    """
    D_Lavg_wrt_WQCi = None

    D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
    F = Number_Of_Decoder_Layers
    i = Decoder_Layer
    m = F - i  # m is number of decoders layers in front of decoder layer i
    D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
    D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_
    D_DOi_wrt_OCAi_ = D_DOi_wrt_OCAi(Decoders, Decoder_Layer)
    """
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    V = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"QKV"][f"Value"]
    K = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"QKV"][f"Key"]
    Layer_Num = Decoder_Layer
    D_OCAi_wrt_WKCi = D_OC_wrt_WKC(X, V, K, Decoders, Layer_Num)
    D_Lavg_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"D_Lavg_wrt_OCAi"]

    D_Lavg_wrt_WKCi = D_Lavg_wrt_OCAi @ D_OCAi_wrt_WKCi


    return D_Lavg_wrt_WKCi


def D_Attention_i_wrt_WQC_NonJacobian(X, V_i, K_i, A_num, Decoders, Layer_Num):  # Refer to page 27
    """
    Helper function for D_OC_wrt_WQC()
    input:
    V_i - value weight matrix for the ith attention head
    K_i - key weight matrix for the ith attention head
    X - input matrix to compute for Query matrix for Cross attention
    A_num - Attention head number

    Outputs:
    Output:
    output:
    Nonjacobian of D_Attention_i_wrt_WQC
    """
    #p rint("WQC = ",  Decoders[f"Decoder{Layer_Num}"]["Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"])
    #p rint("A_Num = ", A_num)
    #p rint("WiQ = ", Decoders[f"Decoder{Layer_Num}"]["Cross_Attention"]["Attention_Heads"][f"Attention_Head{A_num}"][f""])

    """
    # Numpy Implementation
    D_WQi_wrt_WQC_NonJacobian = cp.empty((model_dimension, dk), dtype=object)
    for i in range(model_dimension):
        for j in range(dk):
            D_WQiij_wrt_WQC = cp.zeros((1, model_dimension * model_dimension))
            D_WQiij_wrt_WQCmk = None

            # initialize D_WQiij_wrt_WQC
            col_D_WQiij_wrt_WQC = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    if i == m and k == ( ( dk * (A_num ) ) + ( j ) ):
                        D_WQiij_wrt_WQCmk = 1
                    else:
                        D_WQiij_wrt_WQCmk = 0
                    D_WQiij_wrt_WQC[0, col_D_WQiij_wrt_WQC] = D_WQiij_wrt_WQCmk
                    #p rint("i , j , m , k = ", i, j, m, k)
                    #p rint("D_WQiij_wrt_WQCmk = ", D_WQiij_wrt_WQCmk)
                    col_D_WQiij_wrt_WQC += 1
                    if col_D_WQiij_wrt_WQC >= (model_dimension * model_dimension):
                        col_D_WQiij_wrt_WQC = 0
                        break
                if col_D_WQiij_wrt_WQC >= (model_dimension * model_dimension):
                    col_D_WQiij_wrt_WQC = 0
                    break

            #p rint("D_WQiij_wrt_WQC = ", D_WQiij_wrt_WQC)



            D_WQi_wrt_WQC_NonJacobian[i,j] = D_WQiij_wrt_WQC
    """

    """
    # Cupy implementation
    A_Num = A_num + 1
    # Create an empty array with the correct shape
    D_WQi_wrt_WQC_NonJacobian = np.zeros((model_dimension * model_dimension, model_dimension, dk))

    # Compute indices efficiently
    rows = np.arange(model_dimension)[:, None]  # Shape (model_dimension, 1)
    cols = dk * (A_Num - 1) + np.arange(dk)    # Shape (dk,)

    # Compute the flattened row indices
    final_rows = rows * model_dimension + cols  # Shape (model_dimension, dk)

    # Assign 1s in a vectorized manner
    D_WQi_wrt_WQC_NonJacobian[final_rows, np.arange(model_dimension)[:, None], np.arange(dk)] = 1
    """
    A_Num = A_num + 1
    D_WQi_wrt_WQC_NonJacobian = Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WQi_wrt_WQ_Nonjacobian"]
    ###########################

    ##print("D_WQi_wrt_WQC_NonJacobian = ", D_WQi_wrt_WQC_NonJacobian)
    #X_Numpy = X.get() # Numpy
    X_Numpy = X
    X_Numpy = X_Numpy[:, None, :, :]


    D_Qi_wrt_WQC_nonjacobian = X_Numpy @ D_WQi_wrt_WQC_NonJacobian
    #p rint("D_Qi_wrt_WQC_nonjacobian ", D_Qi_wrt_WQC_nonjacobian)
    #KiT = K_i.T
    KiT = np.transpose(K_i, (0,2,1))
    #KiT_Numpy = KiT.get()  # Numpy
    KiT_Numpy = KiT
    KiT_Numpy = KiT_Numpy[:, None, :, :]


    ##print("K_i = ", K_i)
    D_S_wrt_WQC_nonjacobian = ( 1/np.sqrt(dk).astype(np.float16)) * (D_Qi_wrt_WQC_nonjacobian @ KiT_Numpy)
    #p rint("D_Qi_wrt_WQC_nonjacobian = ", D_Qi_wrt_WQC_nonjacobian)
    #p rint("dk = ", dk)
    #p rint(" K_i.T = ", K_i.T)
    #p rint("D_S_wrt_WQC_nonjacobian = ", D_S_wrt_WQC_nonjacobian)
    #p rint("D_S_wrt_WQC_nonjacobian shape = ", D_S_wrt_WQC_nonjacobian.shape)

    #D_S_wrt_WQC = To_Jacobian(D_S_wrt_WQC_nonjacobian)  # Numpy
    D_S_wrt_WQC = reshape_cupy_4d_to_3d(D_S_wrt_WQC_nonjacobian)

    #p rint("D_S_wrt_WQC  = ", D_S_wrt_WQC )
    A_Num = A_num + 1
    P = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    S =  Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]

    D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, S) # softmax derivative funtion wrt to its input
    D_Softmax_S_wrt_WQC = D_Softmax_S_wrt_S @ D_S_wrt_WQC

    #D_Softmax_S_wrt_WQC_nonjacobian = To_Nonjacobian(D_Softmax_S_wrt_WQC, sequence_length_O, sequence_length)  # Numpy
    D_Softmax_S_wrt_WQC_nonjacobian = reshape_cupy_3d_to_4d(D_Softmax_S_wrt_WQC, sequence_length_O, sequence_length)

    #V_i_Numpy = V_i.get() # Numpy
    V_i_Numpy = V_i # Cupy
    V_i_Numpy = V_i_Numpy[:, None, :, :]
    D_Attention_i_wrt_WQC_NonJacobian = D_Softmax_S_wrt_WQC_nonjacobian @ V_i_Numpy
    return D_Attention_i_wrt_WQC_NonJacobian


def D_OC_wrt_WQC(X, V, K, Decoders, Layer_Num):  #refer to page 26
    """
    X - input matrix to compute query weight matrix
    V - value weight matrix
    K - key weight matrix
    helper function for D_Loss_wrt_W_Q_C()

    """
    split_V = split(V)
    split_K = split(K)

    A = []
    for i in range(number_of_heads):
        V_i = split_V[i]
        K_i = split_K[i]
        A_num = i
        A.append(D_Attention_i_wrt_WQC_NonJacobian(X, V_i, K_i, A_num, Decoders, Layer_Num))

    #D_concat_wrt_WQC = concatenate(A)  # Numpy
    D_concat_wrt_WQC = concatenate_cupy_matrices(A)

    W_O = Decoders[f"Decoder{Layer_Num}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    #W_O_Numpy = W_O.get()  # Numpy
    W_O_Numpy = W_O # Cupy
    D_OC_wrt_WQC_nonjacobian = D_concat_wrt_WQC @ W_O_Numpy
    #D_OC_wrt_WQC = To_Jacobian(D_OC_wrt_WQC_nonjacobian)
    D_OC_wrt_WQC = reshape_cupy_4d_to_3d(D_OC_wrt_WQC_nonjacobian)
    return D_OC_wrt_WQC
def D_DOi_wrt_OCAi(Decoders, Decoder_Layer): # refer to page 44
    """
    Input:
    Decoders - Decoders dict
    Decoder_Layer - decoder layer number for which derivative is being computed
    Output:
    Derivative of ith Decoder layer Output wrt ith Decoder layer Query weight matrix for cross attention in jacobian form
    """
    """
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Input"]
    D_DOi_wrt_FFi = compute_layernorm_derivative(O, X)  #LNFFi is the layernorm FF output for the ith decoder where i is the decoder layer for the WQC derivative that is being computed for

    Decoder = True
    W_FF1 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
    D_FFi_wrt_LNCAi =  compute_derivative_FF( Decoder, W_FF1, W_FF2, ReLu_Input) # FF derivative wrt to its input
    """


    D_DOi_wrt_FFi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNFFi_wrt_FFi"]
    D_FFi_wrt_LNCAi = Decoders[f"Decoder{Decoder_Layer}"][f"D_FFi_wrt_LNCAi"]
    D_DOi_wrt_LNCAi = D_DOi_wrt_FFi @ D_FFi_wrt_LNCAi

    """
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    D_LNCAi_wrt_OCAi = compute_layernorm_derivative(O, X) #layernorm derivative wrt its input
    """
    D_LNCAi_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNCAi_wrt_OCAi"]
    D_DOi_wrt_OCAi = D_DOi_wrt_LNCAi @ D_LNCAi_wrt_OCAi

    return D_DOi_wrt_OCAi

def D_Lavg_wrt_DOF(Final_Layers): # refer to page 44
    """
    Input:

    Output:
    Derivative of loss average wrt final decoder output in jacobian form
    """
    X = Final_Layers[f"Softmax"][f"Output"]
    Y = Final_Layers[f"True_Labels"]

    D_Lavg_wrt_Softmax =  D_Lavg_wrt_X(X, Y) # Derivative of Lavg wrt final softmax
    #p rint("D_Lavg_wrt_Softmax = ", D_Lavg_wrt_Softmax)
    P = Final_Layers[f"Softmax"][f"Output"] # P = Softmax(S)
    S = Final_Layers[f"Linear"][f"Output"]
    D_Softmax_wrt_logits = Derivative_SoftmaxS_wrt_S(P, S) # derivative of softmax wrt its input
    #p rint("D_Softmax_wrt_logits = ", D_Softmax_wrt_logits)

    D_Lavg_wrt_logits = D_Lavg_wrt_Softmax @ D_Softmax_wrt_logits
    W_L = Final_Layers[f"Linear"][f"Weights"]
    D_logits_wrt_DOF = compute_derivative_Linear(W_L) # derivative of liner layer wrt its input
    #p rint("D_logits_wrt_LNFF = ", D_logits_wrt_LNFF)

    D_Lavg_wrt_DOF = D_Lavg_wrt_logits @ D_logits_wrt_DOF  # LNFF is the layernorm FF output of the last decoder layer

    return D_Lavg_wrt_DOF
def D_EOi_wrt_SOi(Encoders, Encoder_Num):
    """
    Encoder_Num - Encoder layer which you are computing derivative of EOi wrt SOi for

    Output:
    calculate D_EOi_wrt_SOi (derivative of encoder output wrt self attention output for the ith encoder layer )
    """
    """
    O = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Output"]
    X = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Input"]
    D_E_wrt_FFE = compute_layernorm_derivative(O, X) #
    """
    D_E_wrt_FFE = Encoders[f"Encoder{Encoder_Num}"][f"D_LNFFi_wrt_FFi"]
    """
    Decoder = False
    W_FF1 = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"ReLu_Input"]
    D_FFE_wrt_LNS = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input) # Feed Forward deriative
    """
    D_FFE_wrt_LNS = Encoders[f"Encoder{Encoder_Num}"][f"D_FFi_wrt_LNSAi"]
    """
    O = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Output"]
    X = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Input"]
    D_LNS_wrt_SO = compute_layernorm_derivative(O, X) #Layernorm derivative
    """
    D_LNS_wrt_SO = Encoders[f"Encoder{Encoder_Num}"][f"D_LNSAi_wrt_OSAi"]

    D_E_wrt_LNS = D_E_wrt_FFE @ D_FFE_wrt_LNS
    D_EOi_wrt_SOi = D_E_wrt_LNS @ D_LNS_wrt_SO
    return D_EOi_wrt_SOi


def cross_entropy_jacobian(Z, Y_logits, T, dYdZ):
    """
    Computes the Jacobian of the average cross-entropy loss w.r.t. encoder output Z using CuPy.

    Parameters:
        Z (cp.ndarray): Encoder output of shape (batch_size, seq_len, hidden_dim)
        Y_logits (cp.ndarray): Decoder logits of shape (batch_size, seq_len, vocab_size)
        T (cp.ndarray): Ground truth token indices of shape (batch_size, seq_len)
        dYdZ (cp.ndarray): Local Jacobian dY/dZ of shape (batch_size, seq_len, vocab_size, hidden_dim)

    Returns:
        cp.ndarray: Full Jacobian matrix of shape (batch_size*seq_len*hidden_dim, batch_size*seq_len*hidden_dim)
    """
    batch_size, seq_len, hidden_dim = Z.shape
    vocab_size = Y_logits.shape[-1]

    # Compute softmax
    Y_softmax = np.exp(Y_logits - np.max(Y_logits, axis=-1, keepdims=True))
    Y_softmax /= np.sum(Y_softmax, axis=-1, keepdims=True)

    # Convert T to one-hot representation
    T_one_hot = np.zeros_like(Y_logits)
    T_one_hot[np.arange(batch_size)[:, None], np.arange(seq_len), T] = 1

    # Compute dL/dY (gradient of cross-entropy loss w.r.t. logits)
    dLdY = (Y_softmax - T_one_hot) / (batch_size * seq_len)

    # Compute dL/dZ using the chain rule (batch-wise multiplication)
    dLdZ = np.einsum('bsv,bsvh->bsh', dLdY, dYdZ)

    # Reshape dLdZ into a Jacobian matrix
    J = np.zeros((batch_size * seq_len * hidden_dim, batch_size * seq_len * hidden_dim))

    for b in range(batch_size):
        for s in range(seq_len):
            idx = b * seq_len * hidden_dim + s * hidden_dim
            J[idx:idx+hidden_dim, idx:idx+hidden_dim] = np.diag(dLdZ[b, s])

    return J

def D_Lavg_wrt_E(Number_Of_Decoder_Layers, Decoders, Final_Layers, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian):
    """

    Output:
    Jacobian of Derivative of loss average wrt final encoder layer output
    """



    # Calculate D_Lavg_wrt_EOi for each encoder layer

    Final_Decoder_Layer = Number_Of_Decoder_Layers
    X = Final_Layers[f"Softmax"][f"Output"]
    Y = Final_Layers[f"True_Labels"]


    D_Lavg_wrt_Softmax =  D_Lavg_wrt_X(X, Y) # Derivative of Lavg wrt final softmax

    #P = Final_Layers[f"Softmax"][f"Output"] # P = Softmax(S)
    #S = Final_Layers[f"Linear"][f"Output"]

    #D_Softmax_wrt_logits = Derivative_SoftmaxS_wrt_S(P, S) # derivative of softmax wrt its input
    D_Softmax_wrt_logits = Final_Layers[f"D_Softmax_wrt_logits"]



    D_Lavg_wrt_logits = D_Lavg_wrt_Softmax @ D_Softmax_wrt_logits
    W_L = Final_Layers[f"Linear"][f"Weights"]
    D_logits_wrt_LNFF = compute_derivative_Linear(W_L) # derivative of liner layer wrt its input



    D_Lavg_wrt_LNFF = D_Lavg_wrt_logits @ D_logits_wrt_LNFF  # LNFF is the layernorm FF output of the last decoder layer
    """
    O = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Feed_Forward"][f"Output"]
    X = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Feed_Forward"][f"Input"]
    D_LNFF_wrt_FF = compute_layernorm_derivative(O, X)  #LNFFi is the layernorm FF output for the ith decoder where i is the decoder layer for the WQC derivative that is being computed for
    """
    D_LNFF_wrt_FF = Decoders[f"Decoder{Final_Decoder_Layer}"][f"D_LNFFi_wrt_FFi"]
    D_Lavg_wrt_FF = D_Lavg_wrt_LNFF @ D_LNFF_wrt_FF

    """
    Decoder = True
    W_FF1 = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
    D_FF_wrt_LNCA =  compute_derivative_FF( Decoder, W_FF1, W_FF2, ReLu_Input) # FF derivative wrt to its input
    """
    D_FF_wrt_LNCA = Decoders[f"Decoder{Final_Decoder_Layer}"][f"D_FFi_wrt_LNCAi"]

    D_Lavg_wrt_LNCA = D_Lavg_wrt_FF @ D_FF_wrt_LNCA

    """
    O = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Cross_Attention"][f"Output"]
    X = Decoders[f"Decoder{Final_Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    D_LNCA_wrt_OC = compute_layernorm_derivative(O, X) #layernorm derivative wrt its input
    """
    D_LNCA_wrt_OC = Decoders[f"Decoder{Final_Decoder_Layer}"][f"D_LNCAi_wrt_OCAi"]
    D_Lavg_wrt_OCAF = D_Lavg_wrt_LNCA @ D_LNCA_wrt_OC  # D_Lavg_wrt_OCAF  derivative of loss average wrt cross attention output of final decoder layer
    """
    #Calculate D_ET_wrt_E_nonjacobian
    D_ET_wrt_E_nonjacobian = cp.empty((model_dimension, sequence_length), dtype=object)

    for i in range(model_dimension):
        for j in range(sequence_length):
            D_eTij_wrt_E = cp.zeros((1, model_dimension * sequence_length))
            ""
            col_D_eTij_wrt_E = 0
            for m in range(model_dimension):
                for k in range(sequence_length):
                    D_eTij_wrt_emk = None
                    if i == k and j == m:
                        D_eTij_wrt_emk = 1
                    else:
                        D_eTij_wrt_emk = 0
                    D_eTij_wrt_E[0,col_D_eTij_wrt_E] = D_eTij_wrt_emk
                    col_D_eTij_wrt_E += 1
                    if col_D_eTij_wrt_E >= (model_dimension * sequence_length):
                        col_D_eTij_wrt_E = 0
                        break
            ""
            Col = j * model_dimension + i
            D_eTij_wrt_E[0, Col] = 1
            D_ET_wrt_E_nonjacobian[i,j] = D_eTij_wrt_E
    """

    start_time = time.time()  # Record start time

    D_OCAF_wrt_E = derivative_cross_attention_output_wrt_encoder_output(Final_Decoder_Layer, Decoders, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian)

    end_time = time.time()  # Record end time
    elapsed_time = end_time - start_time  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_EOF_ INSIDE: {elapsed_time:.6f} seconds")

    D_Lavg_wrt_E = D_Lavg_wrt_OCAF @ D_OCAF_wrt_E
    return D_Lavg_wrt_E


def D_Lavg_wrt_SOi(Decoders, Encoders, Final_Layers , Number_Of_Decoder_Layers, Number_Of_Encoder_Layers, Encoder_Layer, D_Lavg_wrt_EOF_):
    """
    Input:
    Encoder_Layer - ith encoder layer which SOi corresponds
    """
    #D_Lavg_wrt_EOF_ = D_Lavg_wrt_E(Number_Of_Decoder_Layers, Decoders, Final_Layers)
    """
    F = Number_Of_Encoder_Layers
    m = F - Encoder_Layer
    D_EOF_wrt_EOi_ = D_EOF_wrt_EOi(m, F, Encoders)
    """

    D_EOi_wrt_SOi_ = D_EOi_wrt_SOi(Encoders, Encoder_Layer)



    D_Lavg_wrt_EOi = Encoders[f"Encoder{Encoder_Layer}"][f"D_Lavg_wrt_EOi"]
    D_Lavg_wrt_SOi = D_Lavg_wrt_EOi @ D_EOi_wrt_SOi_
    return D_Lavg_wrt_SOi

def D_Lavg_wrt_OC(Decoders, Decoder_Layer, Final_Layers):  # refer to page 26
    """
    Decoders - dictionary containing the input outputs to all the layers in encoder
    Encoders - dictionary containing the input outputs to all the layers in encoder
    Final_Layers - dictionary containing the final layer input and outputs, softmax input and output, and true labels

    compute Derivative of loss average wrt Output of cross attention in jacobian form
    Decoder_Layer - current decoder layer wh ich OC for D_Lavg_wrt_OC is in
    helper function for:
    D_Loss_wrt_W_Q_C()
    output - jacobian form
    """
    X = Final_Layers[f"Softmax"][f"Output"]
    Y = Final_Layers[f"True_Labels"]

    D_Lavg_wrt_Softmax =  D_Lavg_wrt_X(X, Y) # Derivative of Lavg wrt final softmax
    #p rint("D_Lavg_wrt_Softmax = ", D_Lavg_wrt_Softmax)
    P = Final_Layers[f"Softmax"][f"Output"] # P = Softmax(S)
    S = Final_Layers[f"Linear"][f"Output"]
    D_Softmax_wrt_logits = Derivative_SoftmaxS_wrt_S(P, S) # derivative of softmax wrt its input
    #p rint("D_Softmax_wrt_logits = ", D_Softmax_wrt_logits)

    D_Lavg_wrt_logits = D_Lavg_wrt_Softmax @ D_Softmax_wrt_logits
    W_L = Final_Layers[f"Linear"][f"Weights"]
    D_logits_wrt_LNFF = compute_derivative_Linear(W_L) # derivative of liner layer wrt its input
    #p rint("D_logits_wrt_LNFF = ", D_logits_wrt_LNFF)

    D_Lavg_wrt_LNFF = D_Lavg_wrt_logits @ D_logits_wrt_LNFF  # LNFF is the layernorm FF output of the last decoder layer
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Input"]


    D_LNFF_wrt_FF = compute_layernorm_derivative(O, X)  #LNFFi is the layernorm FF output for the ith decoder where i is the decoder layer for the WQC derivative that is being computed for

    D_Lavg_wrt_FF = D_Lavg_wrt_LNFF @ D_LNFF_wrt_FF
    Decoder = True
    W_FF1 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
    #p rint("X = ", Decoders[f"Decoder1"][f"Layer_Norm_CA"][f"Output"])
    D_FF_wrt_LNCA =  compute_derivative_FF( Decoder, W_FF1, W_FF2, ReLu_Input) # FF derivative wrt to its input
    #p rint("D_FF_wrt_LNCA = ", D_FF_wrt_LNCA)
    D_Lavg_wrt_LNCA = D_Lavg_wrt_FF @ D_FF_wrt_LNCA
    O = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Output"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    D_LNCA_wrt_OC = compute_layernorm_derivative(O, X) #layernorm derivative wrt its input
    #p rint("D_LNCA_wrt_OC = ", D_LNCA_wrt_OC)
    D_Lavg_wrt_OC = D_Lavg_wrt_LNCA @ D_LNCA_wrt_OC
    return D_Lavg_wrt_OC


def D_Loss_wrt_W_Q_C_i(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers): # refer to page 26 and 45
    """
    Input:
    Decoder_Layer - decoder layer which deriviative for WQC is being computed for
    Number_Of_Decoder_Layers = F
    Output:
    compute Derivative of loss wrt Query weight matrix for cross attention for the ith decoder layer and return in jacobian form
    """
    """
    D_Lavg_wrt_WQCi = None


    D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
    F = Number_Of_Decoder_Layers
    i = Decoder_Layer
    m = F - i  # m is number of decoders layers in front of decoder layer i
    D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
    D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_
    D_DOi_wrt_OCAi_ = D_DOi_wrt_OCAi(Decoders, Decoder_Layer)
    """
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"Input_Decoder"]
    V = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"QKV"][f"Value"]
    K = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"QKV"][f"Key"]
    Layer_Num = Decoder_Layer


    D_OCAi_wrt_WQCi = D_OC_wrt_WQC(X, V, K, Decoders, Layer_Num)


    D_Lavg_wrt_OCAi = Decoders[f"Decoder{Decoder_Layer}"][f"Cross_Attention"][f"D_Lavg_wrt_OCAi"]

    D_Lavg_wrt_WQCi = D_Lavg_wrt_OCAi @ D_OCAi_wrt_WQCi


    return D_Lavg_wrt_WQCi

def D_Lavg_wrt_WOMi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers):  # refer to page 35
    """
    """
    """
    #Calculate D_MMA_wrt_WQM
    D_MMA_wrt_WVM = None
    A = []
    for i in range(1, number_of_heads + 1):
        A_Num = i
        D_Attention_i_wrt_WVM_ = D_Attention_i_wrt_WVM(Decoders, A_Num, Layer_Num)
        A.
        append(D_Attention_i_wrt_WVM_)
    D_Concat_wrt_WVM = concatenate(A)
    W_OM = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    ####
    """
    # Calculate D_Lavg_wrt_OMAi
    """
    Layer_Num = Decoder_Layer
    D_Lavg_wrt_OCAi_ = D_Lavg_wrt_OCAi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers)
    D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Layer_Num) # OC derivative wrt X (decoder input)
    O = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Output"]
    X = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Input"]
    D_LNMAi_wrt_OMAi =  compute_layernorm_derivative(O, X) #layernorm derivative
    D_OCAi_wrt_OMAi = D_OCAi_wrt_LNMAi @ D_LNMAi_wrt_OMAi
    D_Lavg_wrt_OMAi = D_Lavg_wrt_OCAi_ @ D_OCAi_wrt_OMAi
    """
    D_Lavg_wrt_OMAi = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"D_Lavg_wrt_OMAi"]
    ##########

    # Calculate D_OMAi_wrt_WOMi
    """
    D_WOM_wrt_WOM_Nonjacobian = cp.empty((model_dimension, model_dimension), dtype=object)
    for i in range(model_dimension):
        for j in range(model_dimension):
            D_WOMij_wrt_WOM = cp.zeros((1, model_dimension * model_dimension))
            col_D_WOMij_wrt_WOM = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WOMij_wrt_WOMmk = None
                    if i == m and j == k:
                        D_WOMij_wrt_WOMmk = 1
                    else:
                        D_WOMij_wrt_WOMmk = 0
                    D_WOMij_wrt_WOM[0,col_D_WOMij_wrt_WOM] = D_WOMij_wrt_WOMmk
                    col_D_WOMij_wrt_WOM += 1
                    if col_D_WOMij_wrt_WOM >= (model_dimension * model_dimension):
                        col_D_WOMij_wrt_WOM = 0
                        break
            D_WOM_wrt_WOM_Nonjacobian[i,j] = D_WOMij_wrt_WOM

    """
    D_WOM_wrt_WOM_Nonjacobian = Decoders[f"D_WO_wrt_WO_Nonjacobian"]
    Concat = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Concat"]
    #Concat_Numpy = Concat.get()  # numpy
    Concat_Numpy = Concat
    Concat_Numpy = Concat_Numpy[:, None, :, :]
    D_MMA_wrt_WOM_Nonjacobian = Concat_Numpy @ D_WOM_wrt_WOM_Nonjacobian


    #D_OMAi_wrt_WOMi = To_Jacobian(D_MMA_wrt_WOM_Nonjacobian)  #  numpy
    D_OMAi_wrt_WOMi = reshape_cupy_4d_to_3d(D_MMA_wrt_WOM_Nonjacobian)


    D_Lavg_wrt_WOMi_ = D_Lavg_wrt_OMAi @ D_OMAi_wrt_WOMi

    return D_Lavg_wrt_WOMi_



def D_Attention_i_wrt_WVM(Decoders, A_Num, Layer_Num):
    """
    Output:
    Nonjacobian of D_Attention_i_wrt_WVM
    """
    """
    D_WiV_wrt_WVM_Nonjacobian = cp.empty((model_dimension, dk), dtype=object)
    for i in range(model_dimension):
        for j in range(dk):
            D_WiVij_wrt_WVM = cp.zeros((1, (model_dimension * model_dimension)))
            col_D_WiVij_wrt_WVM = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiVij_wrt_WVMmk = None
                    if i == m and k == ( dk*(A_Num - 1) + j ):
                        D_WiVij_wrt_WVMmk = 1
                    else:
                        D_WiVij_wrt_WVMmk = 0
                    D_WiVij_wrt_WVM[0,col_D_WiVij_wrt_WVM] = D_WiVij_wrt_WVMmk
                    col_D_WiVij_wrt_WVM += 1
                    if col_D_WiVij_wrt_WVM >= (model_dimension * model_dimension):
                        col_D_WiVij_wrt_WVM = 0
                        break
            D_WiV_wrt_WVM_Nonjacobian[i,j] = D_WiVij_wrt_WVM

    """
    D_WiV_wrt_WVM_Nonjacobian = Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WVi_wrt_WV_Nonjacobian"]
    Decoder_Layer = Layer_Num
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Input"]
    #X_Numpy = X.get()  # Numpy
    X_Numpy = X
    X_Numpy = X_Numpy[:, None, :, :]
    D_Vi_wrt_WVM_Nonjacobian = X_Numpy @ D_WiV_wrt_WVM_Nonjacobian
    Softmax_S = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
    #Softmax_S_Numpy = Softmax_S.get()  # numpy
    Softmax_S_Numpy = Softmax_S
    Softmax_S_Numpy = Softmax_S_Numpy[:, None, :, :]
    D_Attention_i_wrt_WVM_Nonjacobian = Softmax_S_Numpy @ D_Vi_wrt_WVM_Nonjacobian
    return D_Attention_i_wrt_WVM_Nonjacobian

def D_Lavg_wrt_WVMi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers):  # refer to page 35 and 45
    """

    output:
    D_Lavg_wrt_WVMi jacobian
    """
    """
    # Calculate D_Lavg_wrt_OMAi
    Layer_Num = Decoder_Layer
    D_Lavg_wrt_OCAi_ = D_Lavg_wrt_OCAi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers)
    D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Layer_Num) # OC derivative wrt X (decoder input)
    O = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Output"]
    X = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Input"]
    D_LNMAi_wrt_OMAi =  compute_layernorm_derivative(O, X) #layernorm derivative
    D_OCAi_wrt_OMAi = D_OCAi_wrt_LNMAi @ D_LNMAi_wrt_OMAi
    D_Lavg_wrt_OMAi = D_Lavg_wrt_OCAi_ @ D_OCAi_wrt_OMAi
    """
    D_Lavg_wrt_OMAi = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"D_Lavg_wrt_OMAi"]

    #Calculate D_MMA_wrt_WQM
    D_OMAi_wrt_WVMi = None
    A = []
    for i in range(1, number_of_heads + 1):
        A_Num = i
        D_Attention_i_wrt_WVM_ = D_Attention_i_wrt_WVM(Decoders, A_Num, Decoder_Layer)
        A.append(D_Attention_i_wrt_WVM_)
    #D_Concat_wrt_WVM = concatenate(A)  # numpy
    D_Concat_wrt_WVM = concatenate_cupy_matrices(A)# Cupy
    W_OM = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    #W_OM_Numpy = W_OM.get()  # numpy
    W_OM_Numpy = W_OM
    D_OMAi_wrt_WVMi_Nonjacobian = D_Concat_wrt_WVM @ W_OM_Numpy


    #D_OMAi_wrt_WVMi = To_Jacobian(D_OMAi_wrt_WVMi_Nonjacobian)  # Numpy
    D_OMAi_wrt_WVMi = reshape_cupy_4d_to_3d(D_OMAi_wrt_WVMi_Nonjacobian)



    D_Lavg_wrt_WVMi = D_Lavg_wrt_OMAi @ D_OMAi_wrt_WVMi  # D_OC_wrt_WQM = Alpha




    return D_Lavg_wrt_WVMi

def D_Attention_i_wrt_WKM(Decoders, A_Num, Layer_Num): # refer to page 34
    """
    Output:
    Nonjacobian of D_Attention_i_wrt_WKM
    """
    Split_Index = A_Num - 1
    Split_V = split(Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Value"])
    V_i = Split_V[Split_Index]
    Split_K = split(Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Key"])
    K_i = Split_K[Split_Index]
    Split_Q = split(Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Query"])
    Q_i = Split_Q[Split_Index]

   ###
    """
    D_WiKT_wrt_WKM_Nonjacobian = cp.empty((dk, model_dimension), dtype=object)
    for i in range(dk):
        for j in range(model_dimension):
            D_WiKT_ij_wrt_WKM = cp.zeros((1, model_dimension * model_dimension))
            D_WiKT_ij_wrt_WKM_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiKT_ij_wrt_WKM_mk = None
                    if j == m and k == ( dk*(A_Num - 1) + i):
                        D_WiKT_ij_wrt_WKM_mk = 1
                    else:
                        D_WiKT_ij_wrt_WKM_mk = 0
                    D_WiKT_ij_wrt_WKM[0,D_WiKT_ij_wrt_WKM_Col] = D_WiKT_ij_wrt_WKM_mk
                    D_WiKT_ij_wrt_WKM_Col += 1
                    if D_WiKT_ij_wrt_WKM_Col >= (model_dimension * model_dimension):
                        D_WiKT_ij_wrt_WKM_Col = 0
                        break
            D_WiKT_wrt_WKM_Nonjacobian[i,j] = D_WiKT_ij_wrt_WKM
    """
    D_WiKT_wrt_WKM_Nonjacobian = Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WKTi_wrt_WK_Nonjacobian"]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Input"]
    #XT = X.T
    XT = np.transpose(X, (0, 2, 1))
    #XT_Numpy = XT.get() #  Numpy
    XT_Numpy = XT # Cupy
    XT_Numpy = XT_Numpy[:, None, :, :]

    D_KiT_wrt_WKM_Nonjacobian = D_WiKT_wrt_WKM_Nonjacobian @ XT_Numpy
    #Q_i_Numpy = Q_i.get() # Numpy
    Q_i_Numpy = Q_i
    Q_i_Numpy = Q_i_Numpy[:, None, :, :]
    D_S_wrt_WKM_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * (Q_i_Numpy @ D_KiT_wrt_WKM_Nonjacobian)

    #D_S_wrt_WKM = To_Jacobian(D_S_wrt_WKM_Nonjacobian)  # Numpy
    D_S_wrt_WKM = reshape_cupy_4d_to_3d(D_S_wrt_WKM_Nonjacobian)

    P = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    S =  Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]
    D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, S) # softmax derivative

    D_Softmax_S_wrt_WKM = D_Softmax_S_wrt_S @ D_S_wrt_WKM

    Softmax_S = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
    B, Rows, Cols = Softmax_S.shape

    #D_Softmax_S_wrt_WKM_Nonjacobian = To_Nonjacobian(D_Softmax_S_wrt_WKM, Rows, Cols )  # Numpy
    D_Softmax_S_wrt_WKM_Nonjacobian = reshape_cupy_3d_to_4d(D_Softmax_S_wrt_WKM, Rows, Cols)

    #V_i_Numpy = V_i.get()  # Numpy
    V_i_Numpy = V_i
    V_i_Numpy = V_i_Numpy[:, None, :, :]
    D_Attention_i_wrt_WKM__ = D_Softmax_S_wrt_WKM_Nonjacobian @ V_i_Numpy

    return D_Attention_i_wrt_WKM__


def D_Lavg_wrt_WKMi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers):  # Refer to page 34 and page 45
    """
    Output:
    Jacobian of D_Lavg_wrt_WKM
    """
    Layer_Num = Decoder_Layer
    #Calculate D_OMAi_wrt_WKMi
    D_OMAi_wrt_WKMi = None
    A = []
    for i in range(1, number_of_heads + 1):
        A_Num = i
        D_Attention_i_wrt_WKM_ = D_Attention_i_wrt_WKM(Decoders, A_Num, Layer_Num)
        A.append(D_Attention_i_wrt_WKM_)
    #D_Concat_wrt_WKM = concatenate(A)  # Numpy
    D_Concat_wrt_WKM = concatenate_cupy_matrices(A)
    W_OM = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    #W_OM_Numpy = W_OM.get() # numpy
    W_OM_Numpy = W_OM
    D_OMAi_wrt_WKMi_Nonjacobian = D_Concat_wrt_WKM @ W_OM_Numpy


    #D_OMAi_wrt_WKMi = To_Jacobian(D_OMAi_wrt_WKMi_Nonjacobian)  # numpy
    D_OMAi_wrt_WKMi = reshape_cupy_4d_to_3d(D_OMAi_wrt_WKMi_Nonjacobian)

    """

    #D_Lavg_wrt_OC_ = D_Lavg_wrt_OC(Decoders, Decoder_Layer, Final_Layers)   # D_Lavg_wrt_OC_ = Alpha
    D_Lavg_wrt_OCAi_ = D_Lavg_wrt_OCAi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers)

    D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Layer_Num) # OC derivative wrt X (decoder input)
    O = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Output"]
    X = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Input"]
    D_LNMAi_wrt_OMAi =  compute_layernorm_derivative(O, X) #layernorm derivative

    D_OCAi_wrt_OMAi = D_OCAi_wrt_LNMAi @ D_LNMAi_wrt_OMAi

    D_OCAi_wrt_WKMi = D_OCAi_wrt_OMAi @ D_OMAi_wrt_WKMi  # D_OC_wrt_WQM = Alpha
    """
    D_Lavg_wrt_OMAi = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"D_Lavg_wrt_OMAi"]


    D_Lavg_wrt_WKMi = D_Lavg_wrt_OMAi @ D_OMAi_wrt_WKMi
    return D_Lavg_wrt_WKMi


def D_Attention_i_wrt_WQM(Decoders, A_Num, Layer_Num):     # refer to page 28



    """
    D_WiQ_wrt_WQM_Nonjacobian = cp.empty((model_dimension, dk), dtype=object)
    # Calculate D_WiQ_wrt_WQM_Nonjacobian
    for i in range(model_dimension):
        for j in range(dk):
            D_WiQ_ij_wrt_WQM = cp.zeros((1, model_dimension * model_dimension))
            D_WiQ_ij_wrt_WQM_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiQ_ij_wrt_WQM_mk = None
                    if i == m and k == ( dk*(A_Num - 1) + j):
                        D_WiQ_ij_wrt_WQM_mk = 1
                    else:
                        D_WiQ_ij_wrt_WQM_mk = 0
                    D_WiQ_ij_wrt_WQM[0,D_WiQ_ij_wrt_WQM_Col] = D_WiQ_ij_wrt_WQM_mk
                    D_WiQ_ij_wrt_WQM_Col += 1
                    if D_WiQ_ij_wrt_WQM_Col >= (model_dimension * model_dimension):
                        D_WiQ_ij_wrt_WQM_Col = 0
                        break
            D_WiQ_wrt_WQM_Nonjacobian[i,j] = D_WiQ_ij_wrt_WQM
    """
    D_WiQ_wrt_WQM_Nonjacobian = Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WQi_wrt_WQ_Nonjacobian"]
    Split_V = split(Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Value"])
    Split_Index = A_Num - 1
    V_i = Split_V[Split_Index]
    Split_K = split(Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"QKV"][f"Key"])

    K_i = Split_K[Split_Index]
    X = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"Input"]
    #X_Numpy = X.get() # numpy
    X_Numpy = X #Cupy
    X_Numpy = X_Numpy[:, None, :, :]
    D_Qi_wrt_WQM_Nonjacobian = X_Numpy @ D_WiQ_wrt_WQM_Nonjacobian
    #KiT = K_i.T
    KiT = np.transpose(K_i, (0, 2, 1))
    #KiT_Numpy = KiT.get() # Numpy
    KiT_Numpy = KiT
    KiT_Numpy = KiT_Numpy[:, None, :, :]
    D_S_wrt_WQM_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * (D_Qi_wrt_WQM_Nonjacobian @ KiT_Numpy)

    #D_S_wrt_WQM = To_Jacobian(D_S_wrt_WQM_Nonjacobian) # Numpy
    D_S_wrt_WQM = reshape_cupy_4d_to_3d(D_S_wrt_WQM_Nonjacobian)

    P = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # refer to page 18
    S =  Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"]
    D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, S) # softmax derivative

    D_Softmax_S_wrt_WQM = D_Softmax_S_wrt_S @ D_S_wrt_WQM

    Softmax_S = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
    B, Rows, Cols = Softmax_S.shape

    #D_Softmax_S_wrt_WQM_Nonjacobian = To_Nonjacobian(D_Softmax_S_wrt_WQM, Rows, Cols )  # Numpy
    D_Softmax_S_wrt_WQM_Nonjacobian = reshape_cupy_3d_to_4d(D_Softmax_S_wrt_WQM, Rows, Cols)
    #V_i_Numpy = V_i.get()  # Numpy
    V_i_Numpy = V_i  # Cupy
    V_i_Numpy = V_i_Numpy[:, None, :, :]
    D_Attention_i_wrt_WQM__ = D_Softmax_S_wrt_WQM_Nonjacobian @ V_i_Numpy

    return D_Attention_i_wrt_WQM__


def D_Lavg_wrt_OCAi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers):
    """
    Output:
    Jacobian of Derivative of loss average wrt cross attention output for the ith decoder layer
    """
    D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
    F = Number_Of_Decoder_Layers
    i = Decoder_Layer
    m = F - i  # m is number of decoders layers in front of decoder layer i
    D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
    D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_
    D_DOi_wrt_OCAi_ = D_DOi_wrt_OCAi(Decoders, Decoder_Layer)

    D_Lavg_wrt_OCAi_ = D_Lavg_wrt_DOi @ D_DOi_wrt_OCAi_
    return D_Lavg_wrt_OCAi_

def D_Lavg_wrt_WQMi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers):  # refer to page 28 and 45
    """
    Layer_Num - current decoder layer

    """
    Layer_Num = Decoder_Layer
    #Calculate D_OMAi_wrt_WQMi
    D_OMAi_wrt_WQMi = None
    A = []
    for i in range(1, number_of_heads + 1):
        A_Num = i
        D_Attention_i_wrt_WQM_ = D_Attention_i_wrt_WQM(Decoders, A_Num, Layer_Num)
        A.append(D_Attention_i_wrt_WQM_)
    #D_Concat_wrt_WQM = concatenate(A)  # Numpy
    D_Concat_wrt_WQM = concatenate_cupy_matrices(A)
    W_OM = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
    #W_OM_Numpy = W_OM.get()  # Numpy
    W_OM_Numpy = W_OM # Cupy
    D_OMAi_wrt_WQMi_Nonjacobian = D_Concat_wrt_WQM @ W_OM_Numpy

    #D_OMAi_wrt_WQMi = To_Jacobian(D_OMAi_wrt_WQMi_Nonjacobian)  # Numpy
    D_OMAi_wrt_WQMi = reshape_cupy_4d_to_3d(D_OMAi_wrt_WQMi_Nonjacobian)

    """
    D_Lavg_wrt_OCAi_ = D_Lavg_wrt_OCAi(Decoders, Decoder_Layer, Final_Layers, Number_Of_Decoder_Layers)
   # D_Lavg_wrt_OC_ = D_Lavg_wrt_OC(Decoders, Decoder_Layer, Final_Layers)   # D_Lavg_wrt_OC_ = Alpha


    D_OCAi_wrt_LNMAi = Compute_D_OC_wrt_X(Decoders, Layer_Num) # OC derivative wrt X (decoder input)
    O = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Output"]
    X = Decoders[f"Decoder{Layer_Num}"][f"Masked_Attention"][f"Input"]
    D_LNMAi_wrt_OMAi =  compute_layernorm_derivative(O, X) #layernorm derivative

    D_OCAi_wrt_OMAi = D_OCAi_wrt_LNMAi @ D_LNMAi_wrt_OMAi
    D_OCAi_wrt_WQMi = D_OCAi_wrt_OMAi @ D_OMAi_wrt_WQMi
    """
    D_Lavg_wrt_OMAi = Decoders[f"Decoder{Decoder_Layer}"][f"Masked_Attention"][f"D_Lavg_wrt_OMAi"]


    D_Lavg_wrt_WQMi = D_Lavg_wrt_OMAi @ D_OMAi_wrt_WQMi
    return D_Lavg_wrt_WQMi
def reshape_cupy_4d_to_3d(matrix_4d):
    """
    Converts a 4D CuPy matrix of shape (b, n, m, v) to a 3D CuPy matrix of shape (b, (m * v), n),
    where each 3D matrix in the input is reshaped using the same logic as `reshape_cupy_3d_to_2d`.
    """
    b, n, m, v = matrix_4d.shape  # Extract dimensions
    return matrix_4d.transpose(0, 2, 3, 1).reshape(b, m * v, n)

def reshape_cupy_3d_to_2d(matrix_3d):
    """
    Converts a 3D CuPy matrix of shape (n, m, v) to a 2D CuPy matrix of shape ((m * v), n),
    where each row in the output corresponds to an element in each matrix in the input.

    Parameters:
        matrix_3d (cp.ndarray): Input CuPy array of shape (n, m, v).

    Returns:
        cp.ndarray: Reshaped CuPy array of shape ((m * v), n).
    """
    n, m, v = matrix_3d.shape  # Extract dimensions
    matrix_2d = matrix_3d.transpose(1, 2, 0).reshape(m * v, n)  # Reorder and reshape
    return matrix_2d

def reshape_cupy_2d_to_3d(matrix_2d, rows, cols):
    """
    Converts a 2D CuPy matrix of shape (m, n) into a 3D CuPy matrix of shape (n, rows, cols),
    where each (rows x cols) matrix in the output corresponds to a row in the input.

    Parameters:
        matrix_2d (cp.ndarray): Input CuPy array of shape (m, n).
        rows (int): Number of rows in each matrix of the 3D output.
        cols (int): Number of columns in each matrix of the 3D output.

    Returns:
        cp.ndarray: Reshaped CuPy array of shape (n, rows, cols).
    """
    m, n = matrix_2d.shape  # Get input dimensions

    # Ensure the total elements match for a valid reshape
    if m != rows * cols:
        raise ValueError("rows * cols must equal the number of rows in the input matrix (m).")

    # Reshape and reorder the dimensions to get (n, rows, cols)
    matrix_3d = matrix_2d.reshape(rows, cols, n).transpose(2, 0, 1)

    return matrix_3d
def reshape_cupy_3d_to_4d(matrices_3d, rows, cols):
    """
    Converts a 3D CuPy matrix of shape (b, m, n) into a 4D CuPy matrix of shape (b, n, rows, cols),
    where each (rows x cols) matrix in the output corresponds to a row in the input matrices.
    """
    b, m, n = matrices_3d.shape
    if m != rows * cols:
        raise ValueError("rows * cols must equal the number of rows in the input matrices (m).")
    return matrices_3d.reshape(b, rows, cols, n).transpose(0, 3, 1, 2)


def D_Attention_i_wrt_WQS_Nonjacobian(Encoders, A_Num, Layer_Num, split_K, split_V):  # refer to page 30
    """
    Input:
    Layer_Num - Encoder layer number
    Output:
    D_Attention_i_wrt_WQS_Nonjacobian
    """
    """
    split_K = split(Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"QKV"][f"Key"])# Key matrix for Self attention from encoders)


    split_V = split(Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"QKV"][f"Value"])# Key matrix for Self attention from encoders)

    """
    split_Index = A_Num -1
    K_i = split_K[split_Index] # Key matrix for the ith attention head from encoders
    #KiT = K_i.T  # Change to  KiT = np.transpose(K_i, ( 0,2,1 ))
    KiT = np.transpose(K_i, (0, 2, 1))
    KiT = KiT[:, None, :, :]
    split_Index = A_Num -1
    V_i = split_V[split_Index] # value matrix for the ith attention head from encoders
    V_i = V_i[:, None, :, :]


    D_WiQ_wrt_WQS_Nonjacobian = Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WQi_wrt_WQ_Nonjacobian"]


    X = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Input"] # input to self attention from encoders
    #X_Numpy = X.get()
    X_Numpy = X
    X_Numpy = X_Numpy[:, None, :, :]
    D_Qi_wrt_WQS_Nonjacobian = np.matmul(X_Numpy , D_WiQ_wrt_WQS_Nonjacobian)


    D_S_wrt_WQS_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * np.matmul(D_Qi_wrt_WQS_Nonjacobian , KiT)

    D_S_wrt_WQS = reshape_cupy_4d_to_3d(D_S_wrt_WQS_Nonjacobian)

    D_Softmax_S_wrt_S = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"D_Softmax_S_wrt_S"]



    D_Softmax_S_wrt_WQS = np.matmul(D_Softmax_S_wrt_S , D_S_wrt_WQS)
    Softmax_S =  Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # Softmax_S for the ith attention head from encoders
    B, Rows, Cols = Softmax_S.shape

    D_Softmax_S_wrt_WQS_Nonjacobian = reshape_cupy_3d_to_4d(D_Softmax_S_wrt_WQS, Rows, Cols)



    D_Attention_i_wrt_WQS_Nonjacobian__ = np.matmul(D_Softmax_S_wrt_WQS_Nonjacobian , V_i)
    return  D_Attention_i_wrt_WQS_Nonjacobian__

"""
def D_Attention_i_wrt_WQS_Nonjacobian(Encoders, A_Num, Layer_Num):  # refer to page 30
    ""
    Input:
    Layer_Num - Encoder layer number
    Output:
    D_Attention_i_wrt_WQS_Nonjacobian
    ""

    ""
    D_WiQ_wrt_WQS_Nonjacobian = cp.empty((model_dimension, dk), dtype=object)
    for i in range(model_dimension):
        for j in range(dk):
            D_WiQ_ij_wrt_WQS = cp.zeros((1, model_dimension * model_dimension))

            ""
            D_WiQ_ij_wrt_WQS_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiQ_ij_wrt_WQS_mk = None
                    if i == m and k == ( dk*(A_Num - 1) + j):
                        D_WiQ_ij_wrt_WQS_mk = 1
                    else:
                        D_WiQ_ij_wrt_WQS_mk = 0
                    D_WiQ_ij_wrt_WQS[0,D_WiQ_ij_wrt_WQS_Col] = D_WiQ_ij_wrt_WQS_mk
                    D_WiQ_ij_wrt_WQS_Col += 1
                    if D_WiQ_ij_wrt_WQS_Col >= (model_dimension * model_dimension):
                        D_WiQ_ij_wrt_WQS_Col = 0
                        break
            ""
            Col = i * model_dimension + (dk*(A_Num - 1) + j)
            D_WiQ_ij_wrt_WQS[0, Col] = 1


            D_WiQ_wrt_WQS_Nonjacobian[i,j] = D_WiQ_ij_wrt_WQS
    ""
    ""
    # Cupy
    # Create an empty array with the correct shape
    D_WiQ_wrt_WQS_Nonjacobian = np.zeros((model_dimension * model_dimension, model_dimension, dk))

    # Compute indices efficiently
    rows = np.arange(model_dimension)[:, None]  # Shape (model_dimension, 1)
    cols = dk * (A_Num - 1) + np.arange(dk)    # Shape (dk,)

    # Compute the flattened row indices
    final_rows = rows * model_dimension + cols  # Shape (model_dimension, dk)

    # Assign 1s in a vectorized manner
    D_WiQ_wrt_WQS_Nonjacobian[final_rows, np.arange(model_dimension)[:, None], np.arange(dk)] = 1
    ""

    D_WiQ_wrt_WQS_Nonjacobian = Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WQi_wrt_WQ_Nonjacobian"]


    # Display result


    X = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Input"] # input to self attention from encoders
    #X_Numpy = X.get()
    X_Numpy = X
    D_Qi_wrt_WQS_Nonjacobian = X_Numpy @ D_WiQ_wrt_WQS_Nonjacobian
    split_K = split(Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"QKV"][f"Key"])# Key matrix for Self attention from encoders)
    split_Index = A_Num -1
    K_i = split_K[split_Index] # Key matrix for the ith attention head from encoders
    KiT = K_i.T
    #KiT_Numpy = KiT.get()
    KiT_Numpy = KiT
    D_S_wrt_WQS_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * (D_Qi_wrt_WQS_Nonjacobian @ KiT_Numpy)
    #D_S_wrt_WQS = To_Jacobian(D_S_wrt_WQS_Nonjacobian)
    D_S_wrt_WQS = reshape_cupy_3d_to_2d(D_S_wrt_WQS_Nonjacobian)

    #P = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] #Softmax_S for the ith attention head from encoders
    #S =  Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"] # S for the ith attention head from encoders
    #D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, S)
    D_Softmax_S_wrt_S = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"D_Softmax_S_wrt_S"]



    D_Softmax_S_wrt_WQS = D_Softmax_S_wrt_S @ D_S_wrt_WQS
    Softmax_S =  Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # Softmax_S for the ith attention head from encoders
    Rows, Cols = Softmax_S.shape
    #D_Softmax_S_wrt_WQS_Nonjacobian = To_Nonjacobian( D_Softmax_S_wrt_WQS, Rows, Cols)# @@@@@@@@@@@@2
    D_Softmax_S_wrt_WQS_Nonjacobian = reshape_cupy_2d_to_3d(D_Softmax_S_wrt_WQS, Rows, Cols)

    split_V = split(Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"QKV"][f"Value"])# Key matrix for Self attention from encoders)
    split_Index = A_Num -1
    V_i = split_V[split_Index] # value matrix for the ith attention head from encoders
    #V_i_Numpy = V_i.get()
    V_i_Numpy = V_i
    D_Attention_i_wrt_WQS_Nonjacobian__ = D_Softmax_S_wrt_WQS_Nonjacobian @ V_i_Numpy
    return  D_Attention_i_wrt_WQS_Nonjacobian__
"""
def D_Lavg_wrt_SO(Encoders, Decoders, Final_Layers, Encoder_Num, Decoder_Layer):
    """
    """
    # Calculate D_Lavg_wrt_SO
    Layer_Num = Decoder_Layer # Layer_Num is decoder layer number
    D_OC_wrt_E = derivative_cross_attention_output_wrt_encoder_output(Layer_Num, Decoders, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian) # OC dervative
    O = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Output"]
    X = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Input"]
    D_E_wrt_FFE = compute_layernorm_derivative(O, X) # Layernorm derivative
    D_OC_wrt_FFE = D_OC_wrt_E @ D_E_wrt_FFE
    Decoder = False
    W_FF1 = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
    W_FF2 = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
    ReLu_Input = Encoders[f"Encoder{Encoder_Num}"][f"Feed_Forward"][f"ReLu_Input"]




    D_FFE_wrt_LNS = compute_derivative_FF(Decoder, W_FF1, W_FF2, ReLu_Input) # Feed Forward deriative
    D_OC_wrt_LNS = D_OC_wrt_FFE @ D_FFE_wrt_LNS
    O = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Output"]
    X = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Input"]
    D_LNS_wrt_SO = compute_layernorm_derivative(O, X) #Layernorm derivative
    D_OC_wrt_SO = D_OC_wrt_LNS @ D_LNS_wrt_SO
    Alpha = D_Lavg_wrt_OC(Decoders, Decoder_Layer, Final_Layers) # alpha derivative (D_lavg_wrt_OC)
    D_Lavg_wrt_SO = Alpha @ D_OC_wrt_SO
    return D_Lavg_wrt_SO

def concatenate_cupy_matrices(matrices):
    """
    Concatenates a list of 3D CuPy matrices along the third dimension.

    Parameters:
        matrices (list of cp.ndarray): List of 3D CuPy matrices to be concatenated.

    Returns:
        cp.ndarray: The concatenated 3D CuPy matrix along the third dimension.
    """
    # Ensure all matrices have the same shape along the first and second dimensions
    shapes = [matrix.shape for matrix in matrices]
    if len(set(shapes)) > 1:
        raise ValueError("All matrices must have the same shape along the first two dimensions.")

    # Concatenate along the third axis (axis=2)
    concatenated_matrix = np.concatenate(matrices, axis=3)

    return concatenated_matrix
def D_Lavg_wrt_WQSi(Encoders, Decoders, Final_Layers, Encoder_Num, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers): #refer to page 30 and 45
    """
    Encoder_Num - Encoder Layer
    output:
    D_Lavg_wrt_WQS in jacobian form
    """
    Encoder_Layer = Encoder_Num
    # Calculate D_SOi_wrt_WQSi


    """
    start_time1 = time.time()  # Record start time
    A = []
    for i in range(1, number_of_heads+1):
        A_Num = i

        D_Attention_i_wrt_WQS_Nonjacobian_ = D_Attention_i_wrt_WQS_Nonjacobian(Encoders, A_Num, Encoder_Num)

        A.append(D_Attention_i_wrt_WQS_Nonjacobian_)

    D_Concat_wrt_WQS_Nonjacobian = concatenate_cupy_matrices(A)

    end_time1 = time.time()  # Record end time
    elapsed_time = end_time1 - start_time1  # Calculate elapsed time
    #print(f"Elapsed time for : {elapsed_time:.6f} seconds")
    """
    D_Concat_wrt_WQS_Nonjacobian = Encoders[f"Encoder{Encoder_Num}"][f"D_Concat_wrt_WQS_Nonjacobian"]

    W_O = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"] # From Encoders
    #W_O_Numpy = W_O.get()
    W_O_Numpy = W_O
    D_SO_wrt_WQS_Nonjacobian = D_Concat_wrt_WQS_Nonjacobian @ W_O_Numpy    # Test  #####################
    #D_SOi_wrt_WQSi = To_Jacobian(D_SO_wrt_WQS_Nonjacobian)
    D_SOi_wrt_WQSi = reshape_cupy_4d_to_3d(D_SO_wrt_WQS_Nonjacobian)

    # Calculate D_Lavg_wrt_SOi
    D_Lavg_wrt_SOi_ = Encoders[f"Encoder{Encoder_Num}"][f"D_Lavg_wrt_SOi"]

    D_Lavg_wrt_WQSi = D_Lavg_wrt_SOi_ @ D_SOi_wrt_WQSi

    return D_Lavg_wrt_WQSi

def D_Attention_i_wrt_WKS_Nonjacobian(Encoders, A_Num, Layer_Num, split_Q, split_V):
    """
    Input:
    Layer_Num - Encoder layer number
    A_Num - Attention head number
    Output:
    D_Attention_i_wrt_WKS_Nonjacobian
    """


    ###
    """
    # Numpy Implementation for D_WiKT_wrt_WKS_Nonjacobian
    D_WiKT_wrt_WKS_Nonjacobian = cp.empty((dk, model_dimension), dtype=object)
    for i in range(dk):
        for j in range(model_dimension):
            D_WiKT_ij_wrt_WKS = cp.zeros((1, model_dimension * model_dimension))
            ""
            D_WiKT_ij_wrt_WKS_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiKT_ij_wrt_WKS_mk = None
                    if j == m and k == ( dk*(A_Num - 1) + i):
                        D_WiKT_ij_wrt_WKS_mk = 1
                    else:
                        D_WiKT_ij_wrt_WKS_mk = 0
                    D_WiKT_ij_wrt_WKS[0,D_WiKT_ij_wrt_WKS_Col] = D_WiKT_ij_wrt_WKS_mk
                    D_WiKT_ij_wrt_WKS_Col += 1
                    if D_WiKT_ij_wrt_WKS_Col >= (model_dimension * model_dimension):
                        D_WiKT_ij_wrt_WKS_Col = 0
                        break
            ""
            Col = j * model_dimension + (dk*(A_Num - 1) + i)
            D_WiKT_ij_wrt_WKS[0, Col] = 1
            D_WiKT_wrt_WKS_Nonjacobian[i,j] = D_WiKT_ij_wrt_WKS

    """
    """
    # Cupy Implementation for D_WiKT_wrt_WKS_Nonjacobian

    D_WiKT_wrt_WKS_Nonjacobian = np.zeros((model_dimension * model_dimension, dk, model_dimension))

    for i in range(dk):
        for j in range(model_dimension):
            Col = j * model_dimension + (dk * (A_Num - 1) + i)
            D_WiKT_wrt_WKS_Nonjacobian[Col, i, j] = 1
    """


    D_WiKT_wrt_WKS_Nonjacobian = Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WKTi_wrt_WK_Nonjacobian"]

    X = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Input"] # input to self attention from encoders
    #XT = X.T
    XT = np.transpose(X, (0, 2, 1))
    XT = XT[:, None, :, :]
    #XT_Numpy = XT.get()   # Numpy Implementation
    #XT_Numpy = XT   # Cupy Implmentation

    D_KiT_wrt_WKS_Nonjacobian = D_WiKT_wrt_WKS_Nonjacobian @ XT
    #split_Q = split(Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"QKV"][f"Query"])# Key matrix for Self attention from encoders)
    split_Index = A_Num -1
    Q_i = split_Q[split_Index] # Key matrix for the ith attention head from encoders
    Q_i = Q_i[:, None, :, :]

    #Q_i_Numpy = Q_i.get() # Numpy Implementation
    #Q_i_Numpy = Q_i # Cupy Implementation
    start_time_T = time.time()  # Record start time
    D_S_wrt_WKS_Nonjacobian = (1/np.sqrt(dk).astype(np.float16)) * (Q_i @ D_KiT_wrt_WKS_Nonjacobian)
    end_time_T = time.time()  # Record end time
    elapsed_time = end_time_T - start_time_T  # Calculate elapsed time
    ##print(f"Elapsed time for WKSI Attention i: {elapsed_time:.6f} seconds")
    ##################


    #D_S_wrt_WKS = To_Jacobian(D_S_wrt_WKS_Nonjacobian) # Numpy Implementation
    D_S_wrt_WKS = reshape_cupy_4d_to_3d(D_S_wrt_WKS_Nonjacobian) # Cupy Implementation

    #P = np.random.rand(sequence_length, sequence_length)
    #P = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] #Softmax_S for the ith attention head from encoders
    #S =  Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"S"] # S for the ith attention head from encoders
    #D_Softmax_S_wrt_S = Derivative_SoftmaxS_wrt_S(P, P)
    D_Softmax_S_wrt_S = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"D_Softmax_S_wrt_S"]


    D_Softmax_S_wrt_WKS = D_Softmax_S_wrt_S @ D_S_wrt_WKS
    Softmax_S =  Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"] # Softmax_S for the ith attention head from encoders
    B, Rows, Cols = Softmax_S.shape

    #D_Softmax_S_wrt_WKS_Nonjacobian = To_Nonjacobian( D_Softmax_S_wrt_WKS, Rows, Cols) # Numpy Implementation
    D_Softmax_S_wrt_WKS_Nonjacobian = reshape_cupy_3d_to_4d(D_Softmax_S_wrt_WKS, Rows, Cols)



    ##################3

    #split_V = split(Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"QKV"][f"Value"])# Key matrix for Self attention from encoders)
    split_Index = A_Num -1

    V_i = split_V[split_Index] # value matrix for the ith attention head from encoders

    V_i = V_i[:, None, :, :]
    #V_i_Numpy = V_i.get() # Numpy Implementation
    #V_i_Numpy = V_i # Cupy Implementation
    D_Attention_i_wrt_WKS_Nonjacobian__ = D_Softmax_S_wrt_WKS_Nonjacobian @ V_i


    return  D_Attention_i_wrt_WKS_Nonjacobian__
def D_Lavg_wrt_WKSi(Encoders, Decoders, Final_Layers, Encoder_Num, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers):  # refer to page 32
    """
    Encoder_Num - Encoder layer number which WKS resides in

    Output:
    D_Lavg_wrt_WKS in jacobian form
    """
    # Calculate D_SOi_wrt_WKSi
    A = []
    start_time_T = time.time()  # Record start time

    Layer_Num = Encoder_Num

    """
    for i in range(1, number_of_heads+1):

        A_Num = i


        D_Attention_i_wrt_WKS_Nonjacobian_ =  D_Attention_i_wrt_WKS_Nonjacobian(Encoders, A_Num, Layer_Num) #D_Attention_i_wrt_WKS_Nonjacobian(Encoders, A_Num, Encoder_Num)
        A.append(D_Attention_i_wrt_WKS_Nonjacobian_)
    #D_Concat_wrt_WKS_Nonjacobian = concatenate(A) # Numpy Implementation
    D_Concat_wrt_WKS_Nonjacobian = concatenate_cupy_matrices(A) # Cupy Implementation
    """
    D_Concat_wrt_WKS_Nonjacobian = Encoders[f"Encoder{Layer_Num}"][f"D_Concat_wrt_WKS_Nonjacobian"]

    W_O = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"] # From Encoders
    #W_O_Numpy = W_O.get() # Numpy Implementation
    W_O_Numpy = W_O # Cupy Implementation
    D_SO_wrt_WKS_Nonjacobian = D_Concat_wrt_WKS_Nonjacobian @ W_O_Numpy

    #D_SOi_wrt_WKSi = To_Jacobian(D_SO_wrt_WKS_Nonjacobian) # Numpy Implementation
    D_SOi_wrt_WKSi =  reshape_cupy_4d_to_3d(D_SO_wrt_WKS_Nonjacobian) # Cupy Implementation
    # Calculate D_Lavg_wrt_SOi
    D_Lavg_wrt_SOi_ = Encoders[f"Encoder{Encoder_Num}"][f"D_Lavg_wrt_SOi"]

    D_Lavg_wrt_WKSi = D_Lavg_wrt_SOi_ @ D_SOi_wrt_WKSi
    end_time_T = time.time()  # Record end time
    elapsed_time = end_time_T - start_time_T  # Calculate elapsed time
    #print(f"Elapsed time for WKSI Attention: {elapsed_time:.6f} seconds")
    return D_Lavg_wrt_WKSi

def D_Attention_i_wrt_WVS_Nonjacobian(Encoders, A_Num, Layer_Num):
    """
    Input:
    Layer_Num - Encoder layer number
    A_Num - Attention head number
    Output:
    D_Attention_i_wrt_WVS_Nonjacobian
    """
    """
    # calculate D_WiV_wrt_WVS_Nonjacobian (Numpy Implementation)

    D_WiV_wrt_WVS_Nonjacobian = cp.empty((model_dimension, dk), dtype=object)
    for i in range(model_dimension):
        for j in range(dk):
            D_WiV_ij_wrt_WVS = cp.zeros((1, model_dimension * model_dimension))
            ""
            D_WiV_ij_wrt_WVS_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WiV_ij_wrt_WVS_mk = None
                    if m == i and k == ( dk*(A_Num - 1) + j):
                        D_WiV_ij_wrt_WVS_mk = 1
                    else:
                        D_WiV_ij_wrt_WVS_mk = 0
                    D_WiV_ij_wrt_WVS[0,D_WiV_ij_wrt_WVS_Col] = D_WiV_ij_wrt_WVS_mk
                    D_WiV_ij_wrt_WVS_Col += 1
                    if D_WiV_ij_wrt_WVS_Col >= (model_dimension * model_dimension):
                        D_WiV_ij_wrt_WVS_Col = 0
                        break

            ""
            Col = i * model_dimension + (dk*(A_Num - 1) + j)
            D_WiV_ij_wrt_WVS[0, Col] = 1
            D_WiV_wrt_WVS_Nonjacobian[i,j] = D_WiV_ij_wrt_WVS
    """
    """
    # calculate D_WiV_wrt_WVS_Nonjacobian (Cupy Implementation)
    D_WiV_wrt_WVS_Nonjacobian = np.zeros((model_dimension * model_dimension, model_dimension, dk))

    for i in range(model_dimension):
        for j in range(dk):
            Col = j * model_dimension + (dk * (A_Num - 1) + i)
            D_WiV_wrt_WVS_Nonjacobian[Col, i, j] = 1
    """
    D_WiV_wrt_WVS_Nonjacobian = Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WVi_wrt_WV_Nonjacobian"]


    X = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Input"]
    #X_Numpy = X.get() # Numpy
    X_Numpy = X # Cupy
    X_Numpy = X_Numpy[:, None, :, :]
    D_Vi_wrt_WVS_Nonjacobian = X_Numpy @ D_WiV_wrt_WVS_Nonjacobian
    Softmax_S = Encoders[f"Encoder{Layer_Num}"][f"Self_Attention"][f"Attention_Heads"][f"Attention_Head{A_Num}"][f"Softmax_S"]
    #Softmax_S_Numpy = Softmax_S.get() # Numpy
    Softmax_S_Numpy = Softmax_S # Cupy
    Softmax_S_Numpy = Softmax_S_Numpy[:, None, :, :]
    D_Attention_i_wrt_WVS_Nonjacobian = Softmax_S_Numpy @ D_Vi_wrt_WVS_Nonjacobian
    return D_Attention_i_wrt_WVS_Nonjacobian
def D_Lavg_wrt_WVSi(Encoders, Decoders, Final_Layers, Encoder_Num, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers):  # refer to page 33
    """
    Output:
    D_Lavg_wrt_WVS in jacobian form
    """
    # Calculate D_SO_wrt_WVS
    A = []
    Layer_Num = Encoder_Num
    start_time_WOSA_Adam = time.time()  # Record start time
    for i in range(1, number_of_heads+1):
        A_Num = i
        D_Attention_i_wrt_WVS_Nonjacobian_ =  D_Attention_i_wrt_WVS_Nonjacobian(Encoders, A_Num, Layer_Num) #D_Attention_i_wrt_WKS_Nonjacobian(Encoders, A_Num, Encoder_Num)
        A.append(D_Attention_i_wrt_WVS_Nonjacobian_)


    end_time_WOSA_Adam = time.time()  # Record end time
    elapsed_time = end_time_WOSA_Adam - start_time_WOSA_Adam  # Calculate elapsed time
    #print(f"Elapsed time for D_Lavg_wrt_WVSi Attention: {elapsed_time:.6f} seconds")

    #D_Concat_wrt_WVS_Nonjacobian = concatenate(A) # Numpy
    D_Concat_wrt_WVS_Nonjacobian =  concatenate_cupy_matrices(A) # Cupy
    W_O = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"] # From Encoders
    #W_O_Numpy = W_O.get() # Numpy
    W_O_Numpy = W_O # Cupy
    D_SO_wrt_WVS_Nonjacobian = D_Concat_wrt_WVS_Nonjacobian @ W_O_Numpy
    #D_SOi_wrt_WVSi = To_Jacobian(D_SO_wrt_WVS_Nonjacobian) # Numpy
    D_SOi_wrt_WVSi = reshape_cupy_4d_to_3d(D_SO_wrt_WVS_Nonjacobian)  # Cupy
    # Calculate D_Lavg_wrt_SOi
    D_Lavg_wrt_SOi_ = Encoders[f"Encoder{Encoder_Num}"][f"D_Lavg_wrt_SOi"]

    D_Lavg_wrt_WVSi = D_Lavg_wrt_SOi_ @ D_SOi_wrt_WVSi
    return D_Lavg_wrt_WVSi
def D_Lavg_wrt_WOSi(Encoders, Decoders, Final_Layers, Encoder_Num, Number_Of_Decoder_Layers, Number_Of_Encoder_Layers):   # refer to pagge 34
    """
    Output:
    D_Lavg_wrt_WOS in jacobian form
    """

    # Calculate D_SO_wrt_WOS
    """
    #Numpy implementation
    D_WO_wrt_WO_Nonjacobian = cp.empty((model_dimension, model_dimension), dtype=object)
    for i in range(model_dimension):
        for j in range(model_dimension):
            D_WO_ij_wrt_WO = cp.zeros((1, model_dimension * model_dimension))
            ""
            D_WO_ij_wrt_WO_Col = 0
            for m in range(model_dimension):
                for k in range(model_dimension):
                    D_WO_ij_wrt_WO_mk = None
                    if i == m and k == j:
                        D_WO_ij_wrt_WO_mk = 1
                    else:
                        D_WO_ij_wrt_WO_mk = 0
                    D_WO_ij_wrt_WO[0,D_WO_ij_wrt_WO_Col] = D_WO_ij_wrt_WO_mk
                    D_WO_ij_wrt_WO_Col += 1
                    if D_WO_ij_wrt_WO_Col >= (model_dimension * model_dimension):
                        D_WO_ij_wrt_WO_Col = 0
                        break
            ""
            Col = i * model_dimension + j
            D_WO_ij_wrt_WO[0, Col] = 1
            D_WO_wrt_WO_Nonjacobian[i,j] = D_WO_ij_wrt_WO
    """

    """
    # Cupy implementation
    D_WO_wrt_WO = np.eye(model_dimension * model_dimension)
    Rows = model_dimension
    Cols = model_dimension

    D_WO_wrt_WO_Nonjacobian = reshape_cupy_2d_to_3d(D_WO_wrt_WO, Rows, Cols)
    """
    D_WO_wrt_WO_Nonjacobian = Encoders[f"D_WO_wrt_WO_Nonjacobian"]


    Concat = Encoders[f"Encoder{Encoder_Num}"][f"Self_Attention"][f"Concat"]
    Concat = Concat[:, None, :, :]
    #Concat_Numpy = Concat.get()  # Numpy
    #Concat_Numpy = Concat
    D_SO_wrt_WOS_Nonjacobian = Concat @ D_WO_wrt_WO_Nonjacobian

    #D_SOi_wrt_WOSi = To_Jacobian(D_SO_wrt_WOS_Nonjacobian)  # Numpy
    D_SOi_wrt_WOSi = reshape_cupy_4d_to_3d(D_SO_wrt_WOS_Nonjacobian)
    # Calculate D_Lavg_wrt_SO
    D_Lavg_wrt_SOi_ = Encoders[f"Encoder{Encoder_Num}"][f"D_Lavg_wrt_SOi"]

    D_Lavg_wrt_WOSi = D_Lavg_wrt_SOi_ @ D_SOi_wrt_WOSi
    return D_Lavg_wrt_WOSi

def D_Lavg_wrt_WFF1i(D_WFF1_wrt_WFF1_Nonjacobian, D_Lavg_wrt_EOF_, Encoder_Layer, Decoder_Layer, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, Final_Layers, Encoders = None, Decoders = None, Is_Encoders = False, Is_Decoders = False): # refer to page 41 bottom half and page 45
    """
    Input:
    Encoders - dictionary containing the input outputs to all the layers in encoder
    If Encoders dict is passed in and not Decoders then assumption is were computing D_Lavg_wrt_WFF1 for one of the for the FF layer of one of the Encoder layers and vice versa
    Decoders - dictionary containing the input outputs to all the layers in encoder
    Encoder_Layer - Encoder layer number for the which the weight matrix FF resides in
    Decoder_Layer - Decoder layer number for the which the weight matrix FF resides in
    Last_Decoder_Layer_Num - number for the last decoder layer
    Is_Encoders - indicates if the derivative wrt FF is in encoders by setting this variable to true otherwise false
    Is_Decoders - indicates if the derivative wrt FF is in decoders by setting this variable to true otherwise false

    Output:
    Jacobian of D_Lavg_wrt_WFF1 in jacobian form
    """
    Last_Decoder_Layer_Num = Number_Of_Decoder_Layers
    # Calculate D_ReLu_wrt_WFF1_Nonjacobian and determine WFF2 and D_Lavg_wrt_FF based on if Encoders or Decoders
    D_ReLu_wrt_WFF1_Nonjacobian = None
    WFF2 = None
    D_Lavg_wrt_FFi = None

    if Is_Encoders:

        # Calculate D_Lavg_wrt_FFi
        #D_Lavg_wrt_EOF_ = D_Lavg_wrt_E(Number_Of_Decoder_Layers, Decoders, Final_Layers)
        """
        F = Number_Of_Encoder_Layers
        m = F - Encoder_Layer
        D_EOF_wrt_EOi_ = D_EOF_wrt_EOi(m, F, Encoders)
        D_Lavg_wrt_EOi = D_Lavg_wrt_EOF_ @ D_EOF_wrt_EOi_   # EOi = LNFFi = Layernorm FF of ith Encoder Layer
        """
        D_Lavg_wrt_EOi = Encoders[f"Encoder{Encoder_Layer}"][f"D_Lavg_wrt_EOi"]

        """
        O = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Output"]
        X = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Input"]
        D_LNFFi_FFi = compute_layernorm_derivative(O, X)
        """
        D_LNFFi_FFi = Encoders[f"Encoder{Encoder_Layer}"][f"D_LNFFi_wrt_FFi"]
        D_Lavg_wrt_FFi = D_Lavg_wrt_EOi @ D_LNFFi_FFi

        Layer_Num = Encoder_Layer
        # Assign WFF2
        WFF2 = Encoders[f"Encoder{Layer_Num}"][f"Feed_Forward"][f"Weights"][f"WFF2"]

        # Calculate D_ReLuR_wrt_R
        """
        ReLu_Input = Encoders[f"Encoder{Layer_Num}"][f"Feed_Forward"][f"ReLu_Input"]

        RowsR, ColsR = ReLu_Input.shape


        D_ReLuR_wrt_R_Nonjacobian = cp.empty((RowsR, ColsR), dtype=object)
        for i in range(RowsR):
            for j in range(ColsR):
                D_ReLuRij_wrt_R = cp.zeros((1, RowsR * ColsR))
                D_ReLuRij_wrt_R_Col = 0
                for m in range(RowsR):
                    for k in range(ColsR):
                        D_ReLuRij_wrt_Rmk = None
                        D_ReLuRij_wrt_Rij = None
                        Rij = ReLu_Input[i,j]
                        if Rij < 0:
                            D_ReLuRij_wrt_Rij = 0
                        else:
                            D_ReLuRij_wrt_Rij = 1
                        D_Rij_wrt_Rmk = None
                        if m == i and k == j:
                            D_Rij_wrt_Rmk = 1
                        else:
                            D_Rij_wrt_Rmk = 0


                        D_ReLuRij_wrt_Rmk = D_ReLuRij_wrt_Rij * D_Rij_wrt_Rmk
                        D_ReLuRij_wrt_R[0,D_ReLuRij_wrt_R_Col] = D_ReLuRij_wrt_Rmk
                        D_ReLuRij_wrt_R_Col += 1
                        if D_ReLuRij_wrt_R_Col >= (RowsR * ColsR):
                            D_ReLuRij_wrt_R_Col = 0
                            break
                D_ReLuR_wrt_R_Nonjacobian[i,j] = D_ReLuRij_wrt_R
        D_ReLuR_wrt_R = To_Jacobian(D_ReLuR_wrt_R_Nonjacobian)
        """
        ReLu_Input = Encoders[f"Encoder{Layer_Num}"][f"Feed_Forward"][f"ReLu_Input"]
        # Cupy
        # Get dimensions
        B, S, F = ReLu_Input.shape  # sequence_length_I and feed_forward_dimension

        # Compute the ReLU derivative mask
        D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)  # Shape: (batch_size, S, F)

        # Flatten the indices for the output 4D tensor
        ColR = np.arange(S * F)

        # Create an empty 4D matrix
        D_ReLuInput_wrt_Input_nonjacobian = np.zeros((batch_size, S * F, S, F))

        # Populate the matrix with the computed values for each batch
        D_ReLuInput_wrt_Input_nonjacobian[:, ColR, ColR // F, ColR % F] = D_ReLuXij_wrt_Xij.reshape(batch_size, -1)
        """
        ###
        S, F = ReLu_Input.shape  # sequence_length_I and feed_forward_dimension
        #S = sequence_length
        #F = feed_forward_dimension

        # Compute the ReLU derivative mask
        D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)


        # Flatten the indices for the output 3D matrix
        ColR = np.arange(S * F)


        # Create an empty 3D matrix
        D_ReLuInput_wrt_Input_nonjacobian = np.zeros((S * F, S, F))


        # Populate the matrix with the computed values
        D_ReLuInput_wrt_Input_nonjacobian[ColR, ColR // F, ColR % F] = D_ReLuXij_wrt_Xij.flatten()
        ######
        """
        #D_ReLuInput_wrt_Input = To_Jacobian(D_ReLuInput_wrt_Input_nonjacobian) # Numpy
        D_ReLuR_wrt_R = reshape_cupy_4d_to_3d(D_ReLuInput_wrt_Input_nonjacobian)

        # Calculate D_R_wrt_WFF1
        X = Encoders[f"Encoder{Layer_Num}"][f"Feed_Forward"][f"Input"]

        """

        D_WFF1_wrt_WFF1_Nonjacobian = cp.empty((model_dimension, feed_forward_dimension), dtype=object)
        for i in range(model_dimension):
            for j in range(feed_forward_dimension):
                D_Wij_wrt_WFF1 = cp.zeros((1, model_dimension * feed_forward_dimension))
                D_Wij_wrt_WFF1_Col = 0
                for m in range(model_dimension):
                    for k in range(feed_forward_dimension):
                        D_Wij_wrt_Wmk = None
                        if m == i and k == j:
                            D_Wij_wrt_Wmk = 1
                        else:
                            D_Wij_wrt_Wmk = 0
                        D_Wij_wrt_WFF1[0,D_Wij_wrt_WFF1_Col] = D_Wij_wrt_Wmk
                        D_Wij_wrt_WFF1_Col += 1
                        if D_Wij_wrt_WFF1_Col >= (model_dimension * feed_forward_dimension):
                            D_Wij_wrt_WFF1_Col = 0
                            break
                D_WFF1_wrt_WFF1_Nonjacobian[i,j] = D_Wij_wrt_WFF1
        """
        #X_Numpy = X.get()
        X_Numpy = X
        X_Numpy = X_Numpy[:, None, :, :]
        D_R_wrt_WFF1_Nonjacobian = X_Numpy @ D_WFF1_wrt_WFF1_Nonjacobian
        #D_R_wrt_WFF1 = To_Jacobian(D_R_wrt_WFF1_Nonjacobian)  # Numpy
        D_R_wrt_WFF1 = reshape_cupy_4d_to_3d(D_R_wrt_WFF1_Nonjacobian)

        D_ReLu_wrt_WFF1 = D_ReLuR_wrt_R @ D_R_wrt_WFF1

        ReLu_Input = Encoders[f"Encoder{Layer_Num}"][f"Feed_Forward"][f"ReLu_Input"]
        B, Rows, Cols =  ReLu_Input.shape # rows and cols for ReLu input shape
        #D_ReLu_wrt_WFF1_Nonjacobian = To_Nonjacobian(D_ReLu_wrt_WFF1, Rows, Cols)  # Numpy
        D_ReLu_wrt_WFF1_Nonjacobian = reshape_cupy_3d_to_4d(D_ReLu_wrt_WFF1, Rows, Cols) # Cupy

    if Is_Decoders:
        """
        """


        ######
        # Calculate D_Lavg_wrt_FFi
        """
        D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
        F = Number_Of_Decoder_Layers
        i = Decoder_Layer
        m = F - i  # m is number of decoders layers in front of decoder layer i
        D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
        D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_

        """
        D_Lavg_wrt_DOi = Decoders[f"Decoder{Decoder_Layer}"][f"D_Lavg_wrt_DOi"]  # CuPy Matrix

        """
        O = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Input"]
        D_DOi_wrt_FFi = compute_layernorm_derivative(O, X)  #LNFFi is the layernorm FF output for the ith decoder where i is the decoder layer for the WQC derivative that is being computed for
        """
        D_DOi_wrt_FFi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNFFi_wrt_FFi"]
        D_Lavg_wrt_FFi = D_Lavg_wrt_DOi @ D_DOi_wrt_FFi
        ############

        # Assign WFF2
        WFF2 = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        """
        # Calculate D_ReLuR_wrt_R
        ReLu_Input = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
        RowsR, ColsR = ReLu_Input.shape

        D_ReLuR_wrt_R_Nonjacobian = cp.empty((RowsR, ColsR), dtype=object)
        for i in range(RowsR):
            for j in range(ColsR):
                D_ReLuRij_wrt_R = cp.zeros((1, RowsR * ColsR))
                D_ReLuRij_wrt_R_Col = 0
                for m in range(RowsR):
                    for k in range(ColsR):
                        D_ReLuRij_wrt_Rmk = None
                        D_ReLuRij_wrt_Rij = None
                        Rij = ReLu_Input[i,j]
                        if Rij < 0:
                            D_ReLuRij_wrt_Rij = 0
                        else:
                            D_ReLuRij_wrt_Rij = 1
                        D_Rij_wrt_Rmk = None
                        if m == i and k == j:
                            D_Rij_wrt_Rmk = 1
                        else:
                            D_Rij_wrt_Rmk = 0


                        D_ReLuRij_wrt_Rmk = D_ReLuRij_wrt_Rij * D_Rij_wrt_Rmk
                        D_ReLuRij_wrt_R[0,D_ReLuRij_wrt_R_Col] = D_ReLuRij_wrt_Rmk
                        D_ReLuRij_wrt_R_Col += 1
                        if D_ReLuRij_wrt_R_Col >= (RowsR * ColsR):
                            D_ReLuRij_wrt_R_Col = 0
                            break
                D_ReLuR_wrt_R_Nonjacobian[i,j] = D_ReLuRij_wrt_R
        D_ReLuR_wrt_R = To_Jacobian(D_ReLuR_wrt_R_Nonjacobian)
        """
        ReLu_Input = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
        # Cupy
        # Get dimensions
        # Get dimensions
        B, S, F = ReLu_Input.shape  # sequence_length_I and feed_forward_dimension

        # Compute the ReLU derivative mask
        D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)  # Shape: (batch_size, S, F)

        # Flatten the indices for the output 4D tensor
        ColR = np.arange(S * F)

        # Create an empty 4D matrix
        D_ReLuInput_wrt_Input_nonjacobian = np.zeros((batch_size, S * F, S, F))

        # Populate the matrix with the computed values for each batch
        D_ReLuInput_wrt_Input_nonjacobian[:, ColR, ColR // F, ColR % F] = D_ReLuXij_wrt_Xij.reshape(batch_size, -1)
        """
        ###
        B, S, F = ReLu_Input.shape  # sequence_length_I and feed_forward_dimension
        #S = sequence_length
        #F = feed_forward_dimension

        # Compute the ReLU derivative mask
        D_ReLuXij_wrt_Xij = np.where(ReLu_Input >= 0, 1, 0)


        # Flatten the indices for the output 3D matrix
        ColR = np.arange(S * F)


        # Create an empty 3D matrix
        D_ReLuInput_wrt_Input_nonjacobian = np.zeros((S * F, S, F))


        # Populate the matrix with the computed values
        D_ReLuInput_wrt_Input_nonjacobian[ColR, ColR // F, ColR % F] = D_ReLuXij_wrt_Xij.flatten()
        ###
        """
        #D_ReLuInput_wrt_Input = To_Jacobian(D_ReLuInput_wrt_Input_nonjacobian) # Numpy
        D_ReLuR_wrt_R = reshape_cupy_4d_to_3d(D_ReLuInput_wrt_Input_nonjacobian)

        # Calculate D_R_wrt_WFF1
        X = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Input"]
        """
        D_WFF1_wrt_WFF1_Nonjacobian = cp.empty((model_dimension, feed_forward_dimension), dtype=object)
        for i in range(model_dimension):
            for j in range(feed_forward_dimension):
                D_Wij_wrt_WFF1 = cp.zeros((1, model_dimension * feed_forward_dimension))
                D_Wij_wrt_WFF1_Col = 0
                for m in range(model_dimension):
                    for k in range(feed_forward_dimension):
                        D_Wij_wrt_Wmk = None
                        if m == i and k == j:
                            D_Wij_wrt_Wmk = 1
                        else:
                            D_Wij_wrt_Wmk = 0
                        D_Wij_wrt_WFF1[0,D_Wij_wrt_WFF1_Col] = D_Wij_wrt_Wmk
                        D_Wij_wrt_WFF1_Col += 1
                        if D_Wij_wrt_WFF1_Col >= (model_dimension * feed_forward_dimension):
                            D_Wij_wrt_WFF1_Col = 0
                            break
                D_WFF1_wrt_WFF1_Nonjacobian[i,j] = D_Wij_wrt_WFF1
        """
        #X_Numpy = X.get()
        X_Numpy = X
        X_Numpy = X_Numpy[:, None, :, :]
        D_R_wrt_WFF1_Nonjacobian = X_Numpy @ D_WFF1_wrt_WFF1_Nonjacobian
        #D_R_wrt_WFF1 = To_Jacobian(D_R_wrt_WFF1_Nonjacobian)  # Numpy
        D_R_wrt_WFF1 = reshape_cupy_4d_to_3d(D_R_wrt_WFF1_Nonjacobian)

        D_ReLu_wrt_WFF1 = D_ReLuR_wrt_R @ D_R_wrt_WFF1

        ReLu_Input = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu_Input"]
        B, Rows, Cols =  ReLu_Input.shape # rows and cols for ReLu input shape
        #D_ReLu_wrt_WFF1_Nonjacobian = To_Nonjacobian(D_ReLu_wrt_WFF1, Rows, Cols)  # Numpy

        D_ReLu_wrt_WFF1_Nonjacobian = reshape_cupy_3d_to_4d(D_ReLu_wrt_WFF1, Rows, Cols) # Cupy


    #W_FF2_Numpy = WFF2.get()
    W_FF2_Numpy = WFF2
    D_FF_wrt_WFF1_Nonjacobian = D_ReLu_wrt_WFF1_Nonjacobian @ W_FF2_Numpy
    #D_FFi_wrt_WFF1i = To_Jacobian(D_FF_wrt_WFF1_Nonjacobian)  # Numpy
    D_FFi_wrt_WFF1i = reshape_cupy_4d_to_3d(D_FF_wrt_WFF1_Nonjacobian)


    D_Lavg_wrt_WFF1i = D_Lavg_wrt_FFi @ D_FFi_wrt_WFF1i
    return D_Lavg_wrt_WFF1i




def D_Lavg_wrt_WFF2i(D_WFF2_wrt_WFF2_Nonjacobian, D_Lavg_wrt_EOF_, Encoder_Layer, Decoder_Layer, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, Final_Layers, Encoders = None, Decoders = None, Is_Encoders = False, Is_Decoders = False):  #refer to page 42
    """
    Encoder_Layer - encoder layer number where WFF2 is in
    Decoder_Layer - decoder layer number where WFF2 is in
    Last_Decoder_Layer_Num - last decoder layer number
    Output:
    Jacobian of D of Loss wrt WFF2
    """
    D_Lavg_wrt_FFi = None
    D_FF_wrt_WFF2 = None
    if Is_Encoders:
        # Calculate D_Lavg_wrt_FFi
        #D_Lavg_wrt_EOF_ = D_Lavg_wrt_E(Number_Of_Decoder_Layers, Decoders, Final_Layers)
        """
        F = Number_Of_Encoder_Layers
        m = F - Encoder_Layer
        start_time = time.time()  # Record start time
        D_EOF_wrt_EOi_ = D_EOF_wrt_EOi(m, F, Encoders)

        end_time = time.time()  # Record end time

        elapsed_time = end_time - start_time  # Calculate elapsed time
        #print(f"Elapsed time for D_EOF_wrt_EOi_: {elapsed_time:.6f} seconds")

        D_Lavg_wrt_EOi = D_Lavg_wrt_EOF_ @ D_EOF_wrt_EOi_   # EOi = LNFFi = Layernorm FF of ith Encoder Layer
        """
        D_Lavg_wrt_EOi = Encoders[f"Encoder{Encoder_Layer}"][f"D_Lavg_wrt_EOi"]


        """
        O = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Output"]
        X = Encoders[f"Encoder{Encoder_Layer}"][f"Feed_Forward"][f"Input"]
        D_LNFFi_FFi = compute_layernorm_derivative(O, X)
        """
        D_LNFFi_FFi = Encoders[f"Encoder{Encoder_Layer}"][f"D_LNFFi_wrt_FFi"]


        D_Lavg_wrt_FFi = D_Lavg_wrt_EOi @ D_LNFFi_FFi
        Layer_Num = Encoder_Layer

        start_time = time.time()  # Record start time
        # calculate D_FFi_wrt_WFF2i
        ReLu = Encoders[f"Encoder{Layer_Num}"][f"Feed_Forward"][f"ReLu"]
        """
        D_WFF2_wrt_WFF2_Nonjacobian = cp.empty((feed_forward_dimension, model_dimension), dtype=object)
        for i in range(feed_forward_dimension):
            for j in range(model_dimension):
                D_Wij_wrt_WFF2 = cp.zeros((1, feed_forward_dimension * model_dimension))
                D_Wij_wrt_WFF2_Col = 0
                for m in range(feed_forward_dimension):
                    for k in range(model_dimension):
                        D_Wij_wrt_Wmk = None
                        if m == i and k == j:
                            D_Wij_wrt_Wmk = 1
                        else:
                            D_Wij_wrt_Wmk = 0
                        D_Wij_wrt_WFF2[0,D_Wij_wrt_WFF2_Col] = D_Wij_wrt_Wmk
                        D_Wij_wrt_WFF2_Col += 1
                        if D_Wij_wrt_WFF2_Col >= (feed_forward_dimension * model_dimension):
                            D_Wij_wrt_WFF2_Col = 0
                            break
                D_WFF2_wrt_WFF2_Nonjacobian[i,j] = D_Wij_wrt_WFF2
        """
        #ReLu_Numpy = ReLu.get()
        ReLu_Numpy = ReLu
        ReLu_Numpy = ReLu_Numpy[:, None, :, :]

        D_FF_wrt_WFF2_Nonjacobian = ReLu_Numpy @ D_WFF2_wrt_WFF2_Nonjacobian
        #D_FF_wrt_WFF2 = To_Jacobian(D_FF_wrt_WFF2_Nonjacobian)  # Numpy
        D_FF_wrt_WFF2 = reshape_cupy_4d_to_3d(D_FF_wrt_WFF2_Nonjacobian)


    if Is_Decoders:
        """
        """

        ######
        # Calculate D_Lavg_wrt_FFi
        """
        D_Lavg_wrt_DOF_ = D_Lavg_wrt_DOF(Final_Layers)
        F = Number_Of_Decoder_Layers
        i = Decoder_Layer
        m = F - i  # m is number of decoders layers in front of decoder layer i
        D_DOF_wrt_DOi_ = D_DOF_wrt_DOi(m, F, Decoders)
        D_Lavg_wrt_DOi = D_Lavg_wrt_DOF_ @ D_DOF_wrt_DOi_
        """

        D_Lavg_wrt_DOi = Decoders[f"Decoder{Decoder_Layer}"][f"D_Lavg_wrt_DOi"]  # CuPy Matrix


        ######
        """
        O = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Output"]
        X = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"Input"]
        D_DOi_wrt_FFi = compute_layernorm_derivative(O, X)  #LNFFi is the layernorm FF output for the ith decoder where i is the decoder layer for the WQC derivative that is being computed for
        """
        D_DOi_wrt_FFi = Decoders[f"Decoder{Decoder_Layer}"][f"D_LNFFi_wrt_FFi"]

        D_Lavg_wrt_FFi = D_Lavg_wrt_DOi @ D_DOi_wrt_FFi
        ############
        # Calculate D_FF_wrt_WFF2
        ReLu = Decoders[f"Decoder{Decoder_Layer}"][f"Feed_Forward"][f"ReLu"]
        """
        D_WFF2_wrt_WFF2_Nonjacobian = cp.empty((feed_forward_dimension, model_dimension), dtype=object)
        for i in range(feed_forward_dimension):
            for j in range(model_dimension):
                D_Wij_wrt_WFF2 = cp.zeros((1, feed_forward_dimension * model_dimension))
                D_Wij_wrt_WFF2_Col = 0
                for m in range(feed_forward_dimension):
                    for k in range(model_dimension):
                        D_Wij_wrt_Wmk = None
                        if m == i and k == j:
                            D_Wij_wrt_Wmk = 1
                        else:
                            D_Wij_wrt_Wmk = 0
                        D_Wij_wrt_WFF2[0,D_Wij_wrt_WFF2_Col] = D_Wij_wrt_Wmk
                        D_Wij_wrt_WFF2_Col += 1
                        if D_Wij_wrt_WFF2_Col >= (feed_forward_dimension * model_dimension):
                            D_Wij_wrt_WFF2_Col = 0
                            break
                D_WFF2_wrt_WFF2_Nonjacobian[i,j] = D_Wij_wrt_WFF2
        """

        #ReLu_Numpy = ReLu.get()
        ReLu_Numpy = ReLu
        ReLu_Numpy = ReLu_Numpy[:, None, :, :]
        D_FF_wrt_WFF2_Nonjacobian = ReLu_Numpy @ D_WFF2_wrt_WFF2_Nonjacobian
        #D_FF_wrt_WFF2 = To_Jacobian(D_FF_wrt_WFF2_Nonjacobian)  # Numpy
        D_FF_wrt_WFF2 = reshape_cupy_4d_to_3d(D_FF_wrt_WFF2_Nonjacobian)

    D_Lavg_wrt_WFF2 = D_Lavg_wrt_FFi @ D_FF_wrt_WFF2
    return D_Lavg_wrt_WFF2
   # potentially delete Momentum_GD_Update later

def D_Lavg_wrt_WL(Final_Layers):
    """
    Output:
    Jacobian of D of Loss wrt WL
    """
    X = Final_Layers[f"Softmax"][f"Output"]
    Y = Final_Layers[f"True_Labels"]

    D_Lavg_wrt_Softmax =  D_Lavg_wrt_X(X, Y) # Derivative of Lavg wrt final softmax
    #p rint("D_Lavg_wrt_Softmax = ", D_Lavg_wrt_Softmax)
    P = Final_Layers[f"Softmax"][f"Output"] # P = Softmax(S)
    S = Final_Layers[f"Linear"][f"Output"]
    D_Softmax_wrt_logits = Derivative_SoftmaxS_wrt_S(P, S) # derivative of softmax wrt its input
    #p rint("D_Softmax_wrt_logits = ", D_Softmax_wrt_logits)

    D_Lavg_wrt_logits = D_Lavg_wrt_Softmax @ D_Softmax_wrt_logits
    XL = Final_Layers[f"Linear"][f"Input"]

    ######### (model_dimension * vocab_size, model_dimension, vocab_size)
    """
    D_WL_wrt_WL_Nonjacobian = cp.empty((model_dimension, vocab_size), dtype=object)
    for i in range(model_dimension):
        for j in range(vocab_size):
            D_Wij_wrt_WL = cp.zeros((1, model_dimension * vocab_size))

            D_Wij_wrt_WL_Col = 0
            for m in range(model_dimension):
                for k in range(vocab_size):
                    D_Wij_wrt_Wmk = None
                    if m == i and k == j:
                        D_Wij_wrt_Wmk = 1
                    else:
                        D_Wij_wrt_Wmk = 0
                    D_Wij_wrt_WL[0,D_Wij_wrt_WL_Col] = D_Wij_wrt_Wmk
                    D_Wij_wrt_WL_Col += 1
                    if D_Wij_wrt_WL_Col >= (model_dimension * vocab_size):
                        D_Wij_wrt_WL_Col = 0
                        break
            D_WL_wrt_WL_Nonjacobian[i,j] = D_Wij_wrt_WL
      """
    # Create a zero matrix of shape (model_dimension * vocab_size, model_dimension, vocab_size)
    D_WL_wrt_WL_Nonjacobian = np.zeros((model_dimension * vocab_size, model_dimension, vocab_size), dtype=cp.float32)

    # Generate indices
    indices = np.arange(model_dimension * vocab_size)
    row_indices = indices // vocab_size  # Compute row indices for the second axis
    col_indices = indices % vocab_size  # Compute column indices for the third axis

    # Set the corresponding positions to 1
    D_WL_wrt_WL_Nonjacobian[indices, row_indices, col_indices] = 1
    D_WL_wrt_WL_Nonjacobian = np.repeat(D_WL_wrt_WL_Nonjacobian[cp.newaxis, ...], batch_size, axis=0)
    ########


    #XL_Numpy = XL.get()
    XL_Numpy = XL
    XL_Numpy = XL_Numpy[:, None, :, :]

    D_Logits_wrt_WL_Nonjacobian = XL_Numpy @ D_WL_wrt_WL_Nonjacobian
    #D_Logits_wrt_WL = To_Jacobian(D_Logits_wrt_WL_Nonjacobian)
    D_Logits_wrt_WL = reshape_cupy_4d_to_3d(D_Logits_wrt_WL_Nonjacobian)
    D_Lavg_wrt_WL = D_Lavg_wrt_logits @ D_Logits_wrt_WL
    return D_Lavg_wrt_WL


def Momentum_GD_Update(momentum_coefficient, J_In_Shape_W, Learning_Rate, Previous_Velocity, Theta_t):
    """
    Input:
    Previous_Velocity - (same shape as J_In_Shape_W)
    Learning rate
    J_In_Shape_W - gradient in W form
    momentum coefficient
    Theta_t - current weight matrix

    from decoders - velocity from the previous time step (initial time step would be zero)
    Output:
    Theta_t1, vt
    when returning reassign theta_t1 to weight matrix in dictionary and reassign vt to previous velocity corresponding to the previous weight matrix
    """

    vt = momentum_coefficient * (Previous_Velocity) - Learning_Rate * J_In_Shape_W
    Theta_t1 = Theta_t + vt
    return Theta_t1, vt

def CLR_Triangular_Mode(t):
    """
    globally defined
    n_min - minimum learning rate
    n_max - maximum learning rate
    T_cycle - number of epochs in one cycle
    Input:
    t - current epoch
    output:
    learning rate for the current time step
    """
    nt = n_min + (n_max - n_min) * abs(1 - ( (t/T_cycle) % (2) ))
    return nt
def xavier_uniform_cupy(shape, fan_in, fan_out):
    """
    1. shape → (512, 256)
    Specifies the dimensions of the weight matrix.
    In this case, it means the weight matrix has 512 output neurons and 256 input neurons.
    Typically written as (out_features, in_features).
    2. fan_in → 256
    The number of input neurons to the layer.
    Represents how many neurons are feeding into each neuron in the current layer.
    Used to scale the weight initialization properly.
    3. fan_out → 512
    The number of output neurons from the layer.

    Output:
    The function returns a randomly initialized tensor of shape shape (e.g., (512, 256)).


    Example Usage:
    # Define shape of weight matrix (512 output neurons, 256 input neurons)
    shape = (512, 256)
    fan_in, fan_out = 256, 512

    # Initialize weights
    weights = xavier_uniform_cupy(shape, fan_in, fan_out)
    """
    limit = np.sqrt(6 / (fan_in + fan_out)).astype(np.float16)

    return np.random.uniform(-limit, limit, shape, dtype=np.float32).astype(np.float16)

def create_Decoders_dict(Num_Of_Layers, Low, High):
    """
    Input:
    Num_Of_Layers
    Output:
    Decoders - dictionary containing the input outputs to all the layers in encoder
    """
    Attention_Heads_Weights = {} # dictionary where each key is an attention head which is a dictionary in itself
    for i in range(1 , number_of_heads+1):
        Attention_Heads_Weights[f"Attention_Head{i}"] = {f"D_WQi_wrt_WQ_Nonjacobian": None, f"D_WVi_wrt_WV_Nonjacobian": None, f"D_WKTi_wrt_WK_Nonjacobian": None}

    Decoders = {f"Attention_Heads_Weights": Attention_Heads_Weights, f"D_WO_wrt_WO_Nonjacobian": None}
    Attention_Heads_MA = {} # dictionary where each key is an attention head which is a dictionary in itself
    for i in range(1 , number_of_heads+1):
        Attention_Heads_MA[f"Attention_Head{i}"] = {f"Softmax_S": np.zeros((sequence_length_O, sequence_length_O)), f"S": np.zeros((sequence_length_O, sequence_length_O)) }
    Attention_Heads_CA = {} # dictionary where each key is an attention head which is a dictionary in itself
    for i in range(1 , number_of_heads+1):
        Attention_Heads_CA[f"Attention_Head{i}"] = {f"Softmax_S": np.zeros((sequence_length_O, sequence_length)), f"S": np.zeros((sequence_length_O, sequence_length)) }

    shape_A = (model_dimension, model_dimension)
    fan_in_A = model_dimension
    fan_out_A = model_dimension
    for i in range(1,  (Num_Of_Layers + 1)):
        Decoders[f"Decoder{i}"] = {f"D_Lavg_wrt_DOi": None, f"D_OMAi_wrt_Xi": None, f"D_OCAi_wrt_LNMAi": None, f"D_FFi_wrt_LNCAi": None, f"D_LNMAi_wrt_OMAi": None, f"D_LNCAi_wrt_OCAi": None, f"D_LNFFi_wrt_FFi": None, }  # Create outer dictionary
        Decoders[f"Decoder{i}"][f"Masked_Attention"] = {
                f"Input": np.zeros((sequence_length_O, model_dimension)),  # Inner dictionary with keys inside
                f"Weights": {f"Query_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Key_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Value_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Output_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A)},
                f"QKV": {f"Query": np.zeros((sequence_length_O, model_dimension)), f"Key": np.zeros((sequence_length_O, model_dimension)), f"Value": np.zeros((sequence_length_O, model_dimension))},
                f"First_Moment": {f"mQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                f"Attention_Heads": Attention_Heads_MA,
                f"Previous_Velocity": {f"Query_Weight_Matrix": np.zeros((model_dimension, model_dimension)), f"Key_Weight_Matrix":np.zeros((model_dimension, model_dimension)), f"Value_Weight_Matrix": np.zeros((model_dimension, model_dimension)), f"Output_Weight_Matrix":np.zeros((model_dimension, model_dimension)) },
                f"D_Lavg_wrt_OMAi": None,
                f"Concat": np.zeros((sequence_length_O, model_dimension)),
                f"Output": np.zeros((sequence_length_O, model_dimension))
            }
        Decoders[f"Decoder{i}"][f"Layer_Norm_MA"] = {
                f"Input": np.zeros((sequence_length_O, model_dimension)),  # Inner dictionary with keys inside
                f"Output": np.zeros((sequence_length_O, model_dimension))
            }
        Decoders[f"Decoder{i}"][f"Cross_Attention"] = {
                f"Input_Encoder":  np.zeros((sequence_length, model_dimension)),
                f"Input_Decoder":  np.zeros((sequence_length_O, model_dimension)),
                f"Weights": {f"Query_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A) , f"Key_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Value_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Output_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A) },
                f"Attention_Heads": Attention_Heads_CA,
                f"QKV": {f"Query": np.zeros((sequence_length_O, model_dimension)), f"Key": np.zeros((sequence_length, model_dimension)), f"Value": np.zeros((sequence_length, model_dimension))},
                f"First_Moment": {f"mQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},


                f"Previous_Velocity": {f"Query_Weight_Matrix": np.zeros((model_dimension, model_dimension)), f"Key_Weight_Matrix":np.zeros((model_dimension, model_dimension)), f"Value_Weight_Matrix": np.zeros((model_dimension, model_dimension)), f"Output_Weight_Matrix":np.zeros((model_dimension, model_dimension)) },
                f"Concat": np.zeros((sequence_length_O, model_dimension)),
                f"Output":  np.zeros((sequence_length_O, model_dimension))
            }
        Decoders[f"Decoder{i}"][f"Layer_Norm_CA"] = {
                f"Input":  np.zeros((sequence_length_O, model_dimension)),  # Inner dictionary with keys inside
                f"Output":  np.zeros((sequence_length_O, model_dimension))
            }
        Decoders[f"Decoder{i}"][f"Feed_Forward"] = {
                f"Input":  np.zeros((sequence_length_O, model_dimension)),  # Inner dictionary with keys inside
                f"ReLu_Input": np.zeros((sequence_length_O, feed_forward_dimension)),
                f"ReLu": np.zeros((sequence_length_O, feed_forward_dimension)),
                f"Weights": {f"WFF1": xavier_uniform_cupy((model_dimension, feed_forward_dimension), model_dimension, feed_forward_dimension) , f"WFF2":   xavier_uniform_cupy(( feed_forward_dimension, model_dimension), feed_forward_dimension , model_dimension)   },
                f"First_Moment": {f"mWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"mWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"vWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},
                f"Previous_Velocity": {f"WFF1": np.zeros((model_dimension, feed_forward_dimension)), f"WFF2": np.zeros((feed_forward_dimension, model_dimension)) },
                f"Output":  np.zeros((sequence_length_O, model_dimension))
            }
        Decoders[f"Decoder{i}"][f"Layer_Norm_FF"] = {
                f"Input":  np.zeros((sequence_length_O, model_dimension)),  # Inner dictionary with keys inside
                f"Output":  np.zeros((sequence_length_O, model_dimension))
            }


    return Decoders

def create_Encoders_dict(Num_Of_Layers, Low, High):
    """
    Input:
    Num_Of_Layers
    Output:
    Encoders - dictionary containing the input outputs to all the layers in encoder
    """
    Attention_Heads_Weights = {} # dictionary where each key is an attention head which is a dictionary in itself
    for i in range(1 , number_of_heads+1):
        Attention_Heads_Weights[f"Attention_Head{i}"] = {f"D_WQi_wrt_WQ_Nonjacobian": None, f"D_WVi_wrt_WV_Nonjacobian": None, f"D_WKTi_wrt_WK_Nonjacobian": None}

    Encoders = {f"Attention_Heads_Weights": Attention_Heads_Weights, f"D_WO_wrt_WO_Nonjacobian": None}
    Attention_Heads = {} # dictionary where each key is an attention head which is a dictionary in itself
    for i in range(1 , number_of_heads+1):
        Attention_Heads[f"Attention_Head{i}"] = {f"Softmax_S": np.zeros((sequence_length, sequence_length)), f"S": np.zeros((sequence_length, sequence_length)), f"D_Softmax_S_wrt_S": None }

    shape_A = (model_dimension, model_dimension)
    fan_in_A = model_dimension
    fan_out_A = model_dimension

    for i in range(1,  (Num_Of_Layers + 1)):
        Encoders[f"Encoder{i}"] = {f"D_Lavg_wrt_EOi": None, f"D_OSAi_wrt_Xi": None, f"D_FFi_wrt_LNSAi": None, f"D_LNFFi_wrt_FFi": None, f"D_LNSAi_wrt_OSAi": None, f"D_Concat_wrt_WQS_Nonjacobian": None, f"D_Concat_wrt_WKS_Nonjacobian": None}      # create outer dictionary
        Encoders[f"Encoder{i}"][f"Self_Attention"] = {
                f"Input": np.zeros((sequence_length, model_dimension)),  # Inner dictionary with keys inside
                f"Weights":{f"Query_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A) , f"Key_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Value_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A), f"Output_Weight_Matrix": xavier_uniform_cupy(shape_A, fan_in_A, fan_out_A) },
                f"QKV": {f"Query": np.zeros((sequence_length, model_dimension)), f"Key": np.zeros((sequence_length, model_dimension)), f"Value": np.zeros((sequence_length, model_dimension))},
                f"First_Moment": {f"mQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},

                f"Attention_Heads": Attention_Heads,
                f"Previous_Velocity": {f"Query_Weight_Matrix": np.zeros((model_dimension, model_dimension)), f"Key_Weight_Matrix":np.zeros((model_dimension, model_dimension)), f"Value_Weight_Matrix": np.zeros((model_dimension, model_dimension)), f"Output_Weight_Matrix":np.zeros((model_dimension, model_dimension)) },
                f"D_Lavg_wrt_SOi": None,
                f"Concat": np.zeros((sequence_length, model_dimension)),
                f"Output": np.zeros((sequence_length, model_dimension))
            }
        Encoders[f"Encoder{i}"][f"Layer_Norm_SA"] = {
                f"Input": np.zeros((sequence_length, model_dimension)),  # Inner dictionary with keys inside
                f"Output": np.zeros((sequence_length, model_dimension))
            }
        Encoders[f"Encoder{i}"][f"Feed_Forward"] = {
                f"Input":  np.zeros((sequence_length, model_dimension)),
                f"ReLu_Input": np.zeros((sequence_length, feed_forward_dimension)),
                f"ReLu": np.zeros((sequence_length, feed_forward_dimension)),
                f"Weights": {f"WFF1": xavier_uniform_cupy((model_dimension, feed_forward_dimension), model_dimension, feed_forward_dimension) , f"WFF2":   xavier_uniform_cupy(( feed_forward_dimension, model_dimension), feed_forward_dimension , model_dimension)   },
                f"Previous_Velocity": {f"WFF1": np.zeros((model_dimension, feed_forward_dimension)), f"WFF2": np.zeros((feed_forward_dimension, model_dimension)) },
                f"First_Moment": {f"mWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"mWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"vWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},
                f"Output":  np.zeros((sequence_length, model_dimension))
            }
        Encoders[f"Encoder{i}"][f"Layer_Norm_FF"] = {
                f"Input":  np.zeros((sequence_length, model_dimension)),  # Inner dictionary with keys inside
                f"Output":  np.zeros((sequence_length, model_dimension))
            }



    return Encoders

def create_Final_Layers_dict(Low, High):
    """
    """
    Final_Layers = {
    f"First_Moment_EM": np.zeros((vocab_size, model_dimension), dtype=np.float16),
    f"Second_Moment_EM": np.zeros((vocab_size, model_dimension), dtype=np.float16),
    f"Encoder_Token_Indices": None,
    f"Decoder_Token_Indices": None,
    f"Embedding_Matrix": np.random.random((vocab_size, model_dimension), dtype=np.float32).astype(np.float16),
    f"D_Softmax_wrt_logits": None,
    f"Linear": {f"Input": np.random.rand(sequence_length_O, model_dimension), f"Weights": xavier_uniform_cupy((model_dimension, vocab_size), model_dimension, vocab_size) , "Output": np.random.rand(sequence_length_O, vocab_size)},
    f"First_Moment": {f"mW": np.zeros((model_dimension, vocab_size), dtype=np.float16)},
    f"Second_Moment": {f"vW": np.zeros((model_dimension, vocab_size), dtype=np.float16)},
    f"Previous_Velocity": {f"Weights": np.zeros((model_dimension, vocab_size))},
    f"Softmax": {f"Input": np.random.rand(sequence_length_O, vocab_size), f"Output": np.random.rand(sequence_length_O, vocab_size)},
    f"True_Labels": np.zeros((sequence_length_O, vocab_size))
    }
    return Final_Layers

def create_Weights_dict(Number_Of_Encoder_Layers, Number_Of_Decoder_Layers):
    """
    Note: when loading back this dictionary, the model parameters should be the same that was used to create it
    """

    Decoders = {}

    for i in range(1,  (Number_Of_Decoder_Layers + 1)):
        if f"Decoder{i}" not in Decoders:  # Initialize if missing
          Decoders[f"Decoder{i}"] = {}

        Decoders[f"Decoder{i}"][f"Masked_Attention"] = {
                f"Weights": {f"Query_Weight_Matrix": None, f"Key_Weight_Matrix": None, f"Value_Weight_Matrix": None, f"Output_Weight_Matrix": None},
                f"First_Moment": {f"mQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},

            }

        Decoders[f"Decoder{i}"][f"Cross_Attention"] = {

                f"Weights": {f"Query_Weight_Matrix": None, f"Key_Weight_Matrix": None, f"Value_Weight_Matrix": None, f"Output_Weight_Matrix": None},
                f"First_Moment": {f"mQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},



            }

        Decoders[f"Decoder{i}"][f"Feed_Forward"] = {

                f"Weights": {f"WFF1": None , f"WFF2":   None  },
                f"First_Moment": {f"mWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"mWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"vWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},

            }

    Encoders = {}



    for i in range(1,  (Number_Of_Encoder_Layers + 1)):
        if f"Encoder{i}" not in Encoders:  # Initialize if missing
            Encoders[f"Encoder{i}"] = {}
        Encoders[f"Encoder{i}"][f"Self_Attention"] = {
                  f"Weights":{f"Query_Weight_Matrix": None , f"Key_Weight_Matrix": None, f"Value_Weight_Matrix": None, f"Output_Weight_Matrix": None },
                  f"First_Moment": {f"mQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"mO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},
                  f"Second_Moment": {f"vQ": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vK": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vV": np.zeros((model_dimension, model_dimension), dtype=np.float16), f"vO": np.zeros((model_dimension, model_dimension), dtype=np.float16)},

            }

        Encoders[f"Encoder{i}"][f"Feed_Forward"] = {

                f"Weights": {f"WFF1": None , f"WFF2":   None  },
                f"First_Moment": {f"mWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"mWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},
                f"Second_Moment": {f"vWFF1": np.zeros((model_dimension, feed_forward_dimension), dtype=np.float16), f"vWFF2": np.zeros((feed_forward_dimension, model_dimension), dtype=np.float16)},

            }
    Final_Layers = {
    f"First_Moment_EM": np.zeros((vocab_size, model_dimension), dtype=np.float16),
    f"Second_Moment_EM": np.zeros((vocab_size, model_dimension), dtype=np.float16),

    f"Embedding_Matrix": np.random.random((vocab_size, model_dimension), dtype=np.float32).astype(np.float16),

    f"Linear": {f"Weights": None },
    f"First_Moment": {f"mW": np.zeros((model_dimension, vocab_size), dtype=np.float16)},
    f"Second_Moment": {f"vW": np.zeros((model_dimension, vocab_size), dtype=np.float16)},

    }



    Weights_dict = {
    f"Encoders": Encoders,
    f"Decoders": Decoders,
    f"Final_Layers": Final_Layers

    }

    return Weights_dict
# Save function
def save_Decoders_dict(Decoders, filename):
    # Convert CuPy arrays to NumPy arrays for saving


    Decoders_Copy = deep_copy = copy.deepcopy(Decoders)
    def convert_to_numpy(d):
        for key, value in d.items():
            if isinstance(value, np.ndarray):  # CuPy array
                d[key] = np.asnumpy(value)  # Convert to NumPy array
            elif isinstance(value, dict):  # Recursively process sub-dictionaries
                convert_to_numpy(value)
        return d

    Decoders_numpy = convert_to_numpy(Decoders_Copy)

    # Save the dictionary using pickle
    with open(filename, 'wb') as f:
        pickle.dump(Decoders_numpy, f)
    #print(f"Dictionary saved to {filename}")

# Load function
def load_Decoders_dict(filename):
    with open(filename, 'rb') as f:
        Decoders_numpy = pickle.load(f)

    # Convert NumPy arrays back to CuPy arrays
    def convert_to_cupy(d):
        for key, value in d.items():
            if isinstance(value, cp.ndarray):  # NumPy array
                d[key] = np.asarray(value)  # Convert to CuPy array
            elif isinstance(value, dict):  # Recursively process sub-dictionaries
                convert_to_cupy(value)
        return d

    Decoders_cupy = convert_to_cupy(Decoders_numpy)

    return Decoders_cupy

def generate_token_indices_and_labels(batch_size, seq_length, vocab_size):
    # Ensure vocab_size is at least 2 (for 0 and vocab_size-1)
    assert vocab_size > 1, "Vocab size must be greater than 1."

    # Randomly determine the split point for each sequence (excluding first and last positions)
    split_points = np.random.randint(1, seq_length - 1, size=(batch_size,))

    # Generate random middle section (values between 1 and vocab_size - 2)
    middle_sections = np.random.randint(1, vocab_size - 1, size=(batch_size, seq_length - 2))

    # Create the token indices matrix
    token_indices = np.full((batch_size, seq_length), vocab_size - 1, dtype=np.int32)
    token_indices[:, 0] = 0  # Set the first column to 0

    # Fill middle section with random values
    for i in range(batch_size):
        token_indices[i, 1:split_points[i] + 1] = middle_sections[i, :split_points[i]]

    # Create the true label tensor (shifted left version of token indices)
    labels = np.hstack([token_indices[:, 1:], np.full((batch_size, 1), vocab_size - 1, dtype=np.int32)])

    # Create the one-hot encoded output tensor
    output_tensor = np.zeros((batch_size, seq_length, vocab_size), dtype=np.float32)

    # Use advanced indexing to set the correct positions to 1
    batch_indices = np.arange(batch_size)[:, None]
    seq_indices = np.arange(seq_length)[None, :]
    output_tensor[batch_indices, seq_indices, labels] = 1

    return token_indices, output_tensor

def generate_random_token_indices(batch_size, seq_length, vocab_size):
    # Ensure vocab_size is at least 3 (so we have valid range 1 to vocab_size-2)
    assert vocab_size > 2, "Vocab size must be greater than 2."

    # Generate random token indices between 1 and vocab_size-2
    return np.random.randint(1, vocab_size - 1, size=(batch_size, seq_length), dtype=np.int32)

def create_one_hot_labels(input_matrix):
    batch_size, seq_length = input_matrix.shape
    one_hot_labels = np.zeros((batch_size, seq_length, vocab_size), dtype=np.float32)

    # Shift the input matrix to get the next token as the label
    shifted_input = np.roll(input_matrix, shift=-1, axis=1)

    # The last token should predict itself
    shifted_input[:, -1] = input_matrix[:, -1]

    # Scatter to create one-hot encoding
    one_hot_labels[np.arange(batch_size)[:, None], np.arange(seq_length)[None, :], shifted_input] = 1

    return one_hot_labels
"""
Decoders - dictionary containing the input outputs to all the layers in encoder
Encoders - dictionary containing the input outputs to all the layers in encoder
Final_Layers - dictionary containing the final layer input and outputs, softmax input and output, and true labels
"""

def generate_batches(num_examples):
    indices = np.arange(1, num_examples + 1)
    np.random.shuffle(indices)
    return [indices[i:i + batch_size] for i in range(0, num_examples, batch_size)]


########

Number_Of_Encoder_Layers = 3
Number_Of_Decoder_Layers = 3
Encoder_Number = 1
Encoder_Layer = Encoder_Number
Decoder_Number = 1
Decoder_Layer = 1
Low, High = 8, 10     # 1, 2 tends to converge to .69314
##################


######### (Run first time) ###########
Encoders = create_Encoders_dict(Number_Of_Encoder_Layers, Low, High)
Decoders = create_Decoders_dict(Number_Of_Decoder_Layers, Low, High)
Final_Layers = create_Final_Layers_dict(Low, High)
######### (Run first time) ###########
"""



############################################# (After Creating Vocab) #################################
#1. (After Creating Vocab)
Encoders = None
with open("/content/drive/MyDrive/encoders_dict_Trained.pkl", "rb") as f:
    Encoders = pickle.load(f)
Decoders = None
with open("/content/drive/MyDrive/decoders_dict_Trained.pkl", "rb") as f:
    Decoders = pickle.load(f)

Final_Layers = None
with open("/content/drive/MyDrive/final_layers_dict_Trained.pkl", "rb") as f:
    Final_Layers = pickle.load(f)
"""
############################################# (After Creating Vocab) #################################

# create True Labels
"""
# Initialize zero matrix
one_hot_matrix = np.zeros((sequence_length_O, vocab_size), dtype=float)
# Set '1' in specified positions
one_hot_matrix[np.arange(sequence_length_O), positions] = 1
"""
"""
# Step 1: Initialize zero matrix for the entire batch
one_hot_batch = np.zeros((batch_size, sequence_length_O, vocab_size), dtype=float)

# Step 2: Randomly generate positions for each sample in the batch
positions = np.random.randint(0, vocab_size, (batch_size, sequence_length_O))

# Step 3: Set '1' at the randomized positions using advanced indexing
batch_indices = np.arange(batch_size)[:, None]  # Shape: (batch_size, 1)
seq_indices = np.arange(sequence_length_O)[None, :]  # Shape: (1, sequence_length_O)
one_hot_batch[batch_indices, seq_indices, positions] = 1  # Assign '1' in the correct positions
"""

embedding_matrix = Final_Layers[f"Embedding_Matrix"]
#tokens, outputs = generate_token_indices_and_labels(batch_size, sequence_length_O, vocab_size)
#random_tokens = generate_random_token_indices(batch_size, sequence_length, vocab_size)

"""
Weights_dict = create_Weights_dict(Number_Of_Encoder_Layers, Number_Of_Decoder_Layers)

#save_Decoders_dict(Weights_dict, 'Weights_dict.pkl')

Weights_dict = None
with open("/content/drive/MyDrive/Weights_dict.pkl", "rb") as f:
    Weights_dict = pickle.load(f)

################################### Loading back in Weight info ########################################
#Weights_dict = load_Decoders_dict('Weights_dict_Trained.pkl')
Final_Layers[f"Embedding_Matrix"] = Weights_dict[f"Final_Layers"][f"Embedding_Matrix"]
Final_Layers[f"First_Moment_EM"] = Weights_dict[f"Final_Layers"][f"First_Moment_EM"]
Final_Layers[f"Second_Moment_EM"] = Weights_dict[f"Final_Layers"][f"Second_Moment_EM"]
Final_Layers[f"Linear"][f"Weights"] = Weights_dict[f"Final_Layers"][f"Linear"][f"Weights"]
Final_Layers[f"First_Moment"][f"mW"] = Weights_dict[f"Final_Layers"][f"First_Moment"][f"mW"]
Final_Layers[f"Second_Moment"][f"vW"] = Weights_dict[f"Final_Layers"][f"Second_Moment"][f"vW"]
if Number_Of_Decoder_Layers == Number_Of_Encoder_Layers:
    Number_Of_Layers = Number_Of_Encoder_Layers
    for i in range(1,  (Number_Of_Layers + 1)):
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mQ"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mQ"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mV"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mV"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mK"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mK"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mO"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"First_Moment"][f"mO"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vQ"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vQ"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vV"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vV"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vK"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vK"]
        Decoders[f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vO"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Second_Moment"][f"vO"]

        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Weights"][f"Query_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Query_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Weights"][f"Key_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Weights"][f"Value_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Value_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Weights"][f"Output_Weight_Matrix"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Masked_Attention"][f"Weights"][f"Output_Weight_Matrix"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mQ"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mQ"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mV"] =  Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mV"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mK"] =  Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mK"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mO"] =  Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"First_Moment"][f"mO"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vQ"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vQ"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vV"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vV"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vK"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vK"]
        Decoders[f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vO"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Cross_Attention"][f"Second_Moment"][f"vO"]

        Decoders[f"Decoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF1"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        Decoders[f"Decoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF2"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        Decoders[f"Decoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"]
        Decoders[f"Decoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"]
        Decoders[f"Decoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"]
        Decoders[f"Decoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"] = Weights_dict[f"Decoders"][f"Decoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"]

        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Query_Weight_Matrix"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Key_Weight_Matrix"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Value_Weight_Matrix"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Weights"][f"Output_Weight_Matrix"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mQ"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mQ"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mK"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mK"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mV"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mV"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mO"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"First_Moment"][f"mO"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vQ"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vQ"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vK"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vK"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vV"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vV"]
        Encoders[f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vO"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Self_Attention"][f"Second_Moment"][f"vO"]

        Encoders[f"Encoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF1"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF1"]
        Encoders[f"Encoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF2"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Feed_Forward"][f"Weights"][f"WFF2"]
        Encoders[f"Encoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF1"]
        Encoders[f"Encoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Feed_Forward"][f"First_Moment"][f"mWFF2"]
        Encoders[f"Encoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF1"]
        Encoders[f"Encoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"] = Weights_dict[f"Encoders"][f"Encoder{i}"][f"Feed_Forward"][f"Second_Moment"][f"vWFF2"]
"""
##########################

"""
Final_Layers = {
    f"First_Moment_EM": np.zeros((vocab_size, model_dimension), dtype=np.float16),
    f"Second_Moment_EM": np.zeros((vocab_size, model_dimension), dtype=np.float16),

    f"Embedding_Matrix": np.random.random((vocab_size, model_dimension), dtype=np.float32).astype(np.float16),

    f"Linear": {f"Weights": xavier_uniform_cupy((model_dimension, vocab_size), model_dimension, vocab_size)},
    f"First_Moment": {f"mW": np.zeros((model_dimension, vocab_size), dtype=np.float16)},
    f"Second_Moment": {f"vW": np.zeros((model_dimension, vocab_size), dtype=np.float16)},

    }
"""

"""
# Save the Decoders dictionary (Cupy Implementation)
save_Decoders_dict(Decoders, 'decoders_dict.pkl')
save_Decoders_dict(Encoders, 'encoders_dict.pkl')
save_Decoders_dict(Final_Layers, 'final_layers_dict.pkl')

# Load the Decoders dictionary back  (Cupy Implementation)
Decoders = load_Decoders_dict('decoders_dict_Trained.pkl')
Encoders = load_Decoders_dict('encoders_dict_Trained.pkl')
Final_Layers = load_Decoders_dict('final_layers_dict_Trained.pkl')
"""
#@@@@@@@@@@@@@@@@@@@ Updated @@@@@@@@@@@@@@@@@@@@@@
# Calucalate Derivative of Wi wrt W  (Derivative of the ith weight matrix wrt the whole weight matrix for Q, K, V, O)
for i in range(1, number_of_heads+1):
    A_Num = i
    # Query Weight Matrix
    # Create an empty array with the correct shape
    D_WQi_wrt_WQ_Nonjacobian = np.zeros((model_dimension * model_dimension, model_dimension, dk))

    # Compute indices efficiently
    rows = np.arange(model_dimension)[:, None]  # Shape (model_dimension, 1)
    cols = dk * (A_Num - 1) + np.arange(dk)    # Shape (dk,)

    # Compute the flattened row indices
    final_rows = rows * model_dimension + cols  # Shape (model_dimension, dk)

    # Assign 1s in a vectorized manner
    D_WQi_wrt_WQ_Nonjacobian[final_rows, np.arange(model_dimension)[:, None], np.arange(dk)] = 1

    D_WQi_wrt_WQ_Nonjacobian = np.repeat(D_WQi_wrt_WQ_Nonjacobian[cp.newaxis, ...], batch_size, axis=0)
    D_WQi_wrt_WQ_Nonjacobian = D_WQi_wrt_WQ_Nonjacobian.astype(np.float16)
    ##############
    # Key Weight Matrix
    # Cupy implementation

    D_WKTi_wrt_WK_Nonjacobian = np.zeros((model_dimension * model_dimension, dk, model_dimension), dtype=cp.float32)

    # Compute indices for assignment
    i_indices = np.arange(dk).reshape(dk, 1)
    j_indices = np.arange(model_dimension).reshape(1, model_dimension)
    Col_indices = j_indices * model_dimension + (dk * (A_Num - 1) + i_indices)

    # Use advanced indexing to assign 1s
    D_WKTi_wrt_WK_Nonjacobian[Col_indices, i_indices, j_indices] = 1

    D_WKTi_wrt_WK_Nonjacobian = np.repeat(D_WKTi_wrt_WK_Nonjacobian[cp.newaxis, ...], batch_size, axis=0)
    D_WKTi_wrt_WK_Nonjacobian = D_WKTi_wrt_WK_Nonjacobian.astype(np.float16)
    #############
    # Value Weight Matrix
    # Cupy implementation

    # Create an empty array with the correct shape
    D_WVi_wrt_WV_Nonjacobian = np.zeros((model_dimension * model_dimension, model_dimension, dk))

    # Compute indices efficiently
    rowsV = np.arange(model_dimension)[:, None]  # Shape (model_dimension, 1)
    colsV = dk * (A_Num - 1) + np.arange(dk)    # Shape (dk,)

    # Compute the flattened row indices
    final_rows_V = rowsV * model_dimension + colsV  # Shape (model_dimension, dk)

    # Assign 1s in a vectorized manner
    D_WVi_wrt_WV_Nonjacobian[final_rows_V, np.arange(model_dimension)[:, None], np.arange(dk)] = 1
    D_WVi_wrt_WV_Nonjacobian = np.repeat(D_WVi_wrt_WV_Nonjacobian[cp.newaxis, ...], batch_size, axis=0)
    D_WVi_wrt_WV_Nonjacobian = D_WVi_wrt_WV_Nonjacobian.astype(np.float16)

    Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WQi_wrt_WQ_Nonjacobian"] = D_WQi_wrt_WQ_Nonjacobian
    Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WVi_wrt_WV_Nonjacobian"] = D_WVi_wrt_WV_Nonjacobian
    Decoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WKTi_wrt_WK_Nonjacobian"] = D_WKTi_wrt_WK_Nonjacobian



    Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WQi_wrt_WQ_Nonjacobian"] = D_WQi_wrt_WQ_Nonjacobian
    Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WVi_wrt_WV_Nonjacobian"] = D_WVi_wrt_WV_Nonjacobian
    Encoders[f"Attention_Heads_Weights"][f"Attention_Head{A_Num}"][f"D_WKTi_wrt_WK_Nonjacobian"] = D_WKTi_wrt_WK_Nonjacobian

D_WO_wrt_WO = np.eye(model_dimension * model_dimension)
D_WO_wrt_WO = np.repeat(D_WO_wrt_WO[cp.newaxis, ...], batch_size, axis=0)

Rows = model_dimension
Cols = model_dimension

D_WO_wrt_WO_Nonjacobian = reshape_cupy_3d_to_4d(D_WO_wrt_WO, Rows, Cols)
D_WO_wrt_WO_Nonjacobian = D_WO_wrt_WO_Nonjacobian.astype(np.float16)
Decoders[f"D_WO_wrt_WO_Nonjacobian"] = D_WO_wrt_WO_Nonjacobian
Encoders[f"D_WO_wrt_WO_Nonjacobian"] = D_WO_wrt_WO_Nonjacobian

######################

"""
# Save dictionary as a string to a text file (Numpy Implementation)
with open("Decoders.txt", "w") as file:
    file.write(str(Decoders))
# Save dictionary as a string to a text file
with open("Encoders.txt", "w") as file:
    file.write(str(Encoders))
# Save dictionary as a string to a text file
with open("Final_Layers.txt", "w") as file:
    file.write(str(Final_Layers))
"""

"""
# Load the dictionary back (Numpy Implementation)
with open("Decoders.txt", "r") as file:
    Decoders_string = file.read()
with open("Encoders.txt", "r") as file:
    Encoders_string = file.read()
with open("Final_Layers.txt", "r") as file:
    Final_Layers_string = file.read()

# Define `array` and `inf` for eval
Decoders = eval(Decoders_string, {"array": np.array, "inf": np.inf, "float32": np.float32})  # Converts dictionary string back to dictionary using eval
Encoders = eval(Encoders_string, {"array": np.array, "inf": np.inf, "float32": np.float32})  # Converts dictionary string back to dictionary using eval
Final_Layers = eval(Final_Layers_string, {"array": np.array, "inf": np.inf, "float32": np.float32})  # Converts dictionary string back to dictionary using eval
"""
###################



###############
"""
import subprocess
import threading
import time

def monitor_gpu(interval=5):
    #Function to monitor GPU utilization in a separate thread.
    while True:
        try:
            # Run nvidia-smi command
            result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
            #print(result.stdout)  # #print GPU utilization details
        except Exception as e:
            #print(f"Error monitoring GPU: {e}")
        time.sleep(interval)  # Wait for the specified interval

# Start GPU monitoring in a separate thread
gpu_thread = threading.Thread(target=monitor_gpu, args=(5,), daemon=True)
gpu_thread.start()

"""
##############
"""
# Run a simple GPU operation to activate the GPU
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = torch.randn(1000, 1000).to(device)
y = torch.mm(x, x)

#print("GPU is now active")

!nvidia-smi
"""
#############



"""
start_time = time.time()  # Record start time
# Code block to time
end_time = time.time()  # Record end time

elapsed_time = end_time - start_time  # Calculate elapsed time
#print(f"Elapsed time: {elapsed_time:.6f} seconds")
"""
######### Preliminary Calculations ##########
D_WFF2_wrt_WFF2 = np.eye(feed_forward_dimension * model_dimension)
D_WFF2_wrt_WFF2 = np.repeat(D_WFF2_wrt_WFF2[cp.newaxis, ...], batch_size, axis=0)
D_WFF2_wrt_WFF2_Nonjacobian = reshape_cupy_3d_to_4d(D_WFF2_wrt_WFF2, feed_forward_dimension, model_dimension)
D_WFF2_wrt_WFF2_Nonjacobian = D_WFF2_wrt_WFF2_Nonjacobian.astype(np.float16)


D_WFF1_wrt_WFF1 = np.eye(model_dimension * feed_forward_dimension)
D_WFF1_wrt_WFF1 = np.repeat(D_WFF1_wrt_WFF1[cp.newaxis, ...], batch_size, axis=0)
D_WFF1_wrt_WFF1_Nonjacobian = reshape_cupy_3d_to_4d(D_WFF1_wrt_WFF1, model_dimension, feed_forward_dimension)
D_WFF1_wrt_WFF1_Nonjacobian = D_WFF1_wrt_WFF1_Nonjacobian.astype(np.float16)


# Preliminary Calculation for derivatives for Encoders ################
# Cupy Implementation
M = model_dimension
S = sequence_length

# Total number of ones (one per (i,j) pair)
total_elements = M * S

# Generate indices corresponding to the row positions.
# In the original loop, rows are filled in the order:
# for i in range(M): for j in range(S): row_index = i * S + j.
rows = np.arange(total_elements)

# Compute the corresponding (i, j) pairs for each row.
# 'i' repeats S times, 'j' cycles every S elements.
i_idx = np.repeat(np.arange(M), S)
j_idx = np.tile(np.arange(S), M)

# The column index for each (i, j) pair is given by: col = j * M + i.
col_indices = j_idx * M + i_idx

# Create the zero matrix and assign 1's at the calculated indices.
D_XT_wrt_X_SA = np.zeros((total_elements, S * M))
D_XT_wrt_X_SA[rows, col_indices] = 1

D_XT_wrt_X_SA = np.repeat(D_XT_wrt_X_SA[cp.newaxis, ...], batch_size, axis=0)
D_XT_wrt_X_SA = D_XT_wrt_X_SA.astype(np.float16)



D_X_wrt_X = np.eye(sequence_length * model_dimension) # identity matrix
D_X_wrt_X = np.repeat(D_X_wrt_X[cp.newaxis, ...], batch_size, axis=0)
Rows = sequence_length
Cols = model_dimension
#D_X_wrt_X_nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols) #Numpy
D_X_wrt_X_nonjacobian_SA = reshape_cupy_3d_to_4d(D_X_wrt_X, Rows, Cols) # Cupy
D_X_wrt_X_nonjacobian_SA = D_X_wrt_X_nonjacobian_SA.astype(np.float16)

# Cupy Implementation
M = model_dimension
S = sequence_length_O

# Total number of ones (one per (i,j) pair)
total_elements = M * S

# Generate indices corresponding to the row positions.
# In the original loop, rows are filled in the order:
# for i in range(M): for j in range(S): row_index = i * S + j.
rows = np.arange(total_elements)

# Compute the corresponding (i, j) pairs for each row.
# 'i' repeats S times, 'j' cycles every S elements.
i_idx = np.repeat(np.arange(M), S)
j_idx = np.tile(np.arange(S), M)

# The column index for each (i, j) pair is given by: col = j * M + i.
col_indices = j_idx * M + i_idx

# Create the zero matrix and assign 1's at the calculated indices.
D_XT_wrt_X_MA = np.zeros((total_elements, S * M))
D_XT_wrt_X_MA[rows, col_indices] = 1
D_XT_wrt_X_MA = np.repeat(D_XT_wrt_X_MA[cp.newaxis, ...], batch_size, axis=0)



# Calculate D_X_wrt_X_nonjacobian
D_X_wrt_X = np.eye(sequence_length_O * model_dimension) # identity matrix
D_X_wrt_X = np.repeat(D_X_wrt_X[cp.newaxis, ...], batch_size, axis=0)

Rows = sequence_length_O
Cols = model_dimension
#D_X_wrt_X_nonjacobian = To_Nonjacobian(D_X_wrt_X, Rows, Cols) # numpy
D_X_wrt_X_nonjacobian_MACA = reshape_cupy_3d_to_4d(D_X_wrt_X, Rows, Cols) # cupy
D_X_wrt_X_nonjacobian_MACA = D_X_wrt_X_nonjacobian_MACA.astype(np.float16)

###
"""
#Calculate D_ET_wrt_E_nonjacobian
D_ET_wrt_E_nonjacobian = cp.empty((model_dimension, sequence_length), dtype=object)

for i in range(model_dimension):
    for j in range(sequence_length):
        D_eTij_wrt_E = cp.zeros((1, model_dimension * sequence_length))

        Col = j * model_dimension + i
        D_eTij_wrt_E[0, Col] = 1
        D_ET_wrt_E_nonjacobian[i,j] = D_eTij_wrt_E

D_E_wrt_E = cp.empty((sequence_length, model_dimension), dtype=object)
for i in range(sequence_length):
    for j in range(model_dimension):
        D_eij_wrt_E = cp.zeros((1, sequence_length * model_dimension))

        Col = i * model_dimension + j
        D_eij_wrt_E[0,Col] = 1

        D_E_wrt_E[i,j] = D_eij_wrt_E

"""
M, S = model_dimension, sequence_length

# Total elements in the flattened version
total_elements = M * S

# Generate indices for the 3D matrix
row_indices = np.arange(total_elements)  # Flattened row index
i_indices = row_indices % M  # Corresponding i index (model dimension)
j_indices = row_indices // M  # Corresponding j index (sequence position)

# Create a zero tensor
D_ET_wrt_E_nonjacobian = np.zeros((total_elements, M, S))

# Assign ones at the correct indices
D_ET_wrt_E_nonjacobian[row_indices, i_indices, j_indices] = 1
D_ET_wrt_E_nonjacobian = np.repeat(D_ET_wrt_E_nonjacobian[cp.newaxis, ...], batch_size, axis=0)
D_ET_wrt_E_nonjacobian = D_ET_wrt_E_nonjacobian.astype(np.float16)

D_E_wrt_E = np.eye(sequence_length * model_dimension)
D_E_wrt_E = np.repeat(D_E_wrt_E[cp.newaxis, ...], batch_size, axis=0)
D_E_wrt_E_Nonjacobian = reshape_cupy_3d_to_4d(D_E_wrt_E, sequence_length, model_dimension)
D_E_wrt_E_Nonjacobian = D_E_wrt_E_Nonjacobian.astype(np.float16)
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
############################

######### (Run first time) ###########
n = 0
All_Loss_List = []
All_Loss_List_Validate = []

######### (Run first time) ###########
"""

############################################# (After Creating Vocab) #################################
#2. (After Creating Vocab)
file_path = "/content/drive/My Drive/int_value.txt"
loaded_integer = None
with open(file_path, "r") as f:
    loaded_integer = int(f.read())

t = loaded_integer
n = t

folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List.pkl')
with open(file_path, 'rb') as f:
    loaded_list = pickle.load(f)
All_Loss_List = loaded_list


folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List_Validate.pkl')
with open(file_path, 'rb') as f:
    loaded_list = pickle.load(f)
All_Loss_List_Validate = loaded_list

"""

############################################# (After Creating Vocab) #################################

num_examples = number_of_examples
train_loss = []
val_loss = []
for n in range(n, n + 100):

    t = n + 1

    start_time1 = time.time()
    batches = generate_batches(num_examples)

    #print("batches: ", batches)
    loss_list = []
    count = 0
    for batch in batches:
        print("Batch Number: ", count)
        count += 1
        input_list = []
        output_list = []
        label_squeezed_list = []

        for i in range(batch_size):
            m = batch[i]
            input = data_tokenized[f"Example {m}"][f"input"]
            output = data_tokenized[f"Example {m}"][f"output"]
            label = data_tokenized[f"Example {m}"][f"label"]

            label_squeezed = np.squeeze(label, axis=0)
            output_squeezed = np.squeeze(output, axis=0)
            input_squeezed = np.squeeze(input, axis=0)
            input_list.append(input_squeezed)
            output_list.append(output_squeezed)
            label_squeezed_list.append(label_squeezed)
        input_tensor_E = np.stack(input_list)
        input_tensor_D = np.stack(output_list)
        labels = np.stack(label_squeezed_list)


        # Find positions of '1' in each row (along last axis)


        """




        positions = cp.argmax(labels, axis=2)  # Shape: (batch_size, num_rows)

        # Transfer result back to CPU for printing
        positions_cpu = positions.get()

        # Print the positions
        for batch_idx, batch in enumerate(positions_cpu):
            print(f"Batch {batch_idx}:")
            for row_idx, pos in enumerate(batch):
                print(f"  Row {row_idx}: 1 is at position {pos}")


        """
        #################################################################################################

        Final_Layers[f"Encoder_Token_Indices"] = input_tensor_E # Encoder Token Indices
        Final_Layers[f"Decoder_Token_Indices"] =  input_tensor_D # Decoder Token Indices

        Final_Layers[f"True_Labels"] = labels # True Labels

        embeddings_E = embed_tokens(input_tensor_E, embedding_matrix)

        embeddings_p_E = positional_encoding(embeddings_E)



        embeddings_D = embed_tokens(input_tensor_D, embedding_matrix)
        embeddings_p_D = positional_encoding(embeddings_D)

        # set initial input
        Encoders[f"Encoder{1}"][f"Self_Attention"][f"Input"] = embeddings_p_E
        Decoders[f"Decoder{1}"][f"Masked_Attention"][f"Input"] = embeddings_p_D
        ###########################################################################

        loss, X, Y =  forward_pass(Encoders, Decoders, Final_Layers, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers)# forward_pass(Encoders, Decoders, Final_Layers, Encoder_Number, Decoder_Number)
        loss_list.append(loss)
        print("loss type: ", type(loss))
        print("X: ", X)


        # Compute max indices
        max_indices_gpu = np.argmax(X, axis=2)
        print("max_indices_gpu: ", max_indices_gpu)
        #print("Loss = ", loss)


        if loss <= 0.05:
            #print("Encoder Input = ", Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Input"])
            #print("Decoder Input = ", Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Input"])
            #print("Loss = ", loss)
            """
            save_Decoders_dict(Decoders, 'decoders_dict_Trained.pkl')
            save_Decoders_dict(Encoders, 'encoders_dict_Trained.pkl')
            save_Decoders_dict(Final_Layers, 'final_layers_dict_Trained.pkl')

            with open("/content/drive/MyDrive/Weights_dict.pkl", "wb") as f:
                pickle.dump(Weights_dict, f)
            """
            # Define paths
            temp_path = "/content/drive/MyDrive/encoders_dict_Trained_temp.pkl"
            final_path = "/content/drive/MyDrive/encoders_dict_Trained.pkl"

            # Write to temporary file
            with open(temp_path, "wb") as f:
                pickle.dump(Encoders, f, protocol=pickle.HIGHEST_PROTOCOL)
                f.flush()  # Ensure all data is written
                os.fsync(f.fileno())  # Forces the OS to write data to disk

            # Rename to final path only after writing is complete
            os.rename(temp_path, final_path)

            # Define paths
            temp_path = "/content/drive/MyDrive/decoders_dict_Trained_temp.pkl"
            final_path = "/content/drive/MyDrive/decoders_dict_Trained.pkl"

            # Write to temporary file
            with open(temp_path, "wb") as f:
                pickle.dump(Decoders, f, protocol=pickle.HIGHEST_PROTOCOL)
                f.flush()  # Ensure all data is written
                os.fsync(f.fileno())  # Forces the OS to write data to disk

            # Rename to final path only after writing is complete
            os.rename(temp_path, final_path)

            # Define paths
            temp_path = "/content/drive/MyDrive/final_layers_dict_Trained_temp.pkl"
            final_path = "/content/drive/MyDrive/final_layers_dict_Trained.pkl"

            # Write to temporary file
            with open(temp_path, "wb") as f:
                pickle.dump(Final_Layers, f, protocol=pickle.HIGHEST_PROTOCOL)
                f.flush()  # Ensure all data is written
                os.fsync(f.fileno())  # Forces the OS to write data to disk

            # Rename to final path only after writing is complete
            os.rename(temp_path, final_path)
            """
            with open("/content/drive/MyDrive/decoders_dict_Trained.pkl", "wb") as f:
                pickle.dump(Decoders, f)
            with open("/content/drive/MyDrive/encoders_dict_Trained.pkl", "wb") as f:
                pickle.dump(Encoders, f)
            with open("/content/drive/MyDrive/final_layers_dict_Trained.pkl", "wb") as f:
                pickle.dump(Final_Layers, f)
            """
            break
        start_time = time.time()  # Record start time



        J_In_Shape_W_List, J = backward_pass( Decoders, Decoder_Layer, Final_Layers, Encoders, Encoder_Layer, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers, D_WFF1_wrt_WFF1_Nonjacobian, D_WFF2_wrt_WFF2_Nonjacobian, D_X_wrt_X_nonjacobian_MACA, D_XT_wrt_X_MA, D_X_wrt_X_nonjacobian_SA, D_XT_wrt_X_SA, D_ET_wrt_E_nonjacobian, D_E_wrt_E_Nonjacobian, t)
        end_time = time.time()  # Record end time
        elapsed_time = end_time - start_time  # Calculate elapsed time
        print(f"Elapsed time Backward pass: {elapsed_time:.6f} seconds")

        """
        if (n % 1 == 0):
            #print("for n = ", n)
            #print("X = ", X)
            #print("Y = ", Y)
            ##print("Encoder Input = ", Encoders[f"Encoder{Encoder_Number}"][f"Self_Attention"][f"Input"])
            ##print("Decoder Input = ", Decoders[f"Decoder{Decoder_Number}"][f"Masked_Attention"][f"Input"])
            #print("Loss = ", loss)
            #print("J_In_Shape_W_List: ", J_In_Shape_W_List)
            ##print("J = ", J)
            ##print("#######################################")
            #import tensorflow as tf

            ##print("Available GPUs:", tf.config.list_physical_devices('GPU'))
        """
    """
    save_Decoders_dict(Decoders, 'decoders_dict_Trained.pkl')
    save_Decoders_dict(Encoders, 'encoders_dict_Trained.pkl')
    save_Decoders_dict(Final_Layers, 'final_layers_dict_Trained.pkl')
    """
    """
    with open("/content/drive/MyDrive/decoders_dict_Trained.pkl", "wb") as f:
        pickle.dump(Decoders, f)
    with open("/content/drive/MyDrive/encoders_dict_Trained.pkl", "wb") as f:
        pickle.dump(Encoders, f)
    with open("/content/drive/MyDrive/final_layers_dict_Trained.pkl", "wb") as f:
        pickle.dump(Final_Layers, f)
    """

    #save_Decoders_dict(Weights_dict, 'Weights_dict.pkl')

    """
    with open("/content/drive/MyDrive/Weights_dict.pkl", "wb") as f:
        pickle.dump(Weights_dict, f)

    print("Weights_dict: ", Weights_dict)
    """
    print("for n = ", n)
    mean_value = sum(loss_list) / len(loss_list)
    mean_value = mean_value.get().item()
    print("mean_value type: ",type(mean_value))
    train_loss.append(mean_value)
    All_Loss_List.append(mean_value)

    print("Loss Avg = ", mean_value)
    end_time1 = time.time()  # Record end time
    elapsed_time = end_time1 - start_time1  # Calculate elapsed time
    print(f"Elapsed time Backward pass for entire epoch: {elapsed_time:.6f} seconds")

    batches = generate_batches(len(data_tokenized_Validate))

    #print("batches: ", batches)
    loss_list = []
    count = 0
    for batch in batches:
        print("Batch Number (Validate): ", count)
        count += 1
        input_list = []
        output_list = []
        label_squeezed_list = []

        for i in range(batch_size):
            m = batch[i]
            input = data_tokenized_Validate[f"Example {m}"][f"input"]
            output = data_tokenized_Validate[f"Example {m}"][f"output"]
            label = data_tokenized_Validate[f"Example {m}"][f"label"]

            label_squeezed = np.squeeze(label, axis=0)
            output_squeezed = np.squeeze(output, axis=0)
            input_squeezed = np.squeeze(input, axis=0)
            input_list.append(input_squeezed)
            output_list.append(output_squeezed)
            label_squeezed_list.append(label_squeezed)
        input_tensor_E = np.stack(input_list)
        input_tensor_D = np.stack(output_list)
        labels = np.stack(label_squeezed_list)


        # Find positions of '1' in each row (along last axis)


        """




        positions = cp.argmax(labels, axis=2)  # Shape: (batch_size, num_rows)

        # Transfer result back to CPU for printing
        positions_cpu = positions.get()

        # Print the positions
        for batch_idx, batch in enumerate(positions_cpu):
            print(f"Batch {batch_idx}:")
            for row_idx, pos in enumerate(batch):
                print(f"  Row {row_idx}: 1 is at position {pos}")


        """
        #################################################################################################

        Final_Layers[f"Encoder_Token_Indices"] = input_tensor_E # Encoder Token Indices
        Final_Layers[f"Decoder_Token_Indices"] =  input_tensor_D # Decoder Token Indices

        Final_Layers[f"True_Labels"] = labels # True Labels
        print("input_tensor_E: ", input_tensor_E)
        embeddings_E = embed_tokens(input_tensor_E, embedding_matrix)
        print("embeddings_E: ", embeddings_E)
        embeddings_p_E = positional_encoding(embeddings_E)



        embeddings_D = embed_tokens(input_tensor_D, embedding_matrix)
        embeddings_p_D = positional_encoding(embeddings_D)

        # set initial input
        Encoders[f"Encoder{1}"][f"Self_Attention"][f"Input"] = embeddings_p_E
        Decoders[f"Decoder{1}"][f"Masked_Attention"][f"Input"] = embeddings_p_D
        ###########################################################################

        loss, X, Y =  forward_pass(Encoders, Decoders, Final_Layers, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers)# forward_pass(Encoders, Decoders, Final_Layers, Encoder_Number, Decoder_Number)
        loss_list.append(loss)
    print("for n = ", n)
    mean_value = sum(loss_list) / len(loss_list)
    mean_value = mean_value.get().item()
    print("mean_value type: ",type(mean_value))
    val_loss.append(mean_value)
    All_Loss_List_Validate.append(mean_value)
    print("Loss Avg Validate= ", mean_value)

import os

# Define the folder and file paths
folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List.pkl')

# Ensure the folder exists
if not os.path.exists(folder_path):
    os.makedirs(folder_path)  # Creates the directory if it doesn't exist

# Define the file path
folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List.pkl')
# Save the list to a pickle file
with open(file_path, 'wb') as f:
    pickle.dump(All_Loss_List, f)




# Define the folder and file paths
folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List_Validate.pkl')

# Ensure the folder exists
if not os.path.exists(folder_path):
    os.makedirs(folder_path)  # Creates the directory if it doesn't exist

# Define the file path
folder_path = '/content/drive/My Drive/colab_files'
file_path = os.path.join(folder_path, 'All_Loss_List_Validate.pkl')
# Save the list to a pickle file
with open(file_path, 'wb') as f:
    pickle.dump(All_Loss_List_Validate, f)



"""
save_Decoders_dict(Decoders, 'decoders_dict_Trained.pkl')
save_Decoders_dict(Encoders, 'encoders_dict_Trained.pkl')
save_Decoders_dict(Final_Layers, 'final_layers_dict_Trained.pkl')
"""
"""
#save_Decoders_dict(Weights_dict, 'Weights_dict.pkl')
with open("/content/drive/MyDrive/Weights_dict.pkl", "wb") as f:
    pickle.dump(Weights_dict, f)
"""
"""
with open("/content/drive/MyDrive/decoders_dict_Trained.pkl", "wb") as f:
    pickle.dump(Decoders, f)
with open("/content/drive/MyDrive/encoders_dict_Trained.pkl", "wb") as f:
    pickle.dump(Encoders, f)
with open("/content/drive/MyDrive/final_layers_dict_Trained.pkl", "wb") as f:
    pickle.dump(Final_Layers, f)
"""
file_path = "/content/drive/My Drive/int_value.txt"

print("t: ", t)
# Step 3: Save an integer value to a file
integer_value = t
with open(file_path, "w") as f:
    f.write(str(integer_value))
 # Define paths
temp_path = "/content/drive/MyDrive/encoders_dict_Trained_temp.pkl"
final_path = "/content/drive/MyDrive/encoders_dict_Trained.pkl"

# Write to temporary file
with open(temp_path, "wb") as f:
    pickle.dump(Encoders, f, protocol=pickle.HIGHEST_PROTOCOL)
    f.flush()  # Ensure all data is written
    os.fsync(f.fileno())  # Forces the OS to write data to disk

# Rename to final path only after writing is complete
os.rename(temp_path, final_path)

# Define paths
temp_path = "/content/drive/MyDrive/decoders_dict_Trained_temp.pkl"
final_path = "/content/drive/MyDrive/decoders_dict_Trained.pkl"

# Write to temporary file
with open(temp_path, "wb") as f:
    pickle.dump(Decoders, f, protocol=pickle.HIGHEST_PROTOCOL)
    f.flush()  # Ensure all data is written
    os.fsync(f.fileno())  # Forces the OS to write data to disk

# Rename to final path only after writing is complete
os.rename(temp_path, final_path)

# Define paths
temp_path = "/content/drive/MyDrive/final_layers_dict_Trained_temp.pkl"
final_path = "/content/drive/MyDrive/final_layers_dict_Trained.pkl"

# Write to temporary file
with open(temp_path, "wb") as f:
    pickle.dump(Final_Layers, f, protocol=pickle.HIGHEST_PROTOCOL)
    f.flush()  # Ensure all data is written
    os.fsync(f.fileno())  # Forces the OS to write data to disk

# Rename to final path only after writing is complete
os.rename(temp_path, final_path)

#!nvidia-smi
#saved at 08:10pm 1/30/25

import matplotlib.pyplot as plt

# Example loss values (replace with your actual loss logs)

#train_loss = [1.4, 2.5, 3.230, 4.435, 5.34]
print("train_loss type: ", type(train_loss))
epochs = range(1, len(train_loss) + 1)
print("epochs type: ", type(epochs))

plt.figure(figsize=(8, 5))
plt.plot(epochs, train_loss, label='Training Loss', marker='o')
plt.plot(epochs, val_loss, label='Validation Loss', marker='s', linestyle='--')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(8, 5))
plt.plot(epochs, train_loss, label='Training Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(8, 5))
plt.plot(epochs, val_loss, label='Validation Loss', marker='s', linestyle='--')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Validation Loss Curve')
plt.legend()
plt.grid()
plt.show()

epochs = range(1, len(All_Loss_List_Validate) + 1)

plt.figure(figsize=(8, 5))
plt.plot(epochs, All_Loss_List_Validate, label='All time loss Loss (validation)', marker='s', linestyle='--')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('All time Loss Loss Curve (validation)')
plt.legend()
plt.grid()
plt.show()

epochs = range(1, len(All_Loss_List) + 1)

plt.figure(figsize=(8, 5))
plt.plot(epochs, All_Loss_List, label='All time loss Loss', marker='s', linestyle='--')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('All time Loss Loss Curve')
plt.legend()
plt.grid()
plt.show()

import re
from collections import defaultdict
import cupy as np
import pickle
def untokenize_text(tokenized_text):
    """Reconstructs the original text from the tokenized representation."""
    words = []
    current_word = []

    for token in tokenized_text:
        if token in ("<START>", "<END>"):
            continue  # Ignore special tokens

        if token.endswith("</w>"):
            current_word.append(token.replace("</w>", ""))  # Remove </w>
            words.append(''.join(current_word))  # Complete the word
            current_word = []  # Reset for the next word
        else:
            current_word.append(token)  # Continue building the word

    if current_word:
        words.aptopend(''.join(current_word))  # Handle any remaining tokens

    return ' '.join(words)  # Join words with spaces
def apply_bpe(text, merges, vocab):
    """Tokenizes input text into subwords using the learned BPE merges."""
    words = re.findall(r'\w+|[^\w\s]', text)  # Extract words and symbols separately
    tokenized_text = ["<START>"]
    token_indices = [vocab["<START>"]]

    for word in words:
        word = list(word) + ["</w>"]  # Mark word boundaries
        while len(word) > 1:
            best_pair = None
            for i in range(len(word) - 1):
                pair = (word[i], word[i + 1])
                if pair in merges:
                    best_pair = pair
                    break

            if best_pair:
                i = word.index(best_pair[0])
                word[i:i + 2] = [''.join(best_pair)]  # Merge pair
            else:
                break  # No more merges possible

        for subword in word:
            tokenized_text.append(subword)
            token_indices.append(vocab.get(subword, -1))

    tokenized_text.append("<END>")
    token_indices.append(vocab["<END>"])

    return tokenized_text, token_indices
text = "What's your favorite color"
tokenized_text, token_indices = apply_bpe(text, merges, vocab)

print("input untokenized: ", untokenize_text(tokenized_text))
print("tokenized_text: ", tokenized_text)
print("token_indices: ", token_indices)
A = data_tokenized[f"Example 1"][f"input"]
print("Data_tokenized Example 1 input: ", A)
B = data_tokenized[f"Example 1"][f"output"]
print("Data_tokenized Example 1 output: ", B)
token_indices_matrix = np.array(token_indices).reshape(1, -1)
m, n = token_indices_matrix.shape
token_indices_matrix_padded_I = np.pad(token_indices_matrix, ((0, 0), (0, sequence_length - n)), mode='edge')
print("token_indices_matrix_padded_I: ", token_indices_matrix_padded_I)
print("token_indices_matrix_padded_I shape: ", token_indices_matrix_padded_I.shape)
Start_Token_Index = vocab[f"<START>"]
print("Start_Token_Index: ", type(Start_Token_Index))
Start_Token_Index_Tensor = np.array(Start_Token_Index).reshape(1, 1)
m, n = Start_Token_Index_Tensor.shape
Start_Token_Index_Tensor_Padded = np.pad(Start_Token_Index_Tensor, ((0, 0), (0, sequence_length_O - n)), mode='edge')

print("Start_Token_Index_Tensor shape:", Start_Token_Index_Tensor.shape)
embeddings_E = embed_tokens(token_indices_matrix_padded_I, embedding_matrix)

embeddings_p_E = positional_encoding(embeddings_E)
print("Start_Token_Index_Tensor_Padded: ", Start_Token_Index_Tensor_Padded)
print("Start_Token_Index_Tensor_Padded shape: ", Start_Token_Index_Tensor_Padded.shape)




embeddings_D = embed_tokens(Start_Token_Index_Tensor_Padded, embedding_matrix)
embeddings_p_D = positional_encoding(embeddings_D)

# TEST ###########
"""
embeddings_D = embed_tokens(B, embedding_matrix)
embeddings_p_D = positional_encoding(embeddings_D)
"""


########3
# set initial input
Encoders[f"Encoder{1}"][f"Self_Attention"][f"Input"] = embeddings_p_E
Decoders[f"Decoder{1}"][f"Masked_Attention"][f"Input"] = embeddings_p_D

##### Test #######
"""
loss, X, Y =  forward_pass(Encoders, Decoders, Final_Layers, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers)
A = Decoders[f"Decoder{1}"][f"Layer_Norm_MA"][f"Output"]
print("Decoder Masked Output Train: ", A)
print("X infer: ", X)

embeddings_D = embed_tokens(Start_Token_Index_Tensor_Padded, embedding_matrix)
embeddings_p_D = positional_encoding(embeddings_D)
Encoders[f"Encoder{1}"][f"Self_Attention"][f"Input"] = embeddings_p_E
Decoders[f"Decoder{1}"][f"Masked_Attention"][f"Input"] = embeddings_p_D
loss, X, Y =  forward_pass(Encoders, Decoders, Final_Layers, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers)
A = Decoders[f"Decoder{1}"][f"Layer_Norm_MA"][f"Output"]
print("X infer: ", X)
print("Decoder Masked Output Infer: ", A)
"""
######################

new_tensor = None
n = 0
print("sequence_length_O: ", sequence_length_O)
for i in range(sequence_length_O - 1):

    loss, X, Y =  forward_pass(Encoders, Decoders, Final_Layers, Number_Of_Encoder_Layers, Number_Of_Decoder_Layers)
    # Compute max indices
    max_indices_gpu = np.argmax(X, axis=2)
    print("max_indices_gpu: ", max_indices_gpu)

    X = np.squeeze(X, axis=0)
    column_index_Predicted = np.argmax(X[i])
    print("i: ", i)
    print("X: ", X)
    print("X shape: ", X.shape)
    print("column_index_Predicted: ", column_index_Predicted)
    column_index_Predicted = column_index_Predicted.reshape(1, 1)
    Start_Token_Index_Tensor_Padded[0,i+1] = column_index_Predicted
    check = column_index_Predicted
    if check == vocab[f"<END>"]:
        break
    #embeddings_D = embed_tokens(Start_Token_Index_Tensor_Padded, embedding_matrix)
    embeddings_D = embed_tokens(Start_Token_Index_Tensor_Padded, embedding_matrix)
    embeddings_p_D = positional_encoding(embeddings_D)
    Decoders[f"Decoder{1}"][f"Masked_Attention"][f"Input"] = embeddings_p_D



vocab_flipped = {v: k for k, v in vocab.items()}
print("vocab_flipped: ", vocab_flipped)
print("Start_Token_Index_Tensor_Padded: ", Start_Token_Index_Tensor_Padded)
num_columns = Start_Token_Index_Tensor_Padded.shape[1]
tokenized = []
for i in range(num_columns):
    print("i = ", i)
    column_index = Start_Token_Index_Tensor_Padded[0, i]
    print("column_index type: ", type(column_index))
    print("column_index : ", column_index)
    column_index = column_index.item()
    Token = vocab_flipped[column_index]
    tokenized.append(Token)
    print(vocab_flipped[column_index])

print("tokenized: ", tokenized)
print("untokenized:", untokenize_text(tokenized))

print(data_tokenized)